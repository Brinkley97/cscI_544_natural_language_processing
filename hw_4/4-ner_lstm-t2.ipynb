{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with Bidirectional LSTMs\n",
    "- TUTORIAL: [Sequence Models and Long Short-Term Memory Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sequence-models-and-long-short-term-memory-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brinkley97/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# import pdb # for step by step debugging\n",
    "# assert(False) # use to stop at a specific line (think of like stop, quit, exit, etc)\n",
    "import gzip\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torcheval.metrics.functional import multiclass_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Update Data\n",
    "def load_data(file_path, new_sentence_col_name, word_col_name, ner_tag_col_name):\n",
    "    sentence_idxs = []\n",
    "    sentences = []\n",
    "    sentences_ner_tags = []\n",
    "\n",
    "    words = []\n",
    "    ner_tags = []\n",
    "\n",
    "    space_sentence_idxs = []\n",
    "    space_words = []\n",
    "    space_ner_tags = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.read().split(\"\\n\") # separate each sentence by new line\n",
    "        # print(lines)\n",
    "    sentence_idx = 0\n",
    "    for line in lines:\n",
    "        # text_idx, input_text, text_ner_tag = line.split(\"[\")\n",
    "        \n",
    "        each_line = line.split(\" \")\n",
    "        # print(len(each_line))\n",
    "        if len(each_line) != 1:\n",
    "            sentence_idx += 1\n",
    "            text_idx, input_text, ner_tag = each_line[0], each_line[1], each_line[2]\n",
    "            # print(f\"Line {text_idx} has word {input_text}, with NER Tag {ner_tag} --- {sentence_idx}\")\n",
    "            sentence_idxs.append(sentence_idx)\n",
    "\n",
    "            words.append(input_text)\n",
    "            ner_tags.append(ner_tag)\n",
    "            \n",
    "\n",
    "            space_sentence_idxs.append(sentence_idx)\n",
    "            space_words.append(input_text)\n",
    "            space_ner_tags.append(ner_tag)\n",
    "        else:\n",
    "            \n",
    "\n",
    "            if words and ner_tags:\n",
    "                \n",
    "                # print(f\"words {words}\")\n",
    "                sentences.append(words)\n",
    "                # print(f\"ner_tags {ner_tags}\")\n",
    "                sentences_ner_tags.append(ner_tags)\n",
    "                \n",
    "                words = []\n",
    "                ner_tags = []\n",
    "\n",
    "            sentence_idx = 0\n",
    "            reset_sentence_idx, input_text, ner_tag = \" \", \" \", \" \"\n",
    "            space_sentence_idxs.append(reset_sentence_idx)\n",
    "            space_words.append(input_text)\n",
    "            space_ner_tags.append(ner_tag)\n",
    "\n",
    "    \"\"\"Return as DF\"\"\"\n",
    "    df = pd.DataFrame(zip(sentence_idxs, words, ner_tags), columns=[new_sentence_col_name, word_col_name, ner_tag_col_name])\n",
    "    with_space_df = pd.DataFrame(zip(space_sentence_idxs, space_words, space_ner_tags), columns=[new_sentence_col_name, word_col_name, ner_tag_col_name])\n",
    "\n",
    "\n",
    "    \"\"\"Return as Dictionaries\"\"\"\n",
    "    sentences_in_dict = {}\n",
    "    ner_tags_in_dict = {}\n",
    "\n",
    "    sentences_in_dict[word_col_name] = sentences\n",
    "    ner_tags_in_dict[ner_tag_col_name] = sentences_ner_tags\n",
    "\n",
    "\n",
    "    return df, with_space_df, sentences_in_dict, ner_tags_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"data/train\"\n",
    "new_sentence_col_name = 'New Sentence Index'\n",
    "word_col_name = 'Word'\n",
    "ner_tag_col_name = 'NER Tag'\n",
    "ner_tag_idx_col_name = 'NER Tag Idx'\n",
    "\n",
    "df, with_space_df, sentences_in_dict, ner_tags_in_dict = load_data(train_file_path, new_sentence_col_name, word_col_name, ner_tag_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tags_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ner_tag_with_idx(to_map, key_name, reordered_dict, ner_tag_idx_col_name):\n",
    "    \"\"\"Pair ner tag with corresponding index\"\"\"\n",
    "    final_dict = {}\n",
    "    per_sentence = []\n",
    "    all_sentences = []\n",
    "\n",
    "    ner_tags = to_map[key_name]\n",
    "\n",
    "    # Iterate over the values_list\n",
    "    for ner_tags_idx in range(len(ner_tags)):\n",
    "        sentence_ner_tags = ner_tags[ner_tags_idx]\n",
    "        # print(sentence_ner_tags)\n",
    "        # assert(False)\n",
    "\n",
    "        for sentence_ner_tags_idx in range(len(sentence_ner_tags)):\n",
    "            ner_tag = sentence_ner_tags[sentence_ner_tags_idx]\n",
    "            # print(ner_tag)\n",
    "\n",
    "            # assert(False)\n",
    "\n",
    "            for key, value in reordered_dict.items():\n",
    "                # print(\"---\", key, value)\n",
    "                # assert(False)\n",
    "\n",
    "                if ner_tag == value:\n",
    "                    per_sentence.append(key)\n",
    "                    # print(\"---\", new_list) \n",
    "        # print(per_sentence)\n",
    "        all_sentences.append(per_sentence)\n",
    "        per_sentence = []\n",
    "    # print(all_sentences)\n",
    "    # assert(False)\n",
    "\n",
    "\n",
    "    final_dict[ner_tag_idx_col_name] = all_sentences\n",
    "\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_at_ner_tag = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
    "\n",
    "ner_tags_idx_in_dict = replace_ner_tag_with_idx(ner_tags_in_dict, ner_tag_col_name, idx_at_ner_tag, ner_tag_idx_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tags_idx_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sentences_dict, ner_tags_dict, ner_tags_idx_dict, word_col_name, ner_tag_idx_col_name, drop_ner_tags=True):\n",
    "\n",
    "\n",
    "    train_dict = {}\n",
    "    train_dict.update(sentences_dict)\n",
    "    train_dict.update(ner_tags_dict)\n",
    "    train_dict.update(ner_tags_idx_dict)\n",
    "\n",
    "    train_df = pd.DataFrame(train_dict)\n",
    "\n",
    "    if drop_ner_tags == True:\n",
    "        sub_df = train_df.loc[:, [word_col_name, ner_tag_idx_col_name]]\n",
    "        train_dataset = Dataset.from_dict(sub_df)\n",
    "        return train_dataset\n",
    "    else:\n",
    "        print(f\"Error with {drop_ner_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(sentences_in_dict, ner_tags_in_dict, ner_tags_idx_in_dict, word_col_name, ner_tag_idx_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = 'glove.6B.100d.gz'\n",
    "# output_file = 'glove.6B.100d.txt'\n",
    "\n",
    "# with gzip.open(input_file, 'rb') as f_in:\n",
    "#     with open(output_file, 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_dataset(glove_file, embedding_dim, vocab_size=None):\n",
    "    embeddings_index = {}\n",
    "    vocabulary = []\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe embeddings\", total=vocab_size):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vocabulary.append(word)\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    # Create a matrix to hold the embeddings\n",
    "    embedding_matrix = torch.randn((vocab_size, embedding_dim))\n",
    "    for i, (word, embedding_vector) in tqdm(enumerate(embeddings_index.items()), desc=\"Creating embedding matrix\"):\n",
    "        if i < vocab_size:\n",
    "            embedding_matrix[i] = torch.from_numpy(embedding_vector)\n",
    "\n",
    "    return embedding_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings: 100%|█████████▉| 400000/400002 [00:07<00:00, 50270.66it/s]\n",
      "Creating embedding matrix: 400000it [00:02, 171450.49it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_file = 'glove.6B.100d.txt'\n",
    "glove_embedding_dim = 100\n",
    "glove_vocab_size = 400002\n",
    "\n",
    "glove_embedding_matrix, glove_vocabulary = load_glove_dataset(glove_file, glove_embedding_dim, glove_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = glove_embedding_matrix.numpy()\n",
    "# glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocabulary = ['PAD', 'UNK'] + glove_vocabulary\n",
    "# glove_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_glove(words):\n",
    "    \"\"\"Build vocabulary for GloVe embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: `list`:\n",
    "        List of vocabulary terms.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word_idx, idx_word: `tuple`\n",
    "        A tuple containing two dictionaries mapping words to their corresponding indices and mapping indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    word_at_idx = {}\n",
    "\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # print(word, word_idx)\n",
    "        \n",
    "        word_at_idx[word] = word_idx\n",
    "        # assert(False)\n",
    "\n",
    "    idx_at_word = {}\n",
    "\n",
    "    for word, word_idx in word_at_idx.items():\n",
    "        # print(word, idx)\n",
    "\n",
    "        idx_at_word[word_idx] = word\n",
    "        \n",
    "    return word_at_idx, idx_at_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_idx, idx_with_words = build_vocab_glove(np.array(glove_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_with_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_with_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion function\n",
    "def convert_text_to_input_ids_glove(row, words_with_idx, ids_of_inputs_col_name, capitalize_words_col_name):\n",
    "    \"\"\"Convert tokenized text into input IDs using GloVe embeddings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row: `dict`\n",
    "        A dictionary containing the tokens for a single data point.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict: A dictionary containing the input IDs and capitalization vector for the tokens.\n",
    "        - 'ids_of_inputs' (list): List of input IDs corresponding to each token.\n",
    "        - 'capital_words_vector' (list): Binary vector indicating capitalization of each token.\n",
    "    \"\"\"\n",
    "    capital_words_vector = []  # Initialize list to store capitalization vector\n",
    "    ids_of_inputs = []  # Initialize list to store input IDs\n",
    "    for Word in row['Word']:  # Iterate through each word in the row\n",
    "        # Check if any character in the word is uppercase\n",
    "        if any(x.isupper() for x in Word):\n",
    "            capital_words_vector.append(1)  # Append 1 if uppercase character is found\n",
    "        else:\n",
    "            capital_words_vector.append(0)  # Append 0 if no uppercase character is found\n",
    "        \n",
    "        Word_lower = Word.lower()  # Convert word to lowercase\n",
    "        # Check if lowercase word is in word2idx dictionary\n",
    "        if Word_lower in words_with_idx:\n",
    "            ids_of_inputs.append(words_with_idx[Word_lower])  # Append corresponding index from word2idx\n",
    "        else:\n",
    "            ids_of_inputs.append(1)  # Append index 1 as default if word not found in word2idx\n",
    "    \n",
    "    return {\n",
    "        ids_of_inputs_col_name: ids_of_inputs,  # Return input IDs\n",
    "        capitalize_words_col_name: capital_words_vector  # Return capitalization vector\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_inputs_col_name = 'ID of Input'\n",
    "capitalize_words_col_name = 'Capital Word'\n",
    "# Assign the conversion function to a variable\n",
    "convert_function = convert_text_to_input_ids_glove\n",
    "\n",
    "# Create an empty list to store converted data points\n",
    "converted_data = []\n",
    "\n",
    "# Iterate through each row in the dataset and apply the conversion function\n",
    "for row in train_dataset:\n",
    "    converted_data.append(convert_function(row, words_with_idx, ids_of_inputs_col_name, capitalize_words_col_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_glove(dataset, convert_function, words_with_idx, word_col_name, ner_tag_idx_col_name, ids_of_inputs_col_name, capitalize_words_col_name):\n",
    "    converted_data = []\n",
    "\n",
    "    for row in dataset:\n",
    "        converted_row = {}\n",
    "        converted_row[word_col_name] = row[word_col_name]\n",
    "        converted_row[ner_tag_idx_col_name] = row[ner_tag_idx_col_name]\n",
    "        converted_result = convert_function(row, words_with_idx, ids_of_inputs_col_name, capitalize_words_col_name)\n",
    "        # print(converted_result)\n",
    "        # assert(False)\n",
    "        converted_row[ids_of_inputs_col_name] = converted_result[ids_of_inputs_col_name]\n",
    "        converted_row[capitalize_words_col_name] = converted_result[capitalize_words_col_name]\n",
    "        converted_data.append(converted_row)\n",
    "\n",
    "\n",
    "    converted_data_dict = {}\n",
    "    for key in converted_data[0]:\n",
    "        converted_data_dict[key] = []\n",
    "        for row in converted_data:\n",
    "            converted_data_dict[key].append(row[key])\n",
    "\n",
    "    # Create the converted_dataset\n",
    "    converted_dataset = Dataset.from_dict(converted_data_dict)\n",
    "\n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_glove = convert_dataset_to_glove(train_dataset, convert_text_to_input_ids_glove, words_with_idx, word_col_name, ner_tag_idx_col_name, ids_of_inputs_col_name, capitalize_words_col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NER Tag Idx': tensor([3, 0, 7, 0, 0, 0, 7, 0, 0]), 'ID of Input': tensor([  646,  7580,   516,   582,     6,  5262,   299, 10240,     4]), 'Capital Word': tensor([1, 0, 1, 0, 0, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_glove.set_format(\n",
    "    type='torch',\n",
    "    columns=[ner_tag_idx_col_name, ids_of_inputs_col_name, capitalize_words_col_name]\n",
    ")\n",
    "print(train_dataset_glove[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_sequence_per_batch(batch, ner_tag_idx_col_name=ner_tag_idx_col_name, ids_of_inputs_col_name=ids_of_inputs_col_name, capitalize_words_col_name=capitalize_words_col_name):\n",
    "    \"\"\"Pad sequences per batch\n",
    "    Combines individual samples into batches, padding sequences to the same length.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    batch: `list`\n",
    "        A list of dictionaries, each containing 'ID of Input', 'Capital Word', and NER Tag Idx.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    padded_data: `dict`\n",
    "        A dictionary containing padded sequences of ids_of_inputs, capital_words_vector, and NER Tag Idx.\n",
    "\n",
    "    \"\"\"\n",
    "    ids_of_inputs_list = []\n",
    "    capital_words_vector_list = []\n",
    "    ner_tag_idx_list = []\n",
    "\n",
    "    # Iterate through each item in the batch\n",
    "    for batch_item in batch:\n",
    "        # Get ids_of_inputs, capital_words_vector, and NER Tag Idx from the current item\n",
    "        ids_of_inputs = batch_item[ids_of_inputs_col_name]\n",
    "        capital_words_vector = batch_item[capitalize_words_col_name]\n",
    "        ner_tag_idx = batch_item[ner_tag_idx_col_name]\n",
    "\n",
    "        ids_of_inputs_list.append(ids_of_inputs)\n",
    "        capital_words_vector_list.append(capital_words_vector)\n",
    "        ner_tag_idx_list.append(ner_tag_idx)\n",
    "\n",
    "    padded_ids_of_inputs = pad_sequence(ids_of_inputs_list, batch_first=True)\n",
    "    padded_capital_words_vector = pad_sequence(capital_words_vector_list, batch_first=True)\n",
    "    padded_ner_tag_idx = pad_sequence(ner_tag_idx_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    padded_data = {\n",
    "        ids_of_inputs_col_name: padded_ids_of_inputs,\n",
    "        capitalize_words_col_name: padded_capital_words_vector,\n",
    "        ner_tag_idx_col_name: padded_ner_tag_idx\n",
    "    }\n",
    "\n",
    "    # Return the dictionary\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LSTM_LAYERS = 1\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.33\n",
    "\n",
    "OUTPUT_DIM = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_glove = torch.utils.data.DataLoader(train_dataset_glove, collate_fn=pad_sequence_per_batch, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLLSTM_NER_Tagger_GloVe(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim, output_dim, dropout_percentage, vocab_mappings, ner_tag_mappings, pretrained_embeddings):\n",
    "    super(BLLSTM_NER_Tagger_GloVe, self).__init__()\n",
    "\n",
    "    self.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(pretrained_embeddings).float())\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=1)\n",
    "    self.dropout = nn.Dropout(p=dropout_percentage)\n",
    "    self.linear_1 = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    self.linear_elu = nn.ELU()\n",
    "    self.linear_classifier = nn.Linear(output_dim, len(ner_tag_mappings))\n",
    "\n",
    "  def forward(self, sentences, all_capitals):\n",
    "    # print(f\"sentences shape: {sentences.shape}\")\n",
    "    # assert(False)\n",
    "    embeds = self.word_embeddings(sentences)\n",
    "    # print(f\"embeds shape: {embeds.shape}\")\n",
    "\n",
    "    all_capitals = all_capitals.unsqueeze(2)\n",
    "    # print(f\"all_capitals shape: {all_capitals.shape}\")\n",
    "\n",
    "    # concatenating captial vectors at the end of embeddings\n",
    "    lstm_input = torch.cat([embeds, all_capitals], dim=2)\n",
    "    # print(f\"lstm_input shape: {lstm_input.shape}\")\n",
    "\n",
    "     # assert(False)\n",
    "    lstm_out, _ = self.lstm(lstm_input)\n",
    "    # print(f\"lstm_out shape: {lstm_out.shape}\")\n",
    "\n",
    "    lstm_dropout = self.dropout(lstm_out)\n",
    "    # print(f\"lstm_dropout shape: {lstm_dropout.shape}\")\n",
    "\n",
    "    fc_layer = self.linear_1(lstm_dropout)\n",
    "    # print(f\"fc_layer shape: {fc_layer.shape}\")\n",
    "\n",
    "    elu_layer = self.linear_elu(fc_layer)\n",
    "    # print(f\"elu_layer shape: {elu_layer.shape}\")\n",
    "\n",
    "    tag_scores = self.linear_classifier(elu_layer)\n",
    "    # print(f\"tag_scores shape: {tag_scores.shape}\")\n",
    "\n",
    "    tag_scores = tag_scores.permute(0, 2, 1)\n",
    "    # print(f\"tag_scores shape: {tag_scores.shape}\")\n",
    "      \n",
    "    return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN THE BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Device: ', device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_with_idx\n",
    "# idx_at_ner_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLLSTM_NER_Tagger_GloVe(\n",
       "  (word_embeddings): Embedding(400002, 100)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear_1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (linear_elu): ELU(alpha=1.0)\n",
       "  (linear_classifier): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloVe_blstm_model_class = BLLSTM_NER_Tagger_GloVe(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, words_with_idx, idx_at_ner_tag, pretrained_embeddings=glove_embeddings)\n",
    "gloVe_blstm_model_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.SGD(gloVe_blstm_model_class.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_in_dict[word_col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sentence_batch_generator(sentences, batch_indices):\n",
    "    for idx in batch_indices:\n",
    "        yield sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_glove(model, training_data, sentences, N_epochs, loss_function, optimizer, word_to_ix, batch_size, per_batch):\n",
    "    ids_of_inputs_col_name, capitalize_words_col_name, ner_tag_idx_col_name = per_batch\n",
    "    store_predicted_ner_tag_scores_per_epoch = []\n",
    "\n",
    "    for epoch_index in tqdm(range(N_epochs), desc=\"Epochs\"):\n",
    "        running_loss = 0\n",
    "        store_predicted_ner_tag_scores = []\n",
    "\n",
    "        model.train(True)  # Set the model to training mode\n",
    "        \n",
    "        for batch_idx, batch_of_training_data in enumerate(tqdm(training_data, desc=f\"Epoch {epoch_index + 1}\", leave=False)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sentences_scores = batch_of_training_data[ids_of_inputs_col_name]\n",
    "            one_hot_capitals = batch_of_training_data[capitalize_words_col_name]\n",
    "            true_ner_tag_mappings = batch_of_training_data[ner_tag_idx_col_name]\n",
    "\n",
    "            pred_ner_tag_scores_per_batch = model(sentences_scores, one_hot_capitals)\n",
    "            loss_per_batch = loss_function(pred_ner_tag_scores_per_batch, true_ner_tag_mappings)\n",
    "\n",
    "            # Accumulate the loss for this batch\n",
    "            running_loss += loss_per_batch.item()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss_per_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Append the predicted scores for this batch\n",
    "            store_predicted_ner_tag_scores.append((epoch_index, batch_idx, pred_ner_tag_scores_per_batch.detach().cpu()))\n",
    "\n",
    "        epoch_loss = running_loss / len(training_data)\n",
    "        print(f\"Epoch {epoch_index}: Average Loss: {epoch_loss}\")\n",
    "\n",
    "        store_predicted_ner_tag_scores_per_epoch.append(store_predicted_ner_tag_scores)\n",
    "\n",
    "    return store_predicted_ner_tag_scores_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vocabulary[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 1/2 [00:24<00:24, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average Loss: 0.17267637612654807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 2/2 [00:51<00:00, 25.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss: 0.12586468667426007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "per_batch = (ids_of_inputs_col_name, capitalize_words_col_name, ner_tag_idx_col_name)\n",
    "predicted_ner_tag_scores_per_epoch = train_glove(gloVe_blstm_model_class, train_dataloader_glove, glove_vocabulary[2:], 2, LOSS_FUNCTION, OPTIMIZER, words_with_idx, TRAIN_BATCH_SIZE, per_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of store_predicted_ner_tag_scores_per_epoch is 2.\n",
      "The length of store_predicted_ner_tag_scores_per_epoch[-1] is 235.\n",
      "This indicates that during the last epoch of training:\n",
      "- The training data was divided into 235 batches,\n",
      "- And the model processed each batch, making predictions for each batch.\n"
     ]
    }
   ],
   "source": [
    "# Get the length of store_predicted_ner_tag_scores_per_epoch and store_predicted_ner_tag_scores_per_epoch[-1]\n",
    "length_of_epoch_scores = len(predicted_ner_tag_scores_per_epoch)\n",
    "last_epoch_scores_length = len(predicted_ner_tag_scores_per_epoch[-1])\n",
    "\n",
    "# Print the explanation\n",
    "print(f\"The length of store_predicted_ner_tag_scores_per_epoch is {length_of_epoch_scores}.\")\n",
    "print(f\"The length of store_predicted_ner_tag_scores_per_epoch[-1] is {last_epoch_scores_length}.\")\n",
    "print(\"This indicates that during the last epoch of training:\")\n",
    "print(\"- The training data was divided into\", last_epoch_scores_length, \"batches,\")\n",
    "print(\"- And the model processed each batch, making predictions for each batch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ner_tags_scores = predicted_ner_tag_scores_per_epoch[-1]\n",
    "# type(predicted_ner_tags_scores), len(predicted_ner_tags_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ner_tags_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_max_score_and_tag(predicted_ner_tags_scores, tag_to_ix):\n",
    "#     \"\"\"Find the maximum scores and corresponding tags\"\"\"\n",
    "#     # print(ner_tag_prediction_scores)\n",
    "#     store_predicted_ner_tags = []\n",
    "# \n",
    "#     for predicted_ner_tags_scores_idx in range(len(predicted_ner_tags_scores)):\n",
    "#         ner_tags_per_sentence = []\n",
    "#         batch = predicted_ner_tags_scores[predicted_ner_tags_scores_idx]\n",
    "#         # print(batch, type(batch))\n",
    "        \n",
    "#         batch_idx = batch[0]\n",
    "#         batch_sentences = batch[1]\n",
    "#         batch_scores = batch[2][0]\n",
    "#         # print(f\"batch_sentences: {batch_sentences}\")\n",
    "#         # print(f\"   batch_scores: {batch_scores.shape}\")\n",
    "        \n",
    "#         max_scores, max_tag_idxs = torch.max(batch_scores, dim=1)\n",
    "#         # print(f\"   max_tag_idxs: {max_tag_idxs}\")\n",
    "#         max_tag_idxs = max_tag_idxs.tolist()\n",
    "#         max_tag_idxs = sum(max_tag_idxs, [])\n",
    "#         # print(f\"   max_tag_idxs: {max_tag_idxs}\")\n",
    "#         # assert(False)\n",
    "\n",
    "#         pred_ner_tag_mapped = []\n",
    "#         for idx in max_tag_idxs:\n",
    "#             for key, value in tag_to_ix.items():\n",
    "#                 if key == idx:\n",
    "#                     # print(True)\n",
    "#                     pred_ner_tag_mapped.append(value)\n",
    "#                     # assert(False)\n",
    "#                     break\n",
    "#             ner_tags_per_sentence.append(pred_ner_tag_mapped)\n",
    "#         # print(\"pred_ner_tag_mapped\")\n",
    "#         # print(pred_ner_tag_mapped)\n",
    "#         # assert(False)\n",
    "#         store_predicted_ner_tags.append((batch_idx, batch_sentences, ner_tags_per_sentence))\n",
    "\n",
    "#     return store_predicted_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ner_tags = find_max_score_and_tag(predicted_ner_tags_scores, idx_at_ner_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_max_score_and_tag(predicted_ner_tags_scores, tag_to_ix):\n",
    "#     \"\"\"Find the maximum scores and corresponding tags\"\"\"\n",
    "#     store_predicted_ner_tags = []\n",
    "\n",
    "#     for batch_idx, batch_sentences, batch_scores in predicted_ner_tags_scores:\n",
    "#         max_scores, max_tag_idxs = torch.max(batch_scores, dim=1)\n",
    "#         max_tag_idxs = max_tag_idxs.tolist()\n",
    "\n",
    "#         # Map tag indices to actual tags using tag_to_ix\n",
    "#         ner_tags_per_sentence = [[tag_to_ix[idx] for idx in tag_idxs] for tag_idxs in max_tag_idxs]\n",
    "\n",
    "#         store_predicted_ner_tags.append((batch_idx, batch_sentences, ner_tags_per_sentence))\n",
    "\n",
    "#     return store_predicted_ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_score_and_tag(predicted_ner_tags_scores, tag_to_ix):\n",
    "    \"\"\"Find the maximum scores and corresponding tags\"\"\"\n",
    "    store_predicted_ner_tags = []\n",
    "\n",
    "    # print(f\"predicted_ner_tags_scores: {len(predicted_ner_tags_scores)}\")\n",
    "    for epoch_idx in range(len(predicted_ner_tags_scores)):\n",
    "        # print(f\"epoch_idx: {epoch_idx}\")\n",
    "\n",
    "        predicted_ner_tags_score_per_epochs = predicted_ner_tags_scores[epoch_idx]\n",
    "        # print(f\"predicted_ner_tags_score_per_epochs: {len(predicted_ner_tags_score_per_epochs)}\")\n",
    "\n",
    "        for batch_idx in range(len(predicted_ner_tags_score_per_epochs)):\n",
    "            predicted_ner_tags_score_per_batch = predicted_ner_tags_score_per_epochs[batch_idx]\n",
    "            # print(f\"predicted_ner_tags_score_per_batch: {type(predicted_ner_tags_score_per_batch)}\")\n",
    "\n",
    "            _, _, predicted_ner_tags_score = predicted_ner_tags_score_per_batch\n",
    "            # print(f\"predicted_ner_tags_score: {type(predicted_ner_tags_score)}\")\n",
    "            \n",
    "        # assert(False)\n",
    "\n",
    "            max_scores, max_tag_idxs = torch.max(predicted_ner_tags_score, dim=1)\n",
    "            max_tag_idxs = max_tag_idxs.tolist()\n",
    "\n",
    "            ner_tags_per_sentence = []\n",
    "            for idxs in max_tag_idxs:\n",
    "                for idx in idxs:\n",
    "                    for key, value in tag_to_ix.items():\n",
    "                        # print(key, value, idx)\n",
    "                        # assert(False)\n",
    "                        if key == idx:\n",
    "                            # print(value)\n",
    "                            # assert(False)\n",
    "                            ner_tags_per_sentence.append(value)\n",
    "                            break\n",
    "                 store_predicted_ner_tags.append((batch_idx, ner_tags_per_sentence))\n",
    "\n",
    "    return store_predicted_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_ner_tag_scores_per_epoch[0][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score_and_tag_predictions = find_max_score_and_tag(predicted_ner_tag_scores_per_epoch, idx_at_ner_tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_score_and_tag_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_dataframe(predicted_ner_tags):\n",
    "    \n",
    "    data_for_df = []\n",
    "    \n",
    "    for idx, batch in enumerate(predicted_ner_tags):\n",
    "        # print(batch)\n",
    "        # assert(False)\n",
    "        batch_idx = batch[0]\n",
    "        batch_sentences = batch[1]\n",
    "        batch_ner_tags = batch[2]\n",
    "        # print(batch_ner_tags)\n",
    "        # assert(False)\n",
    "\n",
    "        \n",
    "        for sentence_tag_idx, (sentence, ner_tags) in enumerate(zip(batch_sentences, batch_ner_tags)):\n",
    "            # print(sentence_tag_idx)\n",
    "            # print(sentence, ner_tags)\n",
    "\n",
    "            for row_idx, (word, tag) in enumerate(zip(sentence, ner_tags)):\n",
    "                # print(row_idx, word, tag)\n",
    "                # if word == \".\":\n",
    "                    # print(word, tag)\n",
    "                data_for_df.append((row_idx + 1, word, tag))\n",
    "            data_for_df.append((\" \", \" \", \" \"))\n",
    "        # print(data_for_df)\n",
    "    results_df = pd.DataFrame(data_for_df, columns=['New Sentence Index', 'Word', 'NER Tag'], index=range(1, len(data_for_df)+1))\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = create_ner_dataframe(predicted_ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'results/train2.out'\n",
    "\n",
    "# when saving dataframe as txt with df.to_csv, my single quotes (\") are turning into triple quotes (\"\"\"). To NOT do this, add quoting=csv.QUOTE_NONE\n",
    "\n",
    "results_df.to_csv(file_path, sep=' ', header=False, quoting=csv.QUOTE_NONE, index=False, escapechar=' ')\n",
    "# updated_results_df.to_csv(file_path, sep=' ', header=False, quoting=csv.QUOTE_NONE, index=False, escapechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
