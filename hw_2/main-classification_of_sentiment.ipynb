{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2- Binary and Ternary Classification for Sentiment Analysis\n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import sklearn\n",
    "import gensim.models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import utils\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "pretrained_word_two_vec_model = api.load('word2vec-google-news-300')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HW Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset Generation\n",
    "    1. Read data\n",
    "    2. Keep reviews and ratings\n",
    "    3. Create binary and ternary classes\n",
    "    4. Clean data steps\n",
    "    4. Clean data function\n",
    "    5. Split data\n",
    "    6. Load pretrained model and train my model\n",
    "        - Get similarity for the pretrained model\n",
    "        - Get similarity for my trained model\n",
    "2. Word Embedding\n",
    "    - Get word embeddings for pretrained model\n",
    "    - Get word embeddings for my model\n",
    "3. Simple models\n",
    "    - Get accuracy for perceptron on pretrained model\n",
    "    - Get accuracy for svm on pretrained model\n",
    "    - Get accuracy for perceptron on my model\n",
    "    - Get accuracy for svm on my model\n",
    "    - What do I conclude from comparing performances\n",
    "4. Feedforward Neural Networks (FFNN)\n",
    "5. Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation\n",
    "\n",
    "- [x] Load the Amazon reviews dataset\n",
    "- [x] Build a balanced dataset of 250K reviews along with their ratings through random selection\n",
    "    - [x] Rating 1: 50K instances\n",
    "    - [x] Rating 2: 50K instances\n",
    "    - [x] Rating 3: 50K instances\n",
    "    - [x] Rating 4: 50K instances\n",
    "    - [x] Rating 5: 50K instances\n",
    "- [x] Create ternary labels using the ratings\n",
    "    - [x] Class 1: Ratings 4 and 5 (positive sentiment)\n",
    "    - [x] Class 2: Ratings 1 and 2 (negative sentiment)\n",
    "    - [x] Class 3: Rating 3 (neutral sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../datasets/amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "amazon_reviews_copy_df = pd.read_csv(dataset, sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                                     Great product.\n",
       "1                 5  What's to say about this commodity item except...\n",
       "2                 5    Haven't used yet, but I am sure I will like it.\n",
       "3                 1  Although this was labeled as &#34;new&#34; the...\n",
       "4                 4                    Gorgeous colors and easy to use\n",
       "...             ...                                                ...\n",
       "2640249           4  I can't live anymore whithout my Palm III. But...\n",
       "2640250           4  Although the Palm Pilot is thin and compact it...\n",
       "2640251           4  This book had a lot of great content without b...\n",
       "2640252           5  I am teaching a course in Excel and am using t...\n",
       "2640253           5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ratings_df = amazon_reviews_copy_df.loc[0:, ['star_rating', 'review_body']]\n",
    "reviews_ratings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_reviews(df: pd.DataFrame, review_col_name: str, number_of_reviews: int = 3):\n",
    "    \"\"\"Include reviews and ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    review_col_name: `str`\n",
    "        The specific_column to get the reviews and ratings of\n",
    "    \n",
    "    number_of_reviews: `int`\n",
    "        Number of samples to include\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Nothing; instead, print the reviews with ratings\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    columns_to_include = [review_col_name, 'star_rating']\n",
    "\n",
    "    # Initialize an empty list to store dictionaries\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # Iterate over the specified columns and retrieve the first three rows\n",
    "    for row in df[columns_to_include].head(3).to_dict(orient='records'):\n",
    "        list_of_dicts.append({'star_rating': row['star_rating'], review_col_name: row[review_col_name]})\n",
    "\n",
    "    for dictionary in list_of_dicts:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## Create binary and ternary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_type(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Update the data type of the star ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with rating values\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ratings = ['1','2','3','4','5']\n",
    "    star_rating_series = df[col_name].copy()\n",
    "\n",
    "    # Convert type to strings\n",
    "    star_rating_series.astype('str')\n",
    "\n",
    "    # Check valid list and see which of our stars match\n",
    "    rows = star_rating_series.index\n",
    "    is_rating_in_valid_ratings = rows[star_rating_series.isin(valid_ratings)]\n",
    "\n",
    "    # Convert to list\n",
    "    is_rating_in_valid_ratings = is_rating_in_valid_ratings.to_list()\n",
    "\n",
    "    updated_df = df.iloc[is_rating_in_valid_ratings]\n",
    "    updated_df[col_name] = updated_df[col_name].astype(int)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/2907883124.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[col_name] = updated_df[col_name].astype(int)\n"
     ]
    }
   ],
   "source": [
    "updated_reviews_ratings_df = update_data_type(reviews_ratings_df, 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640237 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640080 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df = updated_reviews_ratings_df.dropna()\n",
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating  review_body\n",
      "0              False        False\n",
      "1              False        False\n",
      "2              False        False\n",
      "3              False        False\n",
      "4              False        False\n",
      "...              ...          ...\n",
      "2640249        False        False\n",
      "2640250        False        False\n",
      "2640251        False        False\n",
      "2640252        False        False\n",
      "2640253        False        False\n",
      "\n",
      "[2640080 rows x 2 columns]\n",
      "There are no NaN values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_check = updated_reviews_ratings_df.isna()\n",
    "\n",
    "# Display the DataFrame with True where NaN values exist\n",
    "print(nan_check)\n",
    "\n",
    "# Check if any NaN value exists in the DataFrame\n",
    "if nan_check.any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviews per rating star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"# reviews per rating\", updated_reviews_ratings_df['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_star_ratings(df: pd.DataFrame, col_name: str, star_value: int, number_of_reviews: int):\n",
    "    \"\"\"Build a subset balanced dataset with reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The dataframe to use\n",
    "    col_name: `str`\n",
    "        The name of the column to get reviews from\n",
    "    star_value: `int`\n",
    "        The star rating of the review\n",
    "    number_of_reviews: `int`\n",
    "        The number of sub reviews to include in sample\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rating_df, sampled_rating_df: `tuple`\n",
    "        All reviews with that rating and the subset reviews with that rating\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_df = df[df[col_name] == star_value]\n",
    "    sampled_rating_df = rating_df.sample(n=number_of_reviews)\n",
    "    \n",
    "    return rating_df, sampled_rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Be sure to change the # of reviews per class when you run. My kernel dies after trying many solutions, including those mentioned in [Piazza post 269](https://piazza.com/class/lr17bka75121t1/post/269)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_reviews = 50000\n",
    "subset_reviews = 30000\n",
    "\n",
    "one_star = 1\n",
    "rating_one, rating_one_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', one_star, subset_reviews)\n",
    "two_stars = 2\n",
    "rating_two, rating_two_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', two_stars, subset_reviews)\n",
    "three_stars = 3\n",
    "rating_three, rating_three_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', three_stars, subset_reviews)\n",
    "four_stars = 4\n",
    "rating_four, rating_four_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', four_stars, subset_reviews)\n",
    "five_stars = 5\n",
    "rating_five, rating_five_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', five_stars, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df = pd.concat([rating_one_sampled, rating_two_sampled, rating_three_sampled, rating_four_sampled, rating_five_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews_by_rating(df: pd.DataFrame, rating_col: str, threshold: int, sentiment_type: str):\n",
    "    \"\"\"Categorizes reviews by adding a rating\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    rating_col: `str`\n",
    "        Column with rating values\n",
    "    \n",
    "    threshold: `int`\n",
    "        Where to split the ratings such that categories can be formed\n",
    "\n",
    "    sentiment_type: `str`\n",
    "        One of three types of sentiment: positive, negative, or neural\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if sentiment_type == 'positive_review_class':\n",
    "        positive_review_threshold = df[rating_col].astype('int32') > threshold\n",
    "        df = df[positive_review_threshold]\n",
    "        df[sentiment_type] = 1\n",
    "\n",
    "    elif sentiment_type == 'negative_review_class':\n",
    "        negative_review_threshold = df[rating_col].astype('int32') < threshold\n",
    "        df = df[negative_review_threshold]\n",
    "        df[sentiment_type] = 2\n",
    "\n",
    "    elif sentiment_type == 'neutral_review_class':\n",
    "        neutral_review_threshold = df[rating_col].astype('int32') == threshold\n",
    "        df = df[neutral_review_threshold]\n",
    "        df[sentiment_type] = 3\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/1400069123.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 2\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/1400069123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 3\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/1400069123.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 1\n"
     ]
    }
   ],
   "source": [
    "negative_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'negative_review_class')\n",
    "neutral_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'neutral_review_class')\n",
    "positive_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'positive_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df = pd.concat([negative_review_class_df, neutral_review_class_df, positive_review_class_df])\n",
    "# sampled_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_df = sampled_reviews_ratings_df['negative_review_class'].dropna()\n",
    "neutral_reviews_df = sampled_reviews_ratings_df['neutral_review_class'].dropna()\n",
    "positive_reviews_df = sampled_reviews_ratings_df['positive_review_class'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'] = pd.concat([negative_reviews_df, positive_reviews_df])\n",
    "sampled_reviews_ratings_df['ternary_review_class'] = pd.concat([negative_reviews_df, neutral_reviews_df, positive_reviews_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sampled_reviews_ratings_df['binary_review_class'].unique())\n",
    "# print(sampled_reviews_ratings_df['ternary_review_class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean data steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_lower_case(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Convert all reviews to lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the lower cased reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_case_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    \n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        # print(text_reviews_idx, type(text_review), text_review)\n",
    "\n",
    "        # NOT all reviews are strings, thus all can't be converted to lower cased\n",
    "        if type(text_review) != str:\n",
    "            print(True, text_review)\n",
    "            converted_str = str(text_review)\n",
    "            lower_case_reviews.append(text_review)\n",
    "         \n",
    "        else:\n",
    "            update_text_review = text_review.lower()\n",
    "            lower_case_reviews.append(update_text_review)\n",
    "\n",
    "    updated_df['lower_cased'] = lower_case_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased = convert_reviews_to_lower_case(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"reviews_lower_cased:\")\n",
    "# generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove HTML and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_urls(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove HTML and URLs from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the html_and_urls removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\. \\S+')\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            # Check and remove HTML tags\n",
    "            has_html = bool(re.search('<.*?>', text_review))\n",
    "            if has_html == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "\n",
    "            no_html_review = re.sub('<.*?>', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML -- \", no_html_review)\n",
    "        \n",
    "            # Check and remove URLs\n",
    "            has_url = bool(re.search(r'http\\S+', no_html_review))\n",
    "            if has_url == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has URL --\", no_html_review)\n",
    "                pass\n",
    "\n",
    "            no_html_url_review = re.sub(r'http\\S+', '', no_html_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML, URL -- \", no_html_url_review)\n",
    "            # print()\n",
    "            cleaned_reviews.append(no_html_url_review)\n",
    "        else:\n",
    "            # print(text_reviews_idx, text_review)\n",
    "            cleaned_reviews.append(text_review)\n",
    "            \n",
    "\n",
    "    updated_df['without_html_urls'] = cleaned_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_html_urls:\")\n",
    "# generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how'd\": \"how would\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_replace_contractions(review):\n",
    "    \"\"\"Find the contractions to replace from a specific review\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    review: `str`\n",
    "        A specific review\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    non_contraction_review: `str`\n",
    "        The updated specific review with contractions expanded\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(review, str):\n",
    "        get_words = review.split()\n",
    "\n",
    "        store_non_contraction_words = []\n",
    "\n",
    "        for word in get_words:\n",
    "            if word in store_contractions:\n",
    "                non_contraction_form = store_contractions[word]\n",
    "                # print(word, \"-->\", non_contraction_form)\n",
    "\n",
    "                store_non_contraction_words.append(non_contraction_form)\n",
    "\n",
    "            else:\n",
    "                # print(word)\n",
    "                store_non_contraction_words.append(word)\n",
    "\n",
    "        non_contraction_review = ' '.join(store_non_contraction_words)\n",
    "        return non_contraction_review\n",
    "    else:\n",
    "        return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove contractions from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_contractions_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"with possible contraction(s) -- \", text_review)\n",
    "\n",
    "        without_contraction = locate_and_replace_contractions(text_review)\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"without contraction -- \", without_contraction)\n",
    "        # print()\n",
    "\n",
    "        without_contractions_reviews.append(without_contraction)\n",
    "\n",
    "    updated_df['without_contractions'] = without_contractions_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_contractions:\")\n",
    "# generate_sample_reviews(no_contractions_df, 'without_contractions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove Non-alphabetical characters from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the non-alphabetical characters removed\n",
    "    \"\"\"\n",
    "\n",
    "    alphabetical_char_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        \n",
    "        if isinstance(text_review, str):\n",
    "\n",
    "            # Check for non-alphabetical characters\n",
    "            has_non_alphabetical_char = bool(re.search(r'[^a-zA-Z]', text_review))\n",
    "            if has_non_alphabetical_char == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove non-alphabetical characters\n",
    "            with_alphabetical_char = re.sub(r'[^a-zA-Z\\s]', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"has HTML -- \", with_alphabetical_char)\n",
    "            alphabetical_char_reviews.append(with_alphabetical_char)\n",
    "        else:\n",
    "            alphabetical_char_reviews.append(text_review)\n",
    "\n",
    "    updated_df['with_alpha_chars_only'] = alphabetical_char_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"with_alpha_chars_only:\")\n",
    "# generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove extra spaces from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    single_spaced_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "        # Check if there are any extra spaces\n",
    "            has_extra_space = bool(re.search(r' +', text_review))\n",
    "            if has_extra_space == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has extra space -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            single_spaced_review = re.sub(r' +', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without extra space -- \", single_spaced_review)\n",
    "            # print()\n",
    "            \n",
    "            single_spaced_reviews.append(single_spaced_review)\n",
    "        else:\n",
    "            single_spaced_reviews.append(text_review)\n",
    "\n",
    "    updated_df['without_extra_space'] = single_spaced_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_extra_space:\")\n",
    "# generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Filter stop words out from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_stop_words_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            text_review_words = word_tokenize(text_review) \n",
    "\n",
    "        \n",
    "\n",
    "            # print(\"Before stop word removal\", text_reviews_idx, \" -- \", text_review)\n",
    "\n",
    "            filtered_review = []\n",
    "\n",
    "            for text_review_words_idx in range(len(text_review_words)):\n",
    "                text_review_word = text_review_words[text_review_words_idx]\n",
    "                \n",
    "                # Check if review word is a stop word\n",
    "                if text_review_word in stop_words:\n",
    "                    # print(\"  Stop word -- \", text_review_word)\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(text_review_word, \" -- is NOT a stop word in review\")\n",
    "                    filtered_review.append(text_review_word)\n",
    "\n",
    "            \n",
    "            filtered_review = \" \".join(filtered_review)\n",
    "            # print(\"After stop word removal\", text_reviews_idx, \" -- \", filtered_review)\n",
    "            # print()\n",
    "            \n",
    "            without_stop_words_reviews.append(filtered_review)\n",
    "        else:\n",
    "            without_stop_words_reviews.append(text_review)\n",
    "        \n",
    "\n",
    "    updated_df['without_stop_words'] = without_stop_words_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_stop_words:\")\n",
    "# generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform lemmatization  \n",
    "\n",
    "- \"A sentence with many words\"\n",
    "    - \"words\" -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmentize_review(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Lemmentize all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmed_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]   \n",
    "        if isinstance(text_review, str):     \n",
    "            words_in_review = word_tokenize(text_review) \n",
    "\n",
    "            # print(\"Before lem update\", text_reviews_idx, \" -- \", text_review)\n",
    "            # print(\"Lemmed words\", words_in_review)\n",
    "            \n",
    "\n",
    "            lemmed_sentence = []\n",
    "\n",
    "            # Split review into words\n",
    "            for lemmed_words_idx in range(len(words_in_review)):\n",
    "                word = words_in_review[lemmed_words_idx]\n",
    "                \n",
    "                apply_lemmatization = lem.lemmatize(word)\n",
    "                # print(apply_lemmatization)\n",
    "                \n",
    "                lemmed_sentence.append(apply_lemmatization)\n",
    "                filtered_review = \" \".join(lemmed_sentence)\n",
    "        \n",
    "            # print(\"After lem update -- \", filtered_review)\n",
    "            # print()\n",
    "\n",
    "            lemmed_reviews.append(filtered_review)\n",
    "        else:\n",
    "            lemmed_reviews.append(text_review)\n",
    "\n",
    "    updated_df['lemmed_reviews'] = lemmed_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_unlemmed_words:\")\n",
    "# generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Clean data function\n",
    "\n",
    "- Implement all the preprocessing steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, col_name):\n",
    "    \"\"\"Perform lower case, remove HTML and URLs, remove contractions, remove non-alphabetical characters, remove extra spaces, remove stop words, and lemmatize\"\"\"\n",
    "\n",
    "    print(\"original reviews:\")\n",
    "    # generate_sample_reviews(df, col_name, 3)\n",
    "\n",
    "    reviews_lower_cased = convert_reviews_to_lower_case(df, col_name)\n",
    "    print(\"reviews_lower_cased:\")\n",
    "    # generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)\n",
    "\n",
    "    no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')\n",
    "    print(\"without_html_urls:\")\n",
    "    # generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)\n",
    "\n",
    "    no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')\n",
    "    print(\"without_contractions:\")\n",
    "    # generate_sample_reviews(no_contractions_df, 'without_contractions', 3)\n",
    "\n",
    "    only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')\n",
    "    print(\"with_alpha_chars_only:\")\n",
    "    # generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)\n",
    "\n",
    "    no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')\n",
    "    print(\"without_extra_space:\")\n",
    "    # generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)\n",
    "\n",
    "    no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')\n",
    "    print(\"without_stop_words:\")\n",
    "    # generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)\n",
    "    \n",
    "    lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')\n",
    "    print(\"without_unlemmed_words:\")\n",
    "    # print(lemmed_df[\"ternary_review_class\"].unique())\n",
    "    # generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)\n",
    "\n",
    "    return lemmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original reviews:\n",
      "reviews_lower_cased:\n",
      "without_html_urls:\n",
      "without_contractions:\n",
      "with_alpha_chars_only:\n",
      "without_extra_space:\n",
      "without_stop_words:\n",
      "without_unlemmed_words:\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews_df = preprocess_data(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. nan  1.]\n",
      "[2. 3. 1.]\n",
      "[ 2. nan  1.]\n",
      "[2. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_reviews_ratings_df['binary_review_class'].unique())\n",
    "print(sampled_reviews_ratings_df['ternary_review_class'].unique())\n",
    "\n",
    "print(cleaned_reviews_df['binary_review_class'].unique())\n",
    "print(cleaned_reviews_df['ternary_review_class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, review_class):\n",
    "    embeddings_df = df.dropna(subset=[review_class])\n",
    "    # print(len(embeddings_df), embeddings_df['star_rating'].unique())\n",
    "\n",
    "    specific_review_class = embeddings_df[review_class]\n",
    "    # print(specific_review_class.unique())\n",
    "\n",
    "    text = embeddings_df.loc[:, ['lemmed_reviews', 'star_rating', review_class]]\n",
    "    # print(text)\n",
    "\n",
    "    ### Train test split so I can have the same train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, specific_review_class, test_size=0.2, random_state=42)\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Binary\\n\")\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = split_data(cleaned_reviews_df, 'binary_review_class')\n",
    "# print(\"\\nTernary\")\n",
    "ternary_X_train, ternary_X_test, ternary_y_train, ternary_y_test = split_data(cleaned_reviews_df, 'ternary_review_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, col_name: str):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: `pd.DataFrame`\n",
    "            The data\n",
    "        \n",
    "        col_name: `str`\n",
    "            Column with reviews\n",
    "\n",
    "        words_in_model: `list`\n",
    "            Words in Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text_reviews = self.df[self.col_name].values\n",
    "\n",
    "        for text_reviews_idx in range(len(text_reviews)):\n",
    "            text_review = text_reviews[text_reviews_idx]\n",
    "            # print(text_reviews_idx, \"--\", text_review)\n",
    "\n",
    "            yield utils.simple_preprocess(text_review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 00:42:39,963 : INFO : collecting all words and their counts\n",
      "2024-02-10 00:42:39,965 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 00:42:40,419 : INFO : PROGRESS: at sentence #10000, processed 296605 words, keeping 13889 word types\n",
      "2024-02-10 00:42:40,792 : INFO : PROGRESS: at sentence #20000, processed 591741 words, keeping 18973 word types\n",
      "2024-02-10 00:42:41,189 : INFO : PROGRESS: at sentence #30000, processed 902184 words, keeping 23041 word types\n",
      "2024-02-10 00:42:41,631 : INFO : PROGRESS: at sentence #40000, processed 1204777 words, keeping 26451 word types\n",
      "2024-02-10 00:42:42,019 : INFO : PROGRESS: at sentence #50000, processed 1500351 words, keeping 29399 word types\n",
      "2024-02-10 00:42:42,408 : INFO : PROGRESS: at sentence #60000, processed 1804429 words, keeping 32247 word types\n",
      "2024-02-10 00:42:42,801 : INFO : PROGRESS: at sentence #70000, processed 2105259 words, keeping 34673 word types\n",
      "2024-02-10 00:42:43,190 : INFO : PROGRESS: at sentence #80000, processed 2405034 words, keeping 37093 word types\n",
      "2024-02-10 00:42:43,585 : INFO : PROGRESS: at sentence #90000, processed 2710579 words, keeping 39328 word types\n",
      "2024-02-10 00:42:43,839 : INFO : collected 40549 word types from a corpus of 2900544 raw words and 96000 sentences\n",
      "2024-02-10 00:42:43,840 : INFO : Creating a fresh vocabulary\n",
      "2024-02-10 00:42:43,867 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 9350 unique words (23.06% of original 40549, drops 31199)', 'datetime': '2024-02-10T00:42:43.867529', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:42:43,868 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 2833242 word corpus (97.68% of original 2900544, drops 67302)', 'datetime': '2024-02-10T00:42:43.868209', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:42:43,908 : INFO : deleting the raw counts dictionary of 40549 items\n",
      "2024-02-10 00:42:43,909 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2024-02-10 00:42:43,909 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2604818.51971769 word corpus (91.9%% of prior 2833242)', 'datetime': '2024-02-10T00:42:43.909634', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:42:43,977 : INFO : estimated required memory for 9350 words and 300 dimensions: 27115000 bytes\n",
      "2024-02-10 00:42:43,978 : INFO : resetting layer weights\n",
      "2024-02-10 00:42:43,989 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-10T00:42:43.989436', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-10 00:42:43,990 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 9350 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-10T00:42:43.990064', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-10 00:42:45,019 : INFO : EPOCH 0 - PROGRESS: at 16.53% examples, 411511 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:42:46,029 : INFO : EPOCH 0 - PROGRESS: at 34.11% examples, 434344 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:47,035 : INFO : EPOCH 0 - PROGRESS: at 52.39% examples, 446188 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-10 00:42:48,051 : INFO : EPOCH 0 - PROGRESS: at 70.53% examples, 451300 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:49,057 : INFO : EPOCH 0 - PROGRESS: at 89.28% examples, 457027 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:42:49,561 : INFO : EPOCH 0: training on 2900544 raw words (2604715 effective words) took 5.6s, 468090 effective words/s\n",
      "2024-02-10 00:42:50,572 : INFO : EPOCH 1 - PROGRESS: at 17.29% examples, 436965 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:51,586 : INFO : EPOCH 1 - PROGRESS: at 34.09% examples, 437545 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:42:52,606 : INFO : EPOCH 1 - PROGRESS: at 52.06% examples, 443417 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:53,634 : INFO : EPOCH 1 - PROGRESS: at 68.76% examples, 439106 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:54,653 : INFO : EPOCH 1 - PROGRESS: at 88.56% examples, 451399 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:55,237 : INFO : EPOCH 1: training on 2900544 raw words (2605176 effective words) took 5.7s, 459509 effective words/s\n",
      "2024-02-10 00:42:56,270 : INFO : EPOCH 2 - PROGRESS: at 17.62% examples, 439522 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:57,275 : INFO : EPOCH 2 - PROGRESS: at 36.12% examples, 462878 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:42:58,292 : INFO : EPOCH 2 - PROGRESS: at 54.20% examples, 460717 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:42:59,302 : INFO : EPOCH 2 - PROGRESS: at 71.25% examples, 456224 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:00,317 : INFO : EPOCH 2 - PROGRESS: at 88.90% examples, 454892 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:00,844 : INFO : EPOCH 2: training on 2900544 raw words (2604714 effective words) took 5.6s, 465718 effective words/s\n",
      "2024-02-10 00:43:01,879 : INFO : EPOCH 3 - PROGRESS: at 18.26% examples, 464523 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:02,914 : INFO : EPOCH 3 - PROGRESS: at 35.81% examples, 455469 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:03,916 : INFO : EPOCH 3 - PROGRESS: at 54.20% examples, 461021 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:04,953 : INFO : EPOCH 3 - PROGRESS: at 71.58% examples, 455756 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:05,955 : INFO : EPOCH 3 - PROGRESS: at 89.63% examples, 457312 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:06,464 : INFO : EPOCH 3: training on 2900544 raw words (2605115 effective words) took 5.6s, 466309 effective words/s\n",
      "2024-02-10 00:43:07,476 : INFO : EPOCH 4 - PROGRESS: at 15.48% examples, 391860 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:08,478 : INFO : EPOCH 4 - PROGRESS: at 32.41% examples, 417648 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:09,485 : INFO : EPOCH 4 - PROGRESS: at 50.35% examples, 431889 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:10,499 : INFO : EPOCH 4 - PROGRESS: at 68.76% examples, 443167 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:11,514 : INFO : EPOCH 4 - PROGRESS: at 86.52% examples, 444348 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:12,192 : INFO : EPOCH 4: training on 2900544 raw words (2604383 effective words) took 5.7s, 455185 effective words/s\n",
      "2024-02-10 00:43:12,193 : INFO : Word2Vec lifecycle event {'msg': 'training on 14502720 raw words (13024103 effective words) took 28.2s, 461808 effective words/s', 'datetime': '2024-02-10T00:43:12.193096', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-10 00:43:12,193 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=9350, vector_size=300, alpha=0.025>', 'datetime': '2024-02-10T00:43:12.193484', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-10 00:43:12,194 : INFO : collecting all words and their counts\n",
      "2024-02-10 00:43:12,195 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ternary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 00:43:12,664 : INFO : PROGRESS: at sentence #10000, processed 302049 words, keeping 13973 word types\n",
      "2024-02-10 00:43:13,064 : INFO : PROGRESS: at sentence #20000, processed 596554 words, keeping 19241 word types\n",
      "2024-02-10 00:43:13,497 : INFO : PROGRESS: at sentence #30000, processed 900299 words, keeping 23254 word types\n",
      "2024-02-10 00:43:13,926 : INFO : PROGRESS: at sentence #40000, processed 1206584 words, keeping 26570 word types\n",
      "2024-02-10 00:43:14,359 : INFO : PROGRESS: at sentence #50000, processed 1510406 words, keeping 29556 word types\n",
      "2024-02-10 00:43:14,772 : INFO : PROGRESS: at sentence #60000, processed 1816969 words, keeping 32178 word types\n",
      "2024-02-10 00:43:15,173 : INFO : PROGRESS: at sentence #70000, processed 2121846 words, keeping 34702 word types\n",
      "2024-02-10 00:43:15,584 : INFO : PROGRESS: at sentence #80000, processed 2430612 words, keeping 37023 word types\n",
      "2024-02-10 00:43:16,029 : INFO : PROGRESS: at sentence #90000, processed 2739666 words, keeping 39258 word types\n",
      "2024-02-10 00:43:16,449 : INFO : PROGRESS: at sentence #100000, processed 3052522 words, keeping 41264 word types\n",
      "2024-02-10 00:43:16,904 : INFO : PROGRESS: at sentence #110000, processed 3364008 words, keeping 43263 word types\n",
      "2024-02-10 00:43:17,331 : INFO : collected 45208 word types from a corpus of 3675717 raw words and 120000 sentences\n",
      "2024-02-10 00:43:17,331 : INFO : Creating a fresh vocabulary\n",
      "2024-02-10 00:43:17,363 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 10434 unique words (23.08% of original 45208, drops 34774)', 'datetime': '2024-02-10T00:43:17.363262', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:43:17,363 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 3601483 word corpus (97.98% of original 3675717, drops 74234)', 'datetime': '2024-02-10T00:43:17.363902', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:43:17,408 : INFO : deleting the raw counts dictionary of 45208 items\n",
      "2024-02-10 00:43:17,409 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2024-02-10 00:43:17,410 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3319653.3511592466 word corpus (92.2%% of prior 3601483)', 'datetime': '2024-02-10T00:43:17.410348', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-10 00:43:17,490 : INFO : estimated required memory for 10434 words and 300 dimensions: 30258600 bytes\n",
      "2024-02-10 00:43:17,490 : INFO : resetting layer weights\n",
      "2024-02-10 00:43:17,502 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-10T00:43:17.502936', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-10 00:43:17,503 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 10434 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-10T00:43:17.503505', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-10 00:43:18,533 : INFO : EPOCH 0 - PROGRESS: at 12.97% examples, 414804 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:19,533 : INFO : EPOCH 0 - PROGRESS: at 27.58% examples, 444988 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:20,548 : INFO : EPOCH 0 - PROGRESS: at 41.94% examples, 452946 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:21,566 : INFO : EPOCH 0 - PROGRESS: at 55.98% examples, 454433 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:22,593 : INFO : EPOCH 0 - PROGRESS: at 69.70% examples, 452873 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:23,598 : INFO : EPOCH 0 - PROGRESS: at 83.90% examples, 456520 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:24,601 : INFO : EPOCH 0 - PROGRESS: at 98.76% examples, 462773 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:24,628 : INFO : EPOCH 0: training on 3675717 raw words (3320034 effective words) took 7.1s, 466905 effective words/s\n",
      "2024-02-10 00:43:25,673 : INFO : EPOCH 1 - PROGRESS: at 14.09% examples, 446498 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:26,693 : INFO : EPOCH 1 - PROGRESS: at 29.17% examples, 465116 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:27,724 : INFO : EPOCH 1 - PROGRESS: at 44.08% examples, 469830 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:28,733 : INFO : EPOCH 1 - PROGRESS: at 58.72% examples, 472569 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:29,754 : INFO : EPOCH 1 - PROGRESS: at 72.45% examples, 467963 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:30,784 : INFO : EPOCH 1 - PROGRESS: at 86.48% examples, 467121 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:31,656 : INFO : EPOCH 1: training on 3675717 raw words (3319398 effective words) took 7.0s, 473793 effective words/s\n",
      "2024-02-10 00:43:32,700 : INFO : EPOCH 2 - PROGRESS: at 10.08% examples, 319197 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:33,706 : INFO : EPOCH 2 - PROGRESS: at 24.83% examples, 395304 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:34,739 : INFO : EPOCH 2 - PROGRESS: at 39.48% examples, 419955 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:35,756 : INFO : EPOCH 2 - PROGRESS: at 54.65% examples, 438503 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-10 00:43:36,760 : INFO : EPOCH 2 - PROGRESS: at 68.38% examples, 442179 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:37,777 : INFO : EPOCH 2 - PROGRESS: at 83.35% examples, 451040 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:38,778 : INFO : EPOCH 2 - PROGRESS: at 96.97% examples, 451840 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:38,908 : INFO : EPOCH 2: training on 3675717 raw words (3319901 effective words) took 7.2s, 458220 effective words/s\n",
      "2024-02-10 00:43:39,943 : INFO : EPOCH 3 - PROGRESS: at 13.82% examples, 438214 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:40,954 : INFO : EPOCH 3 - PROGRESS: at 28.35% examples, 454195 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:41,962 : INFO : EPOCH 3 - PROGRESS: at 42.49% examples, 457224 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:42,968 : INFO : EPOCH 3 - PROGRESS: at 56.48% examples, 458999 words/s, in_qsize 5, out_qsize 1\n",
      "2024-02-10 00:43:43,984 : INFO : EPOCH 3 - PROGRESS: at 71.38% examples, 464641 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:44,997 : INFO : EPOCH 3 - PROGRESS: at 85.49% examples, 465710 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:45,940 : INFO : EPOCH 3: training on 3675717 raw words (3319712 effective words) took 7.0s, 472971 effective words/s\n",
      "2024-02-10 00:43:46,967 : INFO : EPOCH 4 - PROGRESS: at 13.22% examples, 421379 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:47,976 : INFO : EPOCH 4 - PROGRESS: at 28.10% examples, 450667 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:48,994 : INFO : EPOCH 4 - PROGRESS: at 42.21% examples, 453202 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:50,017 : INFO : EPOCH 4 - PROGRESS: at 56.25% examples, 454118 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:51,032 : INFO : EPOCH 4 - PROGRESS: at 70.27% examples, 455474 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-10 00:43:52,035 : INFO : EPOCH 4 - PROGRESS: at 84.94% examples, 461769 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-10 00:43:53,050 : INFO : EPOCH 4 - PROGRESS: at 99.29% examples, 463952 words/s, in_qsize 3, out_qsize 0\n",
      "2024-02-10 00:43:53,063 : INFO : EPOCH 4: training on 3675717 raw words (3319429 effective words) took 7.1s, 466416 effective words/s\n",
      "2024-02-10 00:43:53,064 : INFO : Word2Vec lifecycle event {'msg': 'training on 18378585 raw words (16598474 effective words) took 35.6s, 466770 effective words/s', 'datetime': '2024-02-10T00:43:53.064429', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-10 00:43:53,064 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10434, vector_size=300, alpha=0.025>', 'datetime': '2024-02-10T00:43:53.064853', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Case\")\n",
    "binary_X_train_sentences = MyCorpus(binary_X_train, 'lemmed_reviews')\n",
    "my_binary_X_train_model = gensim.models.Word2Vec(sentences=binary_X_train_sentences, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "print(\"\\nTernary Case\")\n",
    "ternary_X_train_sentences = MyCorpus(ternary_X_train, 'lemmed_reviews')\n",
    "my_ternary_X_train_model = gensim.models.Word2Vec(sentences=ternary_X_train_sentences, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings\n",
    "\n",
    "- Word embeddings [vector representation of each word]\n",
    "- TUTORIAL: [Word2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    "    - Follow for the purpose of using the Gensimm library\n",
    "---\n",
    "\n",
    "## (a) Pretrained Word2Vec\n",
    "\n",
    "- [x] Load the pretrained “word2vec-google-news-300” Word2Vec model\n",
    "- [x] Extract word embeddings (per word)\n",
    "- [x] Check semantic similarities (ie: (1) King − Man + Woman = Queen, (2) excellent ∼ outstanding) of my own\n",
    "\n",
    "## (b) My trained Word2Vec\n",
    "- [x] Train a Word2Vec model using my own dataset\n",
    "    - [x] Set the embedding size to be 300\n",
    "    - [x] Set the window size to be 11\n",
    "    - [x] Consider a minimum word count of 10\n",
    "- [x] Check semantic similarities\n",
    "    - [x] What do you conclude from comparing vectors generated by yourself and the pretrained model? My model doesn't perform as well as the pretrained because the pretrained has trained on more data compared to my model. Thus, my model don't have sufficient training compared to the pretrained model.\n",
    "    - [x] Which of the Word2Vec models seems to encode semantic similarities between words better? The pretrained model because of the numerous data sources it has been trained on. I've only trained mine on this one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = pretrained_word_two_vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x220d07dd0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_X_train_model = my_binary_X_train_model.wv\n",
    "my_trained_binary_X_train_model\n",
    "\n",
    "my_trained_ternary_X_train_model = my_ternary_X_train_model.wv\n",
    "my_trained_ternary_X_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, word in enumerate(my_trained_binary_X_train_model.index_to_key):\n",
    "#     if index == 20:\n",
    "#         break\n",
    "#     print(f\"word #{index}/{len(my_trained_binary_X_train_model.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, word in enumerate(my_trained_ternary_X_train_model.index_to_key):\n",
    "#     if index == 50:\n",
    "#         break\n",
    "#     print(f\"word #{index}/{len(my_trained_ternary_X_train_model.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cartriges', 0.7031320333480835), ('cartidges', 0.693719208240509), ('ink', 0.6915202736854553), ('inkjet', 0.6712480187416077), ('refilled', 0.6340457797050476), ('jet', 0.6265697479248047), ('magenta', 0.625919759273529), ('cyan', 0.6182397603988647), ('xl', 0.6165218353271484), ('tank', 0.6153622269630432)]\n",
      "\n",
      "[('feather', 0.663263201713562), ('ballpoint', 0.5980835556983948), ('nib', 0.5847675800323486), ('tipped', 0.5815163850784302), ('bleed', 0.5780794024467468), ('ball', 0.577994704246521), ('writing', 0.575954794883728), ('barrel', 0.5729708671569824), ('notepad', 0.5657536387443542), ('scribble', 0.5600907206535339)]\n"
     ]
    }
   ],
   "source": [
    "my_binary_similarity_score = my_trained_binary_X_train_model.most_similar(positive=['printer', 'cartridge'], negative=['phone'], topn=10)\n",
    "print(my_binary_similarity_score)\n",
    "\n",
    "print()\n",
    "\n",
    "my_ternary_similarity_score = my_trained_ternary_X_train_model.most_similar(positive=['pen', 'paper'], negative=['hp'], topn=10)\n",
    "print(my_ternary_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mean_sentences_vectorized\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "    print(len(mean_sentences_vectorized))\n",
    "    return mean_sentences_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "96000\n",
      "Binary Pretrained Test\n",
      "24000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((96000, 300), (24000, 300))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_embeddings = np.array(pretrained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_embeddings = np.array(pretrained_binary_test_embeddings)\n",
    "\n",
    "pretrained_binary_train_embeddings.shape, pretrained_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "120000\n",
      "Ternary Pretrained Test\n",
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((120000, 300), (30000, 300))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_embeddings = np.array(pretrained_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary Pretrained Test\")\n",
    "pretrained_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_embeddings = np.array(pretrained_ternary_test_embeddings)\n",
    "\n",
    "pretrained_ternary_train_embeddings.shape, pretrained_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "96000\n",
      "Binary My Model Test\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_trained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_train_embeddings = np.array(my_trained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_trained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_test_embeddings = np.array(my_trained_binary_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "120000\n",
      "Ternary My Model Test\n",
      "30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((120000, 300), (30000, 300))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_train_embeddings = np.array(my_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary My Model Test\")\n",
    "my_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_test_embeddings = np.array(my_ternary_test_embeddings)\n",
    "\n",
    "my_ternary_train_embeddings.shape, my_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_word_embeddings(df: pd.DataFrame, col_name: str, model_to_use, about: str):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    aboout: `str`\n",
    "        Specifics of model, classification, and train/test\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    concatenated_vectors: list\n",
    "        List of concatenated vectors for each review (first 10 Word2Vec vectors)\n",
    "    \"\"\"\n",
    "    print(\"About:\", about)\n",
    "    mean_concatenated_vectors = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vectorized_words = []\n",
    "        words = sentence.split(\" \")[:10]  # Select the first 10 words\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "            vectorized_words.append(vector_of_word)\n",
    "        \n",
    "        concatenated_features = np.concatenate(vectorized_words, axis=0)\n",
    "        # Ensure dimensionality of 300 for each sentence\n",
    "        if concatenated_features.shape[0] < 300:\n",
    "            # If concatenated_features has less than 300 dimensions, pad it with zeros\n",
    "            concatenated_features = np.pad(concatenated_features, ((0, 300 - concatenated_features.shape[0]), (0, 0)), mode='constant')\n",
    "        elif concatenated_features.shape[0] > 300:\n",
    "            # If concatenated_features has more than 300 dimensions, truncate it\n",
    "            concatenated_features = concatenated_features[:300 :]\n",
    "            \n",
    "        mean_concatenated_vectors.append(concatenated_features)\n",
    "\n",
    "    mean_concatenated_vectors = np.array(mean_concatenated_vectors)\n",
    "    print(\"   Concat embeddings shape --- \", mean_concatenated_vectors.shape)\n",
    "    print()\n",
    "    return mean_concatenated_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary ---\n",
      "About: Pretrained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (96000, 300)\n",
      "\n",
      "About: Pretrained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (24000, 300)\n",
      "\n",
      "About: My trained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (96000, 300)\n",
      "\n",
      "About: My trained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (24000, 300)\n",
      "\n",
      "--- Ternary ---\n",
      "About: Pretrained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (120000, 300)\n",
      "\n",
      "About: Pretrained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (30000, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (120000, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (30000, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Binary ---\")\n",
    "pretrained_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")\n",
    "\n",
    "my_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Binary --- Test\")\n",
    "\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")\n",
    "\n",
    "my_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_feature_extraction(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Extract the TF-IDF features from the reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tf_idf_features:\n",
    "        A matrix containing the TF-IDF features extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_features = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    return tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_df = cleaned_reviews_df.dropna(subset=['binary_review_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features = tf_idf_feature_extraction(cleaned_reviews_df, 'lemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x45152 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binnary_reviews = cleaned_reviews_df['binary_review_class']\n",
    "binnary_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tf_idf_features, binnary_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models\n",
    "\n",
    "**GOAL:** Train simple models below, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "\n",
    "- [x] Train a perceptron and report accuracy on the testing split for\n",
    "    - [x] Pretrained average embeddings\n",
    "    - [x] My trained average embeddings\n",
    "    - [x] TF-IDF embeddings\n",
    "- [x] Train a support vector machine (SVM) and report accuracy on the testing split for\n",
    "    - [x] Pretrained average embeddings\n",
    "    - [x] My trained average embeddings\n",
    "    - [x] TF-IDF embeddings\n",
    "- [x] Compare\n",
    "    - [x] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? --- The performances are quite interesting as TF-IDF don't capture semanitc meaning. The pretrained model should perform better as it has a slew sources it was trained on. My model doesn't perform as well because of the limted data sourcess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results_dict, w2v_type, method, classification, accuracy):\n",
    "    \"\"\"\n",
    "    Store results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    results_dict (dict): Dictionary to store the results.\n",
    "    w2v_type (str): Type of word2vec.\n",
    "    method (str): Method used.\n",
    "    classification (str): Type of classification.\n",
    "    accuracy (float): Accuracy of the classification.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the stored results.\n",
    "    \"\"\"\n",
    "    results_dict['W2V Type'].append(w2v_type)\n",
    "    results_dict['Method'].append(method)\n",
    "    results_dict['Classification'].append(classification)\n",
    "    results_dict['Accuracy'].append(accuracy)\n",
    "\n",
    "    return pd.DataFrame(results_dict)  # Return DataFrame with a single row\n",
    "\n",
    "results_dict = {'W2V Type': [], 'Method': [], 'Classification': [], 'Accuracy': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_prediction):\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_metric(y_train_true, y_train_predictions):\n",
    "    accuracy = eval_accuracy(y_train_true, y_train_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def test_eval_metric(y_test_true, y_test_predictions):\n",
    "    accuracy = eval_accuracy(y_test_true, y_test_predictions)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = Perceptron(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = perceptron_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       W2V Type          Method Classification  Accuracy\n",
      "0  TF-IDF-train  Perceptron-avg         Binary  0.887177\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'TF-IDF-train', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "perceptron_train_metrics, perceptron_test_metrics = perceptron_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained-train', 'Perceptron-avg', 'Binary', list(perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'Perceptron-avg', 'Binary', list(perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96000, 300), (24000, 300), (96000,), (24000,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_train_embeddings.shape, my_trained_binary_test_embeddings.shape, binary_y_train.shape, binary_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "my_perceptron_train_metrics, my_perceptron_test_metrics = perceptron_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.77840625}, {'Accuracy': 0.775875})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron_train_metrics, my_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'Perceptron-avg', 'Binary', list(my_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'Perceptron-avg', 'Binary', list(my_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = LinearSVC(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics = svm_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'TF-IDF-train', 'SVM-avg', 'Binary', list(tfidf_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'SVM-avg', 'Binary', list(tfidf_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics = svm_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained-train', 'SVM-avg', 'Binary', list(svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'SVM-avg', 'Binary', list(svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics = svm_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5    My model-train         SVM-avg         Binary  0.842896\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'SVM-avg', 'Binary', list(my_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'SVM-avg', 'Binary', list(my_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks (FFNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "- [x] Train a feedforward multilayer perceptron (MLP) network\n",
    "    - [x] 2 hidden layers each with 50 and 10 nodes, respectively\n",
    "    - [x] Cross entropy loss\n",
    "    - [x] Decide other hyperparameters (ie: nonlinearity, #epochs, etc)\n",
    "- [x] TUTORIAL: [Pytorch Multi-Layer Perceptron, MNIST](https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook) for image data.  Much of my code is based on this\n",
    "\n",
    "---\n",
    "\n",
    "## (a) Average Embeddings\n",
    " \n",
    "- [x] Train a FFNN and report accuracy on the testing split for on average embeddings for\n",
    "    - [x] Pretrained \n",
    "    - [x] My trained average embeddings\n",
    "- [x] Train a FFNN and report accuracy on the testing split for\n",
    "    - [x] Pretrained average embeddings\n",
    "    - [x] My trained average embeddings\n",
    "\n",
    "## (b) Concatenate Embeddings\n",
    "\n",
    "- [x] Concatenate the first 10 Word2Vec vectors for each review\n",
    "- [x] Compare\n",
    "    - [x] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? - All models are performing roughly the same. All have the same data and preprocesing steps and are expected to have some differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the NN architecture\"\"\"\n",
    "\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # self.height = heightght = height\n",
    "        # self.width = width\n",
    "        # linear layer (1200 x 300 dot 300 x 50 -> 1200 x 50)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes) # change to 3 for ternery\n",
    "        # dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add hidden layer, with relu activation function, dropout, relu, dropout, output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        # set initial \"min\" to infinity\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass to compute predictions, loss, backward pass to compute gradient wrt model params\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            # print(\"train_loss:\", train_loss)\n",
    "            \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1, \n",
    "                train_loss,\n",
    "                ))    \n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = self(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Convert predictions and ground truth lists to numpy arrays\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (predictions == ground_truth).mean()\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000 300\n"
     ]
    }
   ],
   "source": [
    "N_binary_embeddings, d_binary_embeddings = my_trained_binary_train_embeddings.shape\n",
    "print(N_binary_embeddings, d_binary_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 0)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    print(data_tensor.size(), target_tensor.size())\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "    \n",
    "    print(len(dataset))\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 25\n",
    "    # number_of_epochs = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 0)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "    # print(len(data_tensor))\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96000, 300]) torch.Size([96000])\n",
      "96000\n",
      "Epoch: 1 \tTraining Loss: 0.691902\n",
      "Epoch: 2 \tTraining Loss: 0.687561\n",
      "Epoch: 3 \tTraining Loss: 0.671988\n",
      "Epoch: 4 \tTraining Loss: 0.626534\n",
      "Epoch: 5 \tTraining Loss: 0.564425\n",
      "Epoch: 6 \tTraining Loss: 0.519339\n",
      "Epoch: 7 \tTraining Loss: 0.492821\n",
      "Epoch: 8 \tTraining Loss: 0.477713\n",
      "Epoch: 9 \tTraining Loss: 0.465888\n",
      "Epoch: 10 \tTraining Loss: 0.459025\n",
      "Epoch: 11 \tTraining Loss: 0.453925\n",
      "Epoch: 12 \tTraining Loss: 0.447706\n",
      "Epoch: 13 \tTraining Loss: 0.444809\n",
      "Epoch: 14 \tTraining Loss: 0.440996\n",
      "Epoch: 15 \tTraining Loss: 0.438127\n",
      "Epoch: 16 \tTraining Loss: 0.435787\n",
      "Epoch: 17 \tTraining Loss: 0.433824\n",
      "Epoch: 18 \tTraining Loss: 0.432851\n",
      "Epoch: 19 \tTraining Loss: 0.431443\n",
      "Epoch: 20 \tTraining Loss: 0.429387\n",
      "Epoch: 21 \tTraining Loss: 0.428593\n",
      "Epoch: 22 \tTraining Loss: 0.425560\n",
      "Epoch: 23 \tTraining Loss: 0.427217\n",
      "Epoch: 24 \tTraining Loss: 0.424653\n",
      "Epoch: 25 \tTraining Loss: 0.424510\n",
      "Accuracy: 0.8074583333333333\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_pretrained_binary = predict_binary_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5    My model-train         SVM-avg         Binary  0.842896\n",
      "6        Pretrained         FFN-avg         Binary  0.807458\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Binary', acc_avg_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96000, 300]) torch.Size([96000])\n",
      "96000\n",
      "Epoch: 1 \tTraining Loss: 0.456267\n",
      "Epoch: 2 \tTraining Loss: 0.397754\n",
      "Epoch: 3 \tTraining Loss: 0.387019\n",
      "Epoch: 4 \tTraining Loss: 0.380668\n",
      "Epoch: 5 \tTraining Loss: 0.377089\n",
      "Epoch: 6 \tTraining Loss: 0.375230\n",
      "Epoch: 7 \tTraining Loss: 0.369923\n",
      "Epoch: 8 \tTraining Loss: 0.368498\n",
      "Epoch: 9 \tTraining Loss: 0.366808\n",
      "Epoch: 10 \tTraining Loss: 0.364098\n",
      "Epoch: 11 \tTraining Loss: 0.360854\n",
      "Epoch: 12 \tTraining Loss: 0.360981\n",
      "Epoch: 13 \tTraining Loss: 0.358024\n",
      "Epoch: 14 \tTraining Loss: 0.358054\n",
      "Epoch: 15 \tTraining Loss: 0.357577\n",
      "Epoch: 16 \tTraining Loss: 0.354990\n",
      "Epoch: 17 \tTraining Loss: 0.353448\n",
      "Epoch: 18 \tTraining Loss: 0.353532\n",
      "Epoch: 19 \tTraining Loss: 0.351835\n",
      "Epoch: 20 \tTraining Loss: 0.349944\n",
      "Epoch: 21 \tTraining Loss: 0.349619\n",
      "Epoch: 22 \tTraining Loss: 0.348821\n",
      "Epoch: 23 \tTraining Loss: 0.349055\n",
      "Epoch: 24 \tTraining Loss: 0.348602\n",
      "Epoch: 25 \tTraining Loss: 0.347480\n",
      "Accuracy: 0.851875\n",
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5    My model-train         SVM-avg         Binary  0.842896\n",
      "6        Pretrained         FFN-avg         Binary  0.807458\n",
      "7          My model         FFN-avg         Binary  0.851875\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_trained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_my_binary = predict_binary_model(my_trained_binary_test_embeddings, binary_y_test)\n",
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Binary', acc_avg_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96000, 300]) torch.Size([96000])\n",
      "96000\n",
      "Epoch: 1 \tTraining Loss: 0.617726\n",
      "Epoch: 2 \tTraining Loss: 0.596769\n",
      "Epoch: 3 \tTraining Loss: 0.591873\n",
      "Epoch: 4 \tTraining Loss: 0.589122\n",
      "Epoch: 5 \tTraining Loss: 0.587285\n",
      "Epoch: 6 \tTraining Loss: 0.584908\n",
      "Epoch: 7 \tTraining Loss: 0.583824\n",
      "Epoch: 8 \tTraining Loss: 0.581230\n",
      "Epoch: 9 \tTraining Loss: 0.580465\n",
      "Epoch: 10 \tTraining Loss: 0.580320\n",
      "Epoch: 11 \tTraining Loss: 0.579628\n",
      "Epoch: 12 \tTraining Loss: 0.578631\n",
      "Epoch: 13 \tTraining Loss: 0.577550\n",
      "Epoch: 14 \tTraining Loss: 0.577159\n",
      "Epoch: 15 \tTraining Loss: 0.576061\n",
      "Epoch: 16 \tTraining Loss: 0.575634\n",
      "Epoch: 17 \tTraining Loss: 0.574742\n",
      "Epoch: 18 \tTraining Loss: 0.574751\n",
      "Epoch: 19 \tTraining Loss: 0.574097\n",
      "Epoch: 20 \tTraining Loss: 0.573412\n",
      "Epoch: 21 \tTraining Loss: 0.573946\n",
      "Epoch: 22 \tTraining Loss: 0.572160\n",
      "Epoch: 23 \tTraining Loss: 0.572430\n",
      "Epoch: 24 \tTraining Loss: 0.572233\n",
      "Epoch: 25 \tTraining Loss: 0.572549\n",
      "Accuracy: 0.684625\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_pretrained_binary = predict_binary_model(pretrained_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5    My model-train         SVM-avg         Binary  0.842896\n",
      "6        Pretrained         FFN-avg         Binary  0.807458\n",
      "7          My model         FFN-avg         Binary  0.851875\n",
      "8        Pretrained      FFN-concat         Binary  0.684625\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Binary', acc_concat_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96000, 300]) torch.Size([96000])\n",
      "96000\n",
      "Epoch: 1 \tTraining Loss: 0.618038\n",
      "Epoch: 2 \tTraining Loss: 0.595689\n",
      "Epoch: 3 \tTraining Loss: 0.588767\n",
      "Epoch: 4 \tTraining Loss: 0.584877\n",
      "Epoch: 5 \tTraining Loss: 0.583007\n",
      "Epoch: 6 \tTraining Loss: 0.580851\n",
      "Epoch: 7 \tTraining Loss: 0.579524\n",
      "Epoch: 8 \tTraining Loss: 0.578494\n",
      "Epoch: 9 \tTraining Loss: 0.577965\n",
      "Epoch: 10 \tTraining Loss: 0.576577\n",
      "Epoch: 11 \tTraining Loss: 0.576342\n",
      "Epoch: 12 \tTraining Loss: 0.574824\n",
      "Epoch: 13 \tTraining Loss: 0.575854\n",
      "Epoch: 14 \tTraining Loss: 0.574346\n",
      "Epoch: 15 \tTraining Loss: 0.574715\n",
      "Epoch: 16 \tTraining Loss: 0.573535\n",
      "Epoch: 17 \tTraining Loss: 0.573954\n",
      "Epoch: 18 \tTraining Loss: 0.572694\n",
      "Epoch: 19 \tTraining Loss: 0.572740\n",
      "Epoch: 20 \tTraining Loss: 0.572308\n",
      "Epoch: 21 \tTraining Loss: 0.572307\n",
      "Epoch: 22 \tTraining Loss: 0.571742\n",
      "Epoch: 23 \tTraining Loss: 0.572141\n",
      "Epoch: 24 \tTraining Loss: 0.571312\n",
      "Epoch: 25 \tTraining Loss: 0.571943\n",
      "Accuracy: 0.4989166666666667\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_my_binary = predict_binary_model(my_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2    My model-train  Perceptron-avg         Binary  0.778406\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4  Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5    My model-train         SVM-avg         Binary  0.842896\n",
      "6        Pretrained         FFN-avg         Binary  0.807458\n",
      "7          My model         FFN-avg         Binary  0.851875\n",
      "8        Pretrained      FFN-concat         Binary  0.684625\n",
      "9          My model      FFN-concat         Binary  0.498917\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Binary', acc_concat_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 20\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=1)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.900908\n",
      "Epoch: 2 \tTraining Loss: 0.842819\n",
      "Epoch: 3 \tTraining Loss: 0.829590\n",
      "Epoch: 4 \tTraining Loss: 0.821414\n",
      "Epoch: 5 \tTraining Loss: 0.815889\n",
      "Epoch: 6 \tTraining Loss: 0.811276\n",
      "Epoch: 7 \tTraining Loss: 0.808081\n",
      "Epoch: 8 \tTraining Loss: 0.804163\n",
      "Epoch: 9 \tTraining Loss: 0.800827\n",
      "Epoch: 10 \tTraining Loss: 0.798457\n",
      "Epoch: 11 \tTraining Loss: 0.794817\n",
      "Epoch: 12 \tTraining Loss: 0.792433\n",
      "Epoch: 13 \tTraining Loss: 0.790367\n",
      "Epoch: 14 \tTraining Loss: 0.789054\n",
      "Epoch: 15 \tTraining Loss: 0.787184\n",
      "Epoch: 16 \tTraining Loss: 0.783907\n",
      "Epoch: 17 \tTraining Loss: 0.784108\n",
      "Epoch: 18 \tTraining Loss: 0.780980\n",
      "Epoch: 19 \tTraining Loss: 0.779470\n",
      "Epoch: 20 \tTraining Loss: 0.780965\n",
      "Accuracy: 0.6546666666666666\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Ternary', acc_avg_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.929748\n",
      "Epoch: 2 \tTraining Loss: 0.840889\n",
      "Epoch: 3 \tTraining Loss: 0.824526\n",
      "Epoch: 4 \tTraining Loss: 0.819691\n",
      "Epoch: 5 \tTraining Loss: 0.817137\n",
      "Epoch: 6 \tTraining Loss: 0.813704\n",
      "Epoch: 7 \tTraining Loss: 0.811791\n",
      "Epoch: 8 \tTraining Loss: 0.809702\n",
      "Epoch: 9 \tTraining Loss: 0.806725\n",
      "Epoch: 10 \tTraining Loss: 0.808033\n",
      "Epoch: 11 \tTraining Loss: 0.805952\n",
      "Epoch: 12 \tTraining Loss: 0.802585\n",
      "Epoch: 13 \tTraining Loss: 0.802132\n",
      "Epoch: 14 \tTraining Loss: 0.800647\n",
      "Epoch: 15 \tTraining Loss: 0.801953\n",
      "Epoch: 16 \tTraining Loss: 0.801180\n",
      "Epoch: 17 \tTraining Loss: 0.800005\n",
      "Epoch: 18 \tTraining Loss: 0.798276\n",
      "Epoch: 19 \tTraining Loss: 0.797582\n",
      "Epoch: 20 \tTraining Loss: 0.800361\n",
      "Accuracy: 0.6688\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_my_ternary = predict_ternary_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Ternary', acc_avg_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.001390\n",
      "Epoch: 2 \tTraining Loss: 0.985214\n",
      "Epoch: 3 \tTraining Loss: 0.978547\n",
      "Epoch: 4 \tTraining Loss: 0.974473\n",
      "Epoch: 5 \tTraining Loss: 0.970298\n",
      "Epoch: 6 \tTraining Loss: 0.968782\n",
      "Epoch: 7 \tTraining Loss: 0.968075\n",
      "Epoch: 8 \tTraining Loss: 0.967153\n",
      "Epoch: 9 \tTraining Loss: 0.966054\n",
      "Epoch: 10 \tTraining Loss: 0.964156\n",
      "Epoch: 11 \tTraining Loss: 0.963040\n",
      "Epoch: 12 \tTraining Loss: 0.963311\n",
      "Epoch: 13 \tTraining Loss: 0.962439\n",
      "Epoch: 14 \tTraining Loss: 0.962571\n",
      "Epoch: 15 \tTraining Loss: 0.961123\n",
      "Epoch: 16 \tTraining Loss: 0.961532\n",
      "Epoch: 17 \tTraining Loss: 0.959495\n",
      "Epoch: 18 \tTraining Loss: 0.960206\n",
      "Epoch: 19 \tTraining Loss: 0.960634\n",
      "Epoch: 20 \tTraining Loss: 0.959392\n",
      "Accuracy: 0.545\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Ternary', acc_concat_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.016846\n",
      "Epoch: 2 \tTraining Loss: 1.010167\n",
      "Epoch: 3 \tTraining Loss: 1.009114\n",
      "Epoch: 4 \tTraining Loss: 1.007095\n",
      "Epoch: 5 \tTraining Loss: 1.004965\n",
      "Epoch: 6 \tTraining Loss: 1.003773\n",
      "Epoch: 7 \tTraining Loss: 1.003777\n",
      "Epoch: 8 \tTraining Loss: 1.002708\n",
      "Epoch: 9 \tTraining Loss: 1.002436\n",
      "Epoch: 10 \tTraining Loss: 1.002996\n",
      "Epoch: 11 \tTraining Loss: 1.003978\n",
      "Epoch: 12 \tTraining Loss: 1.003062\n",
      "Epoch: 13 \tTraining Loss: 1.002738\n",
      "Epoch: 14 \tTraining Loss: 1.003151\n",
      "Epoch: 15 \tTraining Loss: 1.003688\n",
      "Epoch: 16 \tTraining Loss: 1.003510\n",
      "Epoch: 17 \tTraining Loss: 1.002601\n",
      "Epoch: 18 \tTraining Loss: 1.003658\n",
      "Epoch: 19 \tTraining Loss: 1.003176\n",
      "Epoch: 20 \tTraining Loss: 1.002293\n",
      "Accuracy: 0.5218666666666667\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_my_ternary = predict_ternary_model(my_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n",
      "13          My model      FFN-concat        Ternary  0.521867\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Ternary', acc_concat_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks (CNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification\n",
    "\n",
    "---\n",
    "\n",
    "- [x] 2 layer CNN with output channel sizes of 50 and 10, respectively\n",
    "- [x] Limit each review to 50 words\n",
    "    - [x] If more, truncate to 50\n",
    "    - [x] If less, pad with 0s to make 50\n",
    "- [x] Use cross entropy\n",
    "- [x] Select hyperparameters (ie: nonlinearity, #epochs, etc.) of my choosing\n",
    "- [x] Train for binary classification\n",
    "- [x] Train for ternary classification\n",
    "- [x] Report for testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_word_embeddings(df: pd.DataFrame, col_name: str, max_sentence_length: int, model_to_use, about):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    conditioned_sequences:\n",
    "        Sequences with only 50 words per review\n",
    "    \"\"\"\n",
    "    print(\"About: \", about)\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    concatenated_features = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        words = sentence.split(\" \")\n",
    "        if len(words) > max_sentence_length:\n",
    "            words = words[:max_sentence_length]\n",
    "        elif len(words) < max_sentence_length:\n",
    "            words += [''] * (max_sentence_length - len(words))\n",
    "\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        \n",
    "    tensors = []\n",
    "    for words in sentence_vectorized:\n",
    "        tensors.append(torch.tensor(words))\n",
    "    padded_sequences = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
    "    print(\"Embeddings shape: \", padded_sequences.shape)\n",
    "    print()\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary ---\n",
      "About:  Pretrained --- Binary --- Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/3244961659.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  tensors.append(torch.tensor(words))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  torch.Size([96000, 50, 300])\n",
      "\n",
      "About:  Pretrained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([24000, 50, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Binary ---\")\n",
    "pretrained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About:  My trained --- Binary --- Train\n",
      "Embeddings shape:  torch.Size([96000, 50, 300])\n",
      "\n",
      "About:  My trained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([24000, 50, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_trained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_trained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ternary ---\n",
      "About:  Pretrained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([120000, 50, 300])\n",
      "\n",
      "About:  Pretrained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([30000, 50, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About:  My trained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([120000, 50, 300])\n",
      "\n",
      "About:  My trained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([30000, 50, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_trained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_trained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_load_data(embeddings: list, y_true: list, classification_type: str, about: str):\n",
    "    \"\"\"Reformat training labels and load with torch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings:\n",
    "\n",
    "    y_true:\n",
    "        1 --- (positive) --- 0 \n",
    "        2 --- (negative) --- 1 \n",
    "        3 --- (neutral)  --- 2 \n",
    "\n",
    "    classification_type:\n",
    "        binary or ternary\n",
    "    \n",
    "    about: `str`\n",
    "        Additional info on model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    loader:\n",
    "        The torch loader\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "\n",
    "    print(\"\\nAbout: \", about, \"\\n\")\n",
    "    # print(\"Original mappings --- \", y_true.unique())\n",
    "    if classification_type == \"binary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "\n",
    "    elif classification_type == \"ternary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "        map_y_true_values = map_y_true_values.replace(3, 2)\n",
    "        # print(\"Remappings for \", classification_type, \"---\", map_y_true_values.unique())\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid classification type\")\n",
    "\n",
    "    # print(\"Remappings        --- \", map_y_true_values.unique())\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    y_true_tensor = torch.tensor(map_y_true_values.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(embeddings_tensor, y_true_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)    \n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_ternary_train_50_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary ---\n",
      "\n",
      "About:  Pretrained x Binary x Train \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_84183/747051197.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "About:  Pretrained x Binary x Test \n",
      "\n",
      "\n",
      "About:  My trained x Binary x Train \n",
      "\n",
      "\n",
      "About:  My trained x Binary x Test \n",
      "\n",
      "\n",
      "--- Ternary ---\n",
      "\n",
      "About:  Pretrained x Ternary x Train \n",
      "\n",
      "\n",
      "About:  Pretrained x Ternary x Test \n",
      "\n",
      "\n",
      "About:  My trained x Ternary x Test \n",
      "\n",
      "\n",
      "About:  My trained x Ternary x Test \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Binary ---\")\n",
    "pretrained_binary_train_loader = format_and_load_data(pretrained_binary_train_50_embeddings, binary_y_train, \"binary\", \"Pretrained x Binary x Train\")\n",
    "pretrained_binary_test_loader = format_and_load_data(pretrained_binary_test_50_embeddings, binary_y_test, \"binary\", \"Pretrained x Binary x Test\")\n",
    "\n",
    "my_trained_binary_train_loader = format_and_load_data(my_trained_binary_train_50_embeddings, binary_y_train, \"binary\", \"My trained x Binary x Train\")\n",
    "my_trained_binary_test_loader = format_and_load_data(my_trained_binary_test_50_embeddings, binary_y_test, \"binary\", \"My trained x Binary x Test\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_loader = format_and_load_data(pretrained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"Pretrained x Ternary x Train\")\n",
    "pretrained_ternary_test_loader = format_and_load_data(pretrained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"Pretrained x Ternary x Test\")\n",
    "\n",
    "my_trained_ternary_train_loader = format_and_load_data(my_trained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"My trained x Ternary x Test\")\n",
    "my_trained_ternary_test_loader = format_and_load_data(my_trained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"My trained x Ternary x Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=output_channels1, kernel_size=kernel_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=kernel_size2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # to calc 460, refer to docs and write my own utils func\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(460, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Before permute ---\", x.shape)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        # print(\"After permute ---\", x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"After Conv layers ---\", x.shape)\n",
    "        # print(\"1st\", x.shape)\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"After Flatten ---\", x.shape)\n",
    "        x = self.fc_layers(x)\n",
    "        # print(\"After Fully connected layers ---\", x.shape)\n",
    "        # print()\n",
    "        return x\n",
    "        \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader, test_loader):\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            test_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "    \n",
    "            # Train the model\n",
    "            self.train()\n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "    \n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_train += (predicted == target).sum().item()\n",
    "                total_train += target.size(0)\n",
    "    \n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            for data, target in test_loader:\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                test_loss += loss.item() * data.size(0)\n",
    "    \n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                correct_test += (predicted == target).sum().item()\n",
    "                total_test += target.size(0)\n",
    "\n",
    "            # Calculate average losses\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "            # Calculate accuracy\n",
    "            train_accuracy = correct_train / total_train\n",
    "            test_accuracy = correct_test / total_test\n",
    "    \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f} \\tTraining Accuracy: {:.4f} \\tTest Accuracy: {:.4f}'.format(\n",
    "                epoch + 1,\n",
    "                train_loss,\n",
    "                test_loss,\n",
    "                train_accuracy,\n",
    "                test_accuracy\n",
    "            ))\n",
    "\n",
    "        print('Finished Training')\n",
    "        return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 50, 300)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, N_words, embedding_size = pretrained_binary_train_50_embeddings.shape\n",
    "N, N_words, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all 4 cases\n",
    "# channels: for each word has 300 dims, so 300 features\n",
    "num_channels = 300\n",
    "\n",
    "output_channels1 = 50\n",
    "output_channels2 = 10\n",
    "kernel_size1 = 3\n",
    "kernel_size2 = 3 \n",
    "\n",
    "number_of_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=460, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "Pretrained x Binary\n",
      "Epoch: 1 \tTraining Loss: 0.692244 \tTest Loss: 0.693146 \tTraining Accuracy: 0.5021 \tTest Accuracy: 0.4917\n",
      "Epoch: 2 \tTraining Loss: 0.693147 \tTest Loss: 0.693146 \tTraining Accuracy: 0.5021 \tTest Accuracy: 0.4918\n",
      "Epoch: 3 \tTraining Loss: 0.693147 \tTest Loss: 0.693146 \tTraining Accuracy: 0.5021 \tTest Accuracy: 0.4918\n",
      "Epoch: 4 \tTraining Loss: 0.693147 \tTest Loss: 0.693146 \tTraining Accuracy: 0.5021 \tTest Accuracy: 0.4918\n",
      "Epoch: 5 \tTraining Loss: 0.622763 \tTest Loss: 0.449991 \tTraining Accuracy: 0.6277 \tTest Accuracy: 0.7991\n",
      "Epoch: 6 \tTraining Loss: 0.408734 \tTest Loss: 0.379733 \tTraining Accuracy: 0.8164 \tTest Accuracy: 0.8334\n",
      "Epoch: 7 \tTraining Loss: 0.365226 \tTest Loss: 0.357652 \tTraining Accuracy: 0.8388 \tTest Accuracy: 0.8438\n",
      "Epoch: 8 \tTraining Loss: 0.338864 \tTest Loss: 0.346991 \tTraining Accuracy: 0.8537 \tTest Accuracy: 0.8492\n",
      "Epoch: 9 \tTraining Loss: 0.319549 \tTest Loss: 0.343151 \tTraining Accuracy: 0.8643 \tTest Accuracy: 0.8501\n",
      "Epoch: 10 \tTraining Loss: 0.301770 \tTest Loss: 0.342116 \tTraining Accuracy: 0.8731 \tTest Accuracy: 0.8511\n",
      "Epoch: 11 \tTraining Loss: 0.285825 \tTest Loss: 0.378118 \tTraining Accuracy: 0.8811 \tTest Accuracy: 0.8418\n",
      "Epoch: 12 \tTraining Loss: 0.271505 \tTest Loss: 0.360597 \tTraining Accuracy: 0.8885 \tTest Accuracy: 0.8486\n",
      "Epoch: 13 \tTraining Loss: 0.258623 \tTest Loss: 0.375818 \tTraining Accuracy: 0.8943 \tTest Accuracy: 0.8440\n",
      "Epoch: 14 \tTraining Loss: 0.247524 \tTest Loss: 0.370229 \tTraining Accuracy: 0.8986 \tTest Accuracy: 0.8502\n",
      "Epoch: 15 \tTraining Loss: 0.246119 \tTest Loss: 0.399888 \tTraining Accuracy: 0.8984 \tTest Accuracy: 0.8411\n",
      "Epoch: 16 \tTraining Loss: 0.244393 \tTest Loss: 0.395165 \tTraining Accuracy: 0.8985 \tTest Accuracy: 0.8425\n",
      "Epoch: 17 \tTraining Loss: 0.234531 \tTest Loss: 0.377481 \tTraining Accuracy: 0.9030 \tTest Accuracy: 0.8406\n",
      "Epoch: 18 \tTraining Loss: 0.220023 \tTest Loss: 0.415709 \tTraining Accuracy: 0.9107 \tTest Accuracy: 0.8263\n",
      "Epoch: 19 \tTraining Loss: 0.216777 \tTest Loss: 0.463348 \tTraining Accuracy: 0.9104 \tTest Accuracy: 0.8091\n",
      "Epoch: 20 \tTraining Loss: 0.214772 \tTest Loss: 0.489350 \tTraining Accuracy: 0.9108 \tTest Accuracy: 0.8027\n",
      "Finished Training\n",
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n",
      "13          My model      FFN-concat        Ternary  0.521867\n",
      "14        Pretrained          CNN-50         Binary  0.802667\n",
      "\n",
      "My trained x Binary\n",
      "Epoch: 1 \tTraining Loss: 0.573582 \tTest Loss: 0.421802 \tTraining Accuracy: 0.5728 \tTest Accuracy: 0.7823\n",
      "Epoch: 2 \tTraining Loss: 0.388979 \tTest Loss: 0.382646 \tTraining Accuracy: 0.8123 \tTest Accuracy: 0.8204\n",
      "Epoch: 3 \tTraining Loss: 0.358012 \tTest Loss: 0.381502 \tTraining Accuracy: 0.8372 \tTest Accuracy: 0.8236\n",
      "Epoch: 4 \tTraining Loss: 0.341939 \tTest Loss: 0.371438 \tTraining Accuracy: 0.8457 \tTest Accuracy: 0.8310\n",
      "Epoch: 5 \tTraining Loss: 0.327950 \tTest Loss: 0.365454 \tTraining Accuracy: 0.8537 \tTest Accuracy: 0.8360\n",
      "Epoch: 6 \tTraining Loss: 0.317712 \tTest Loss: 0.371215 \tTraining Accuracy: 0.8588 \tTest Accuracy: 0.8310\n",
      "Epoch: 7 \tTraining Loss: 0.307951 \tTest Loss: 0.372169 \tTraining Accuracy: 0.8654 \tTest Accuracy: 0.8343\n",
      "Epoch: 8 \tTraining Loss: 0.299268 \tTest Loss: 0.379414 \tTraining Accuracy: 0.8686 \tTest Accuracy: 0.8333\n",
      "Epoch: 9 \tTraining Loss: 0.290801 \tTest Loss: 0.388868 \tTraining Accuracy: 0.8730 \tTest Accuracy: 0.8279\n",
      "Epoch: 10 \tTraining Loss: 0.284689 \tTest Loss: 0.398351 \tTraining Accuracy: 0.8750 \tTest Accuracy: 0.8235\n",
      "Epoch: 11 \tTraining Loss: 0.279403 \tTest Loss: 0.402405 \tTraining Accuracy: 0.8772 \tTest Accuracy: 0.8234\n",
      "Epoch: 12 \tTraining Loss: 0.272677 \tTest Loss: 0.409787 \tTraining Accuracy: 0.8812 \tTest Accuracy: 0.8225\n",
      "Epoch: 13 \tTraining Loss: 0.267991 \tTest Loss: 0.407110 \tTraining Accuracy: 0.8827 \tTest Accuracy: 0.8218\n",
      "Epoch: 14 \tTraining Loss: 0.261976 \tTest Loss: 0.422162 \tTraining Accuracy: 0.8851 \tTest Accuracy: 0.8175\n",
      "Epoch: 15 \tTraining Loss: 0.259205 \tTest Loss: 0.415033 \tTraining Accuracy: 0.8857 \tTest Accuracy: 0.8242\n",
      "Epoch: 16 \tTraining Loss: 0.253775 \tTest Loss: 0.439582 \tTraining Accuracy: 0.8879 \tTest Accuracy: 0.8113\n",
      "Epoch: 17 \tTraining Loss: 0.248872 \tTest Loss: 0.441293 \tTraining Accuracy: 0.8898 \tTest Accuracy: 0.8116\n",
      "Epoch: 18 \tTraining Loss: 0.246845 \tTest Loss: 0.453899 \tTraining Accuracy: 0.8911 \tTest Accuracy: 0.8103\n",
      "Epoch: 19 \tTraining Loss: 0.243071 \tTest Loss: 0.456071 \tTraining Accuracy: 0.8914 \tTest Accuracy: 0.8084\n",
      "Epoch: 20 \tTraining Loss: 0.238567 \tTest Loss: 0.458250 \tTraining Accuracy: 0.8930 \tTest Accuracy: 0.8152\n",
      "Finished Training\n",
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n",
      "13          My model      FFN-concat        Ternary  0.521867\n",
      "14        Pretrained          CNN-50         Binary  0.802667\n",
      "15          My model          CNN-50         Binary  0.815167\n"
     ]
    }
   ],
   "source": [
    "binary_num_classes = 2  # Number of output classes\n",
    "\n",
    "binary_cnn_net_model = CNN_Net(embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, binary_num_classes)\n",
    "print(binary_cnn_net_model)\n",
    "\n",
    "optimizer = optim.SGD(binary_cnn_net_model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Pretrained x Binary\")\n",
    "cnn_pretrained_binary = binary_cnn_net_model.train_network(number_of_epochs, optimizer, criterion, pretrained_binary_train_loader, pretrained_binary_test_loader)\n",
    "# print(cnn_pretrained_binary)\n",
    "results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Binary', cnn_pretrained_binary)\n",
    "print(results_df)\n",
    "print()\n",
    "\n",
    "print(\"My trained x Binary\")\n",
    "cnn_my_trained_binary = binary_cnn_net_model.train_network(number_of_epochs, optimizer, criterion, my_trained_binary_train_loader, my_trained_binary_test_loader)\n",
    "# print(cnn_my_trained_binary)\n",
    "results_df = store_results(results_dict, 'My model', 'CNN-50', 'Binary', cnn_my_trained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=460, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "Pretrained x Ternary\n",
      "Epoch: 1 \tTraining Loss: 0.879824 \tTest Loss: 0.816312 \tTraining Accuracy: 0.6026 \tTest Accuracy: 0.6446\n",
      "Epoch: 2 \tTraining Loss: 0.772441 \tTest Loss: 0.788958 \tTraining Accuracy: 0.6658 \tTest Accuracy: 0.6595\n",
      "Epoch: 3 \tTraining Loss: 0.739417 \tTest Loss: 0.772979 \tTraining Accuracy: 0.6821 \tTest Accuracy: 0.6676\n",
      "Epoch: 4 \tTraining Loss: 0.715706 \tTest Loss: 0.766782 \tTraining Accuracy: 0.6946 \tTest Accuracy: 0.6736\n",
      "Epoch: 5 \tTraining Loss: 0.694744 \tTest Loss: 0.755726 \tTraining Accuracy: 0.7053 \tTest Accuracy: 0.6787\n",
      "Epoch: 6 \tTraining Loss: 0.676082 \tTest Loss: 0.742203 \tTraining Accuracy: 0.7148 \tTest Accuracy: 0.6849\n",
      "Epoch: 7 \tTraining Loss: 0.660962 \tTest Loss: 0.746151 \tTraining Accuracy: 0.7222 \tTest Accuracy: 0.6849\n",
      "Epoch: 8 \tTraining Loss: 0.646665 \tTest Loss: 0.750832 \tTraining Accuracy: 0.7291 \tTest Accuracy: 0.6854\n",
      "Epoch: 9 \tTraining Loss: 0.633984 \tTest Loss: 0.765243 \tTraining Accuracy: 0.7344 \tTest Accuracy: 0.6819\n",
      "Epoch: 10 \tTraining Loss: 0.623026 \tTest Loss: 0.788032 \tTraining Accuracy: 0.7395 \tTest Accuracy: 0.6748\n",
      "Epoch: 11 \tTraining Loss: 0.614804 \tTest Loss: 0.815213 \tTraining Accuracy: 0.7431 \tTest Accuracy: 0.6649\n",
      "Epoch: 12 \tTraining Loss: 0.608020 \tTest Loss: 0.857792 \tTraining Accuracy: 0.7462 \tTest Accuracy: 0.6555\n",
      "Epoch: 13 \tTraining Loss: 0.598339 \tTest Loss: 0.862162 \tTraining Accuracy: 0.7504 \tTest Accuracy: 0.6549\n",
      "Epoch: 14 \tTraining Loss: 0.589577 \tTest Loss: 0.851610 \tTraining Accuracy: 0.7548 \tTest Accuracy: 0.6601\n",
      "Epoch: 15 \tTraining Loss: 0.584318 \tTest Loss: 0.879534 \tTraining Accuracy: 0.7579 \tTest Accuracy: 0.6527\n",
      "Epoch: 16 \tTraining Loss: 0.579581 \tTest Loss: 0.878099 \tTraining Accuracy: 0.7591 \tTest Accuracy: 0.6513\n",
      "Epoch: 17 \tTraining Loss: 0.571726 \tTest Loss: 0.845466 \tTraining Accuracy: 0.7620 \tTest Accuracy: 0.6586\n",
      "Epoch: 18 \tTraining Loss: 0.561629 \tTest Loss: 0.870339 \tTraining Accuracy: 0.7671 \tTest Accuracy: 0.6590\n",
      "Epoch: 19 \tTraining Loss: 0.552590 \tTest Loss: 0.863779 \tTraining Accuracy: 0.7705 \tTest Accuracy: 0.6596\n",
      "Epoch: 20 \tTraining Loss: 0.545637 \tTest Loss: 0.869463 \tTraining Accuracy: 0.7731 \tTest Accuracy: 0.6602\n",
      "Finished Training\n",
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n",
      "13          My model      FFN-concat        Ternary  0.521867\n",
      "14        Pretrained          CNN-50         Binary  0.802667\n",
      "15          My model          CNN-50         Binary  0.815167\n",
      "16        Pretrained          CNN-50        Ternary  0.660200\n",
      "\n",
      "My trained x Ternary\n",
      "Epoch: 1 \tTraining Loss: 1.115694 \tTest Loss: 1.098612 \tTraining Accuracy: 0.3992 \tTest Accuracy: 0.4029\n",
      "Epoch: 2 \tTraining Loss: 1.098603 \tTest Loss: 1.098612 \tTraining Accuracy: 0.3993 \tTest Accuracy: 0.4029\n",
      "Epoch: 3 \tTraining Loss: 1.098603 \tTest Loss: 1.098612 \tTraining Accuracy: 0.3993 \tTest Accuracy: 0.4029\n",
      "Epoch: 4 \tTraining Loss: 1.098603 \tTest Loss: 1.098612 \tTraining Accuracy: 0.3993 \tTest Accuracy: 0.4029\n",
      "Epoch: 5 \tTraining Loss: 1.098603 \tTest Loss: 1.098612 \tTraining Accuracy: 0.3993 \tTest Accuracy: 0.4029\n",
      "Epoch: 6 \tTraining Loss: 1.097961 \tTest Loss: 1.093035 \tTraining Accuracy: 0.3995 \tTest Accuracy: 0.4054\n",
      "Epoch: 7 \tTraining Loss: 1.059170 \tTest Loss: 1.017248 \tTraining Accuracy: 0.4029 \tTest Accuracy: 0.4081\n",
      "Epoch: 8 \tTraining Loss: 0.885079 \tTest Loss: 0.822234 \tTraining Accuracy: 0.5627 \tTest Accuracy: 0.6416\n",
      "Epoch: 9 \tTraining Loss: 0.800581 \tTest Loss: 0.811380 \tTraining Accuracy: 0.6534 \tTest Accuracy: 0.6469\n",
      "Epoch: 10 \tTraining Loss: 0.779973 \tTest Loss: 0.806682 \tTraining Accuracy: 0.6614 \tTest Accuracy: 0.6460\n",
      "Epoch: 11 \tTraining Loss: 0.764438 \tTest Loss: 0.794613 \tTraining Accuracy: 0.6662 \tTest Accuracy: 0.6531\n",
      "Epoch: 12 \tTraining Loss: 0.750442 \tTest Loss: 0.793956 \tTraining Accuracy: 0.6704 \tTest Accuracy: 0.6543\n",
      "Epoch: 13 \tTraining Loss: 0.740260 \tTest Loss: 0.790486 \tTraining Accuracy: 0.6736 \tTest Accuracy: 0.6560\n",
      "Epoch: 14 \tTraining Loss: 0.732046 \tTest Loss: 0.793165 \tTraining Accuracy: 0.6765 \tTest Accuracy: 0.6572\n",
      "Epoch: 15 \tTraining Loss: 0.723413 \tTest Loss: 0.789904 \tTraining Accuracy: 0.6795 \tTest Accuracy: 0.6562\n",
      "Epoch: 16 \tTraining Loss: 0.716365 \tTest Loss: 0.793513 \tTraining Accuracy: 0.6814 \tTest Accuracy: 0.6538\n",
      "Epoch: 17 \tTraining Loss: 0.709039 \tTest Loss: 0.792894 \tTraining Accuracy: 0.6835 \tTest Accuracy: 0.6546\n",
      "Epoch: 18 \tTraining Loss: 0.702864 \tTest Loss: 0.795653 \tTraining Accuracy: 0.6865 \tTest Accuracy: 0.6562\n",
      "Epoch: 19 \tTraining Loss: 0.696332 \tTest Loss: 0.796331 \tTraining Accuracy: 0.6887 \tTest Accuracy: 0.6556\n",
      "Epoch: 20 \tTraining Loss: 0.691335 \tTest Loss: 0.810725 \tTraining Accuracy: 0.6909 \tTest Accuracy: 0.6550\n",
      "Finished Training\n",
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
      "2     My model-train  Perceptron-avg         Binary  0.778406\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
      "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
      "5     My model-train         SVM-avg         Binary  0.842896\n",
      "6         Pretrained         FFN-avg         Binary  0.807458\n",
      "7           My model         FFN-avg         Binary  0.851875\n",
      "8         Pretrained      FFN-concat         Binary  0.684625\n",
      "9           My model      FFN-concat         Binary  0.498917\n",
      "10        Pretrained         FFN-avg        Ternary  0.654667\n",
      "11          My model         FFN-avg        Ternary  0.668800\n",
      "12        Pretrained      FFN-concat        Ternary  0.545000\n",
      "13          My model      FFN-concat        Ternary  0.521867\n",
      "14        Pretrained          CNN-50         Binary  0.802667\n",
      "15          My model          CNN-50         Binary  0.815167\n",
      "16        Pretrained          CNN-50        Ternary  0.660200\n",
      "17          My model          CNN-50        Ternary  0.655000\n"
     ]
    }
   ],
   "source": [
    "ternary_num_classes = 3  # Number of output classes\n",
    "\n",
    "ternary_cnn_net_model = CNN_Net(embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, ternary_num_classes)\n",
    "print(ternary_cnn_net_model)\n",
    "\n",
    "optimizer = optim.SGD(ternary_cnn_net_model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Pretrained x Ternary\")\n",
    "cnn_pretrained_ternary = ternary_cnn_net_model.train_network(number_of_epochs, optimizer, criterion, pretrained_ternary_train_loader, pretrained_ternary_test_loader)\n",
    "results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Ternary', cnn_pretrained_ternary)\n",
    "print(results_df)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"My trained x Ternary\")\n",
    "cnn_my_trained_ternary = ternary_cnn_net_model.train_network(number_of_epochs, optimizer, criterion, my_trained_ternary_train_loader, my_trained_ternary_test_loader)\n",
    "results_df = store_results(results_dict, 'My model', 'CNN-50', 'Ternary', cnn_my_trained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W2V Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF-train</td>\n",
       "      <td>Perceptron-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.887177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretrained-train</td>\n",
       "      <td>Perceptron-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.694156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My model-train</td>\n",
       "      <td>Perceptron-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.778406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF-train</td>\n",
       "      <td>SVM-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.916125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pretrained-train</td>\n",
       "      <td>SVM-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.812615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My model-train</td>\n",
       "      <td>SVM-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.842896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>FFN-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.807458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My model</td>\n",
       "      <td>FFN-avg</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.851875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>FFN-concat</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.684625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My model</td>\n",
       "      <td>FFN-concat</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.498917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>FFN-avg</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.654667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>My model</td>\n",
       "      <td>FFN-avg</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>FFN-concat</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My model</td>\n",
       "      <td>FFN-concat</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.521867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>CNN-50</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.802667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>My model</td>\n",
       "      <td>CNN-50</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.815167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pretrained</td>\n",
       "      <td>CNN-50</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>My model</td>\n",
       "      <td>CNN-50</td>\n",
       "      <td>Ternary</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            W2V Type          Method Classification  Accuracy\n",
       "0       TF-IDF-train  Perceptron-avg         Binary  0.887177\n",
       "1   Pretrained-train  Perceptron-avg         Binary  0.694156\n",
       "2     My model-train  Perceptron-avg         Binary  0.778406\n",
       "3       TF-IDF-train         SVM-avg         Binary  0.916125\n",
       "4   Pretrained-train         SVM-avg         Binary  0.812615\n",
       "5     My model-train         SVM-avg         Binary  0.842896\n",
       "6         Pretrained         FFN-avg         Binary  0.807458\n",
       "7           My model         FFN-avg         Binary  0.851875\n",
       "8         Pretrained      FFN-concat         Binary  0.684625\n",
       "9           My model      FFN-concat         Binary  0.498917\n",
       "10        Pretrained         FFN-avg        Ternary  0.654667\n",
       "11          My model         FFN-avg        Ternary  0.668800\n",
       "12        Pretrained      FFN-concat        Ternary  0.545000\n",
       "13          My model      FFN-concat        Ternary  0.521867\n",
       "14        Pretrained          CNN-50         Binary  0.802667\n",
       "15          My model          CNN-50         Binary  0.815167\n",
       "16        Pretrained          CNN-50        Ternary  0.660200\n",
       "17          My model          CNN-50        Ternary  0.655000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
