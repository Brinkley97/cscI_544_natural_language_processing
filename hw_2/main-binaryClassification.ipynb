{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2- Binary and Ternary Classification for Sentiment Analysis\n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import sklearn\n",
    "import gensim.models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import utils\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "pretrained_word_two_vec_model = api.load('word2vec-google-news-300')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HW Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset Generation\n",
    "    1. Read data\n",
    "    2. Keep reviews and ratings\n",
    "    3. Create binary and ternary classes\n",
    "    4. Clean data steps\n",
    "    4. Clean data function\n",
    "    5. Split data\n",
    "    6. Load pretrained model and train my model\n",
    "        - Get similarity for the pretrained model\n",
    "        - Get similarity for my trained model\n",
    "2. Word Embedding\n",
    "    - Get word embeddings for pretrained model\n",
    "    - Get word embeddings for my model\n",
    "3. Simple models\n",
    "    - Get accuracy for perceptron on pretrained model\n",
    "    - Get accuracy for svm on pretrained model\n",
    "    - Get accuracy for perceptron on my model\n",
    "    - Get accuracy for svm on my model\n",
    "    - What do I conclude from comparing performances\n",
    "4. Feedforward Neural Networks (FFNN)\n",
    "5. Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation\n",
    "\n",
    "- [x] Load the Amazon reviews dataset\n",
    "- [x] Build a balanced dataset of 250K reviews along with their ratings through random selection\n",
    "    - [x] Rating 1: 50K instances\n",
    "    - [x] Rating 2: 50K instances\n",
    "    - [x] Rating 3: 50K instances\n",
    "    - [x] Rating 4: 50K instances\n",
    "    - [x] Rating 5: 50K instances\n",
    "- [x] Create ternary labels using the ratings\n",
    "    - [x] Class 1: Ratings 4 and 5 (positive sentiment)\n",
    "    - [x] Class 2: Ratings 1 and 2 (negative sentiment)\n",
    "    - [x] Class 3: Rating 3 (neutral sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../datasets/amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "amazon_reviews_copy_df = pd.read_csv(dataset, sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                                     Great product.\n",
       "1                 5  What's to say about this commodity item except...\n",
       "2                 5    Haven't used yet, but I am sure I will like it.\n",
       "3                 1  Although this was labeled as &#34;new&#34; the...\n",
       "4                 4                    Gorgeous colors and easy to use\n",
       "...             ...                                                ...\n",
       "2640249           4  I can't live anymore whithout my Palm III. But...\n",
       "2640250           4  Although the Palm Pilot is thin and compact it...\n",
       "2640251           4  This book had a lot of great content without b...\n",
       "2640252           5  I am teaching a course in Excel and am using t...\n",
       "2640253           5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ratings_df = amazon_reviews_copy_df.loc[0:, ['star_rating', 'review_body']]\n",
    "reviews_ratings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_reviews(df: pd.DataFrame, review_col_name: str, number_of_reviews: int = 3):\n",
    "    \"\"\"Include reviews and ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    review_col_name: `str`\n",
    "        The specific_column to get the reviews and ratings of\n",
    "    \n",
    "    number_of_reviews: `int`\n",
    "        Number of samples to include\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Nothing; instead, print the reviews with ratings\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    columns_to_include = [review_col_name, 'star_rating']\n",
    "\n",
    "    # Initialize an empty list to store dictionaries\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # Iterate over the specified columns and retrieve the first three rows\n",
    "    for row in df[columns_to_include].head(3).to_dict(orient='records'):\n",
    "        list_of_dicts.append({'star_rating': row['star_rating'], review_col_name: row[review_col_name]})\n",
    "\n",
    "    for dictionary in list_of_dicts:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## Create binary and ternary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_type(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Update the data type of the star ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with rating values\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ratings = ['1','2','3','4','5']\n",
    "    star_rating_series = df[col_name].copy()\n",
    "\n",
    "    # Convert type to strings\n",
    "    star_rating_series.astype('str')\n",
    "\n",
    "    # Check valid list and see which of our stars match\n",
    "    rows = star_rating_series.index\n",
    "    is_rating_in_valid_ratings = rows[star_rating_series.isin(valid_ratings)]\n",
    "\n",
    "    # Convert to list\n",
    "    is_rating_in_valid_ratings = is_rating_in_valid_ratings.to_list()\n",
    "\n",
    "    updated_df = df.iloc[is_rating_in_valid_ratings]\n",
    "    updated_df[col_name] = updated_df[col_name].astype(int)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_9372/2907883124.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[col_name] = updated_df[col_name].astype(int)\n"
     ]
    }
   ],
   "source": [
    "updated_reviews_ratings_df = update_data_type(reviews_ratings_df, 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640237 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640080 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df = updated_reviews_ratings_df.dropna()\n",
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating  review_body\n",
      "0              False        False\n",
      "1              False        False\n",
      "2              False        False\n",
      "3              False        False\n",
      "4              False        False\n",
      "...              ...          ...\n",
      "2640249        False        False\n",
      "2640250        False        False\n",
      "2640251        False        False\n",
      "2640252        False        False\n",
      "2640253        False        False\n",
      "\n",
      "[2640080 rows x 2 columns]\n",
      "There are no NaN values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_check = updated_reviews_ratings_df.isna()\n",
    "\n",
    "# Display the DataFrame with True where NaN values exist\n",
    "print(nan_check)\n",
    "\n",
    "# Check if any NaN value exists in the DataFrame\n",
    "if nan_check.any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviews per rating star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"# reviews per rating\", updated_reviews_ratings_df['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_star_ratings(df: pd.DataFrame, col_name: str, star_value: int, number_of_reviews: int):\n",
    "    \"\"\"Build a subset balanced dataset with reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The dataframe to use\n",
    "    col_name: `str`\n",
    "        The name of the column to get reviews from\n",
    "    star_value: `int`\n",
    "        The star rating of the review\n",
    "    number_of_reviews: `int`\n",
    "        The number of sub reviews to include in sample\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rating_df, sampled_rating_df: `tuple`\n",
    "        All reviews with that rating and the subset reviews with that rating\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_df = df[df[col_name] == star_value]\n",
    "    sampled_rating_df = rating_df.sample(n=number_of_reviews)\n",
    "    return rating_df, sampled_rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ ] Change #reviews per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_reviews = 50000\n",
    "subset_reviews = 200\n",
    "\n",
    "one_star = 1\n",
    "rating_one, rating_one_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', one_star, subset_reviews)\n",
    "two_stars = 2\n",
    "rating_two, rating_two_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', two_stars, subset_reviews)\n",
    "three_stars = 3\n",
    "rating_three, rating_three_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', three_stars, subset_reviews)\n",
    "four_stars = 4\n",
    "rating_four, rating_four_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', four_stars, subset_reviews)\n",
    "five_stars = 5\n",
    "rating_five, rating_five_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', five_stars, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df = pd.concat([rating_one_sampled, rating_two_sampled, rating_three_sampled, rating_four_sampled, rating_five_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179381</th>\n",
       "      <td>1</td>\n",
       "      <td>not a real projector .  It is a piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190334</th>\n",
       "      <td>1</td>\n",
       "      <td>When it works, this laser printer gets the job...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775668</th>\n",
       "      <td>1</td>\n",
       "      <td>I do not recommend this product at all as it r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248304</th>\n",
       "      <td>1</td>\n",
       "      <td>None of the pens worked upon arrival.  All six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014722</th>\n",
       "      <td>1</td>\n",
       "      <td>The cartridges were empty! I installed the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121749</th>\n",
       "      <td>5</td>\n",
       "      <td>These were exactly what I was looking for to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227060</th>\n",
       "      <td>5</td>\n",
       "      <td>Have not used the item much yet, but overall I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413043</th>\n",
       "      <td>5</td>\n",
       "      <td>About a year ago we purchased the Panasonic KX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611857</th>\n",
       "      <td>5</td>\n",
       "      <td>I use this flash card in my digital frame so t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567559</th>\n",
       "      <td>5</td>\n",
       "      <td>This support works great for me. I use it in m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "1179381            1              not a real projector .  It is a piece\n",
       "1190334            1  When it works, this laser printer gets the job...\n",
       "1775668            1  I do not recommend this product at all as it r...\n",
       "2248304            1  None of the pens worked upon arrival.  All six...\n",
       "2014722            1  The cartridges were empty! I installed the fir...\n",
       "...              ...                                                ...\n",
       "121749             5  These were exactly what I was looking for to c...\n",
       "1227060            5  Have not used the item much yet, but overall I...\n",
       "2413043            5  About a year ago we purchased the Panasonic KX...\n",
       "1611857            5  I use this flash card in my digital frame so t...\n",
       "1567559            5  This support works great for me. I use it in m...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews_by_rating(df: pd.DataFrame, rating_col: str, threshold: int, sentiment_type: str):\n",
    "    \"\"\"Categorizes reviews by adding a rating\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    rating_col: `str`\n",
    "        Column with rating values\n",
    "    \n",
    "    threshold: `int`\n",
    "        Where to split the ratings such that categories can be formed\n",
    "\n",
    "    sentiment_type: `str`\n",
    "        One of three types of sentiment: positive, negative, or neural\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if sentiment_type == 'positive_review_class':\n",
    "        positive_review_threshold = df[rating_col].astype('int32') > threshold\n",
    "        df = df[positive_review_threshold]\n",
    "        df[sentiment_type] = 1\n",
    "\n",
    "    elif sentiment_type == 'negative_review_class':\n",
    "        negative_review_threshold = df[rating_col].astype('int32') < threshold\n",
    "        df = df[negative_review_threshold]\n",
    "        df[sentiment_type] = 2\n",
    "\n",
    "    elif sentiment_type == 'neutral_review_class':\n",
    "        neutral_review_threshold = df[rating_col].astype('int32') == threshold\n",
    "        df = df[neutral_review_threshold]\n",
    "        df[sentiment_type] = 3\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_9372/1400069123.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 2\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_9372/1400069123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 3\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_9372/1400069123.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 1\n"
     ]
    }
   ],
   "source": [
    "negative_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'negative_review_class')\n",
    "neutral_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'neutral_review_class')\n",
    "positive_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'positive_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179381</th>\n",
       "      <td>1</td>\n",
       "      <td>not a real projector .  It is a piece</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190334</th>\n",
       "      <td>1</td>\n",
       "      <td>When it works, this laser printer gets the job...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775668</th>\n",
       "      <td>1</td>\n",
       "      <td>I do not recommend this product at all as it r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248304</th>\n",
       "      <td>1</td>\n",
       "      <td>None of the pens worked upon arrival.  All six...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014722</th>\n",
       "      <td>1</td>\n",
       "      <td>The cartridges were empty! I installed the fir...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121749</th>\n",
       "      <td>5</td>\n",
       "      <td>These were exactly what I was looking for to c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227060</th>\n",
       "      <td>5</td>\n",
       "      <td>Have not used the item much yet, but overall I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413043</th>\n",
       "      <td>5</td>\n",
       "      <td>About a year ago we purchased the Panasonic KX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611857</th>\n",
       "      <td>5</td>\n",
       "      <td>I use this flash card in my digital frame so t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567559</th>\n",
       "      <td>5</td>\n",
       "      <td>This support works great for me. I use it in m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "1179381            1              not a real projector .  It is a piece   \n",
       "1190334            1  When it works, this laser printer gets the job...   \n",
       "1775668            1  I do not recommend this product at all as it r...   \n",
       "2248304            1  None of the pens worked upon arrival.  All six...   \n",
       "2014722            1  The cartridges were empty! I installed the fir...   \n",
       "...              ...                                                ...   \n",
       "121749             5  These were exactly what I was looking for to c...   \n",
       "1227060            5  Have not used the item much yet, but overall I...   \n",
       "2413043            5  About a year ago we purchased the Panasonic KX...   \n",
       "1611857            5  I use this flash card in my digital frame so t...   \n",
       "1567559            5  This support works great for me. I use it in m...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \n",
       "1179381                    2.0                   NaN                    NaN  \n",
       "1190334                    2.0                   NaN                    NaN  \n",
       "1775668                    2.0                   NaN                    NaN  \n",
       "2248304                    2.0                   NaN                    NaN  \n",
       "2014722                    2.0                   NaN                    NaN  \n",
       "...                        ...                   ...                    ...  \n",
       "121749                     NaN                   NaN                    1.0  \n",
       "1227060                    NaN                   NaN                    1.0  \n",
       "2413043                    NaN                   NaN                    1.0  \n",
       "1611857                    NaN                   NaN                    1.0  \n",
       "1567559                    NaN                   NaN                    1.0  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df = pd.concat([negative_review_class_df, neutral_review_class_df, positive_review_class_df])\n",
    "sampled_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_df = sampled_reviews_ratings_df['negative_review_class'].dropna()\n",
    "neutral_reviews_df = sampled_reviews_ratings_df['neutral_review_class'].dropna()\n",
    "positive_reviews_df = sampled_reviews_ratings_df['positive_review_class'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'] = pd.concat([negative_reviews_df, positive_reviews_df])\n",
    "sampled_reviews_ratings_df['ternary_review_class'] = pd.concat([negative_reviews_df, neutral_reviews_df, positive_reviews_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. nan  1.]\n",
      "[2. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_reviews_ratings_df['binary_review_class'].unique())\n",
    "print(sampled_reviews_ratings_df['ternary_review_class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean data steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_lower_case(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Convert all reviews to lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the lower cased reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_case_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    \n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        # print(text_reviews_idx, type(text_review), text_review)\n",
    "\n",
    "        # NOT all reviews are strings, thus all can't be converted to lower cased\n",
    "        if type(text_review) != str:\n",
    "            print(True, text_review)\n",
    "            converted_str = str(text_review)\n",
    "            lower_case_reviews.append(text_review)\n",
    "         \n",
    "        else:\n",
    "            update_text_review = text_review.lower()\n",
    "            lower_case_reviews.append(update_text_review)\n",
    "\n",
    "    updated_df['lower_cased'] = lower_case_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased = convert_reviews_to_lower_case(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"reviews_lower_cased:\")\n",
    "# generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove HTML and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_urls(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove HTML and URLs from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the html_and_urls removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\. \\S+')\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            # Check and remove HTML tags\n",
    "            has_html = bool(re.search('<.*?>', text_review))\n",
    "            if has_html == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "\n",
    "            no_html_review = re.sub('<.*?>', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML -- \", no_html_review)\n",
    "        \n",
    "            # Check and remove URLs\n",
    "            has_url = bool(re.search(r'http\\S+', no_html_review))\n",
    "            if has_url == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has URL --\", no_html_review)\n",
    "                pass\n",
    "\n",
    "            no_html_url_review = re.sub(r'http\\S+', '', no_html_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML, URL -- \", no_html_url_review)\n",
    "            # print()\n",
    "            cleaned_reviews.append(no_html_url_review)\n",
    "        else:\n",
    "            # print(text_reviews_idx, text_review)\n",
    "            cleaned_reviews.append(text_review)\n",
    "            \n",
    "\n",
    "    updated_df['without_html_urls'] = cleaned_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_html_urls:\")\n",
    "# generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how'd\": \"how would\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_replace_contractions(review):\n",
    "    \"\"\"Find the contractions to replace from a specific review\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    review: `str`\n",
    "        A specific review\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    non_contraction_review: `str`\n",
    "        The updated specific review with contractions expanded\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(review, str):\n",
    "        get_words = review.split()\n",
    "\n",
    "        store_non_contraction_words = []\n",
    "\n",
    "        for word in get_words:\n",
    "            if word in store_contractions:\n",
    "                non_contraction_form = store_contractions[word]\n",
    "                # print(word, \"-->\", non_contraction_form)\n",
    "\n",
    "                store_non_contraction_words.append(non_contraction_form)\n",
    "\n",
    "            else:\n",
    "                # print(word)\n",
    "                store_non_contraction_words.append(word)\n",
    "\n",
    "        non_contraction_review = ' '.join(store_non_contraction_words)\n",
    "        return non_contraction_review\n",
    "    else:\n",
    "        return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove contractions from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_contractions_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"with possible contraction(s) -- \", text_review)\n",
    "\n",
    "        without_contraction = locate_and_replace_contractions(text_review)\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"without contraction -- \", without_contraction)\n",
    "        # print()\n",
    "\n",
    "        without_contractions_reviews.append(without_contraction)\n",
    "\n",
    "    updated_df['without_contractions'] = without_contractions_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_contractions:\")\n",
    "# generate_sample_reviews(no_contractions_df, 'without_contractions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove Non-alphabetical characters from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the non-alphabetical characters removed\n",
    "    \"\"\"\n",
    "\n",
    "    alphabetical_char_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        \n",
    "        if isinstance(text_review, str):\n",
    "\n",
    "            # Check for non-alphabetical characters\n",
    "            has_non_alphabetical_char = bool(re.search(r'[^a-zA-Z]', text_review))\n",
    "            if has_non_alphabetical_char == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove non-alphabetical characters\n",
    "            with_alphabetical_char = re.sub(r'[^a-zA-Z\\s]', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"has HTML -- \", with_alphabetical_char)\n",
    "            alphabetical_char_reviews.append(with_alphabetical_char)\n",
    "        else:\n",
    "            alphabetical_char_reviews.append(text_review)\n",
    "\n",
    "    updated_df['with_alpha_chars_only'] = alphabetical_char_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"with_alpha_chars_only:\")\n",
    "# generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove extra spaces from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    single_spaced_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "        # Check if there are any extra spaces\n",
    "            has_extra_space = bool(re.search(r' +', text_review))\n",
    "            if has_extra_space == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has extra space -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            single_spaced_review = re.sub(r' +', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without extra space -- \", single_spaced_review)\n",
    "            # print()\n",
    "            \n",
    "            single_spaced_reviews.append(single_spaced_review)\n",
    "        else:\n",
    "            single_spaced_reviews.append(text_review)\n",
    "\n",
    "    updated_df['without_extra_space'] = single_spaced_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_extra_space:\")\n",
    "# generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Filter stop words out from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_stop_words_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            text_review_words = word_tokenize(text_review) \n",
    "\n",
    "        \n",
    "\n",
    "            # print(\"Before stop word removal\", text_reviews_idx, \" -- \", text_review)\n",
    "\n",
    "            filtered_review = []\n",
    "\n",
    "            for text_review_words_idx in range(len(text_review_words)):\n",
    "                text_review_word = text_review_words[text_review_words_idx]\n",
    "                \n",
    "                # Check if review word is a stop word\n",
    "                if text_review_word in stop_words:\n",
    "                    # print(\"  Stop word -- \", text_review_word)\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(text_review_word, \" -- is NOT a stop word in review\")\n",
    "                    filtered_review.append(text_review_word)\n",
    "\n",
    "            \n",
    "            filtered_review = \" \".join(filtered_review)\n",
    "            # print(\"After stop word removal\", text_reviews_idx, \" -- \", filtered_review)\n",
    "            # print()\n",
    "            \n",
    "            without_stop_words_reviews.append(filtered_review)\n",
    "        else:\n",
    "            without_stop_words_reviews.append(text_review)\n",
    "        \n",
    "\n",
    "    updated_df['without_stop_words'] = without_stop_words_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_stop_words:\")\n",
    "# generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Perform lemmatization  \n",
    "\n",
    "- \"A sentence with many words\"\n",
    "    - \"words\" -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmentize_review(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Lemmentize all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmed_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]   \n",
    "        if isinstance(text_review, str):     \n",
    "            words_in_review = word_tokenize(text_review) \n",
    "\n",
    "            # print(\"Before lem update\", text_reviews_idx, \" -- \", text_review)\n",
    "            # print(\"Lemmed words\", words_in_review)\n",
    "            \n",
    "\n",
    "            lemmed_sentence = []\n",
    "\n",
    "            # Split review into words\n",
    "            for lemmed_words_idx in range(len(words_in_review)):\n",
    "                word = words_in_review[lemmed_words_idx]\n",
    "                \n",
    "                apply_lemmatization = lem.lemmatize(word)\n",
    "                # print(apply_lemmatization)\n",
    "                \n",
    "                lemmed_sentence.append(apply_lemmatization)\n",
    "                filtered_review = \" \".join(lemmed_sentence)\n",
    "        \n",
    "            # print(\"After lem update -- \", filtered_review)\n",
    "            # print()\n",
    "\n",
    "            lemmed_reviews.append(filtered_review)\n",
    "        else:\n",
    "            lemmed_reviews.append(text_review)\n",
    "\n",
    "    updated_df['lemmed_reviews'] = lemmed_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_unlemmed_words:\")\n",
    "# generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, col_name):\n",
    "    \"\"\"Perform lower case, remove HTML and URLs, remove contractions, remove non-alphabetical characters, remove extra spaces, remove stop words, and lemmatize\"\"\"\n",
    "\n",
    "    print(\"original reviews:\")\n",
    "    # generate_sample_reviews(df, col_name, 3)\n",
    "\n",
    "    reviews_lower_cased = convert_reviews_to_lower_case(df, col_name)\n",
    "    print(\"reviews_lower_cased:\")\n",
    "    # generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)\n",
    "\n",
    "    no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')\n",
    "    print(\"without_html_urls:\")\n",
    "    # generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)\n",
    "\n",
    "    no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')\n",
    "    print(\"without_contractions:\")\n",
    "    # generate_sample_reviews(no_contractions_df, 'without_contractions', 3)\n",
    "\n",
    "    only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')\n",
    "    print(\"with_alpha_chars_only:\")\n",
    "    # generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)\n",
    "\n",
    "    no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')\n",
    "    print(\"without_extra_space:\")\n",
    "    # generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)\n",
    "\n",
    "    no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')\n",
    "    print(\"without_stop_words:\")\n",
    "    # generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)\n",
    "    \n",
    "    lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')\n",
    "    print(\"without_unlemmed_words:\")\n",
    "    # print(lemmed_df[\"ternary_review_class\"].unique())\n",
    "    # generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)\n",
    "\n",
    "    return lemmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original reviews:\n",
      "reviews_lower_cased:\n",
      "without_html_urls:\n",
      "without_contractions:\n",
      "with_alpha_chars_only:\n",
      "without_extra_space:\n",
      "without_stop_words:\n",
      "without_unlemmed_words:\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews_df = preprocess_data(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. nan  1.]\n",
      "[2. 3. 1.]\n",
      "[ 2. nan  1.]\n",
      "[2. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_reviews_ratings_df['binary_review_class'].unique())\n",
    "print(sampled_reviews_ratings_df['ternary_review_class'].unique())\n",
    "\n",
    "print(cleaned_reviews_df['binary_review_class'].unique())\n",
    "print(cleaned_reviews_df['ternary_review_class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, review_class):\n",
    "    embeddings_df = df.dropna(subset=[review_class])\n",
    "    # print(len(embeddings_df), embeddings_df['star_rating'].unique())\n",
    "\n",
    "    specific_review_class = embeddings_df[review_class]\n",
    "    # print(specific_review_class.unique())\n",
    "\n",
    "    text = embeddings_df.loc[:, ['lemmed_reviews', 'star_rating', review_class]]\n",
    "    # print(text)\n",
    "\n",
    "    ### Train test split so I can have the same train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, specific_review_class, test_size=0.2, random_state=42)\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "      <th>binary_review_class</th>\n",
       "      <th>ternary_review_class</th>\n",
       "      <th>lower_cased</th>\n",
       "      <th>without_html_urls</th>\n",
       "      <th>without_contractions</th>\n",
       "      <th>with_alpha_chars_only</th>\n",
       "      <th>without_extra_space</th>\n",
       "      <th>without_stop_words</th>\n",
       "      <th>lemmed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1179381</th>\n",
       "      <td>1</td>\n",
       "      <td>not a real projector .  It is a piece</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>not a real projector .  it is a piece</td>\n",
       "      <td>not a real projector .  it is a piece</td>\n",
       "      <td>not a real projector . it is a piece</td>\n",
       "      <td>not a real projector   it is a piece</td>\n",
       "      <td>not a real projector it is a piece</td>\n",
       "      <td>real projector piece</td>\n",
       "      <td>real projector piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190334</th>\n",
       "      <td>1</td>\n",
       "      <td>When it works, this laser printer gets the job...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>when it works, this laser printer gets the job...</td>\n",
       "      <td>when it works, this laser printer gets the job...</td>\n",
       "      <td>when it works, this laser printer gets the job...</td>\n",
       "      <td>when it works  this laser printer gets the job...</td>\n",
       "      <td>when it works this laser printer gets the job ...</td>\n",
       "      <td>works laser printer gets job done unfortunatel...</td>\n",
       "      <td>work laser printer get job done unfortunately ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775668</th>\n",
       "      <td>1</td>\n",
       "      <td>I do not recommend this product at all as it r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i do not recommend this product at all as it r...</td>\n",
       "      <td>i do not recommend this product at all as it r...</td>\n",
       "      <td>i do not recommend this product at all as it r...</td>\n",
       "      <td>i do not recommend this product at all as it r...</td>\n",
       "      <td>i do not recommend this product at all as it r...</td>\n",
       "      <td>recommend product resulted power supplies two ...</td>\n",
       "      <td>recommend product resulted power supply two op...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "1179381            1              not a real projector .  It is a piece   \n",
       "1190334            1  When it works, this laser printer gets the job...   \n",
       "1775668            1  I do not recommend this product at all as it r...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \\\n",
       "1179381                    2.0                   NaN                    NaN   \n",
       "1190334                    2.0                   NaN                    NaN   \n",
       "1775668                    2.0                   NaN                    NaN   \n",
       "\n",
       "         binary_review_class  ternary_review_class  \\\n",
       "1179381                  2.0                   2.0   \n",
       "1190334                  2.0                   2.0   \n",
       "1775668                  2.0                   2.0   \n",
       "\n",
       "                                               lower_cased  \\\n",
       "1179381              not a real projector .  it is a piece   \n",
       "1190334  when it works, this laser printer gets the job...   \n",
       "1775668  i do not recommend this product at all as it r...   \n",
       "\n",
       "                                         without_html_urls  \\\n",
       "1179381              not a real projector .  it is a piece   \n",
       "1190334  when it works, this laser printer gets the job...   \n",
       "1775668  i do not recommend this product at all as it r...   \n",
       "\n",
       "                                      without_contractions  \\\n",
       "1179381               not a real projector . it is a piece   \n",
       "1190334  when it works, this laser printer gets the job...   \n",
       "1775668  i do not recommend this product at all as it r...   \n",
       "\n",
       "                                     with_alpha_chars_only  \\\n",
       "1179381               not a real projector   it is a piece   \n",
       "1190334  when it works  this laser printer gets the job...   \n",
       "1775668  i do not recommend this product at all as it r...   \n",
       "\n",
       "                                       without_extra_space  \\\n",
       "1179381                 not a real projector it is a piece   \n",
       "1190334  when it works this laser printer gets the job ...   \n",
       "1775668  i do not recommend this product at all as it r...   \n",
       "\n",
       "                                        without_stop_words  \\\n",
       "1179381                               real projector piece   \n",
       "1190334  works laser printer gets job done unfortunatel...   \n",
       "1775668  recommend product resulted power supplies two ...   \n",
       "\n",
       "                                            lemmed_reviews  \n",
       "1179381                               real projector piece  \n",
       "1190334  work laser printer get job done unfortunately ...  \n",
       "1775668  recommend product resulted power supply two op...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Binary\\n\")\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = split_data(cleaned_reviews_df, 'binary_review_class')\n",
    "# print(\"\\nTernary\")\n",
    "ternary_X_train, ternary_X_test, ternary_y_train, ternary_y_test = split_data(cleaned_reviews_df, 'ternary_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmed_reviews</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ternary_review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1015252</th>\n",
       "      <td>cheaply made ut broke could use</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882960</th>\n",
       "      <td>fit well warm cheaply made velcro keep mitten ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034797</th>\n",
       "      <td>great eraser</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891130</th>\n",
       "      <td>refilis yet see refill available would preferr...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976530</th>\n",
       "      <td>blob ink ever smooth writing every time qualit...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128683</th>\n",
       "      <td>warned two tone died even pinter showed left o...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912167</th>\n",
       "      <td>basket sit inside bottom basket take half spac...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877745</th>\n",
       "      <td>great picture quality print</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701849</th>\n",
       "      <td>pleased size laminator issue even though light...</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088615</th>\n",
       "      <td>product embedded microban endocrine disruptor ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            lemmed_reviews  star_rating  \\\n",
       "1015252                    cheaply made ut broke could use            1   \n",
       "882960   fit well warm cheaply made velcro keep mitten ...            3   \n",
       "1034797                                       great eraser            4   \n",
       "1891130  refilis yet see refill available would preferr...            3   \n",
       "1976530  blob ink ever smooth writing every time qualit...            5   \n",
       "...                                                    ...          ...   \n",
       "2128683  warned two tone died even pinter showed left o...            1   \n",
       "1912167  basket sit inside bottom basket take half spac...            2   \n",
       "877745                         great picture quality print            5   \n",
       "1701849  pleased size laminator issue even though light...            3   \n",
       "1088615  product embedded microban endocrine disruptor ...            1   \n",
       "\n",
       "         ternary_review_class  \n",
       "1015252                   2.0  \n",
       "882960                    3.0  \n",
       "1034797                   1.0  \n",
       "1891130                   3.0  \n",
       "1976530                   1.0  \n",
       "...                       ...  \n",
       "2128683                   2.0  \n",
       "1912167                   2.0  \n",
       "877745                    1.0  \n",
       "1701849                   3.0  \n",
       "1088615                   2.0  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ternary_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015252    2.0\n",
       "882960     3.0\n",
       "1034797    1.0\n",
       "1891130    3.0\n",
       "1976530    1.0\n",
       "          ... \n",
       "2128683    2.0\n",
       "1912167    2.0\n",
       "877745     1.0\n",
       "1701849    3.0\n",
       "1088615    2.0\n",
       "Name: ternary_review_class, Length: 800, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ternary_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, col_name: str):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: `pd.DataFrame`\n",
    "            The data\n",
    "        \n",
    "        col_name: `str`\n",
    "            Column with reviews\n",
    "\n",
    "        words_in_model: `list`\n",
    "            Words in Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text_reviews = self.df[self.col_name].values\n",
    "\n",
    "        for text_reviews_idx in range(len(text_reviews)):\n",
    "            text_review = text_reviews[text_reviews_idx]\n",
    "            # print(text_reviews_idx, \"--\", text_review)\n",
    "\n",
    "            yield utils.simple_preprocess(text_review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 15:11:10,605 : INFO : collecting all words and their counts\n",
      "2024-02-09 15:11:10,606 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 15:11:10,652 : INFO : collected 3864 word types from a corpus of 20194 raw words and 640 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 15:11:10,653 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 15:11:10,657 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 442 unique words (11.44% of original 3864, drops 3422)', 'datetime': '2024-02-09T15:11:10.657063', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,658 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 12580 word corpus (62.30% of original 20194, drops 7614)', 'datetime': '2024-02-09T15:11:10.658107', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,661 : INFO : deleting the raw counts dictionary of 3864 items\n",
      "2024-02-09 15:11:10,662 : INFO : sample=0.001 downsamples 101 most-common words\n",
      "2024-02-09 15:11:10,662 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 9963.231730678814 word corpus (79.2%% of prior 12580)', 'datetime': '2024-02-09T15:11:10.662927', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,667 : INFO : estimated required memory for 442 words and 300 dimensions: 1281800 bytes\n",
      "2024-02-09 15:11:10,668 : INFO : resetting layer weights\n",
      "2024-02-09 15:11:10,670 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T15:11:10.670283', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 15:11:10,670 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 442 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T15:11:10.670902', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 15:11:10,720 : INFO : EPOCH 0: training on 20194 raw words (9906 effective words) took 0.0s, 276395 effective words/s\n",
      "2024-02-09 15:11:10,760 : INFO : EPOCH 1: training on 20194 raw words (9960 effective words) took 0.0s, 309947 effective words/s\n",
      "2024-02-09 15:11:10,801 : INFO : EPOCH 2: training on 20194 raw words (9977 effective words) took 0.0s, 293433 effective words/s\n",
      "2024-02-09 15:11:10,841 : INFO : EPOCH 3: training on 20194 raw words (10006 effective words) took 0.0s, 301056 effective words/s\n",
      "2024-02-09 15:11:10,880 : INFO : EPOCH 4: training on 20194 raw words (9899 effective words) took 0.0s, 317718 effective words/s\n",
      "2024-02-09 15:11:10,880 : INFO : Word2Vec lifecycle event {'msg': 'training on 100970 raw words (49748 effective words) took 0.2s, 237637 effective words/s', 'datetime': '2024-02-09T15:11:10.880751', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 15:11:10,881 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=442, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T15:11:10.881379', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-09 15:11:10,882 : INFO : collecting all words and their counts\n",
      "2024-02-09 15:11:10,883 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 15:11:10,923 : INFO : collected 4415 word types from a corpus of 25696 raw words and 800 sentences\n",
      "2024-02-09 15:11:10,924 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 15:11:10,926 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 543 unique words (12.30% of original 4415, drops 3872)', 'datetime': '2024-02-09T15:11:10.926833', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,927 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 16861 word corpus (65.62% of original 25696, drops 8835)', 'datetime': '2024-02-09T15:11:10.927566', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,930 : INFO : deleting the raw counts dictionary of 4415 items\n",
      "2024-02-09 15:11:10,931 : INFO : sample=0.001 downsamples 89 most-common words\n",
      "2024-02-09 15:11:10,931 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 13807.715674554844 word corpus (81.9%% of prior 16861)', 'datetime': '2024-02-09T15:11:10.931684', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 15:11:10,936 : INFO : estimated required memory for 543 words and 300 dimensions: 1574700 bytes\n",
      "2024-02-09 15:11:10,936 : INFO : resetting layer weights\n",
      "2024-02-09 15:11:10,939 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T15:11:10.939513', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 15:11:10,940 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 543 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T15:11:10.940510', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 15:11:11,002 : INFO : EPOCH 0: training on 25696 raw words (13781 effective words) took 0.1s, 253710 effective words/s\n",
      "2024-02-09 15:11:11,054 : INFO : EPOCH 1: training on 25696 raw words (13796 effective words) took 0.0s, 311456 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ternary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 15:11:11,110 : INFO : EPOCH 2: training on 25696 raw words (13796 effective words) took 0.0s, 321155 effective words/s\n",
      "2024-02-09 15:11:11,167 : INFO : EPOCH 3: training on 25696 raw words (13772 effective words) took 0.0s, 319368 effective words/s\n",
      "2024-02-09 15:11:11,219 : INFO : EPOCH 4: training on 25696 raw words (13799 effective words) took 0.0s, 445750 effective words/s\n",
      "2024-02-09 15:11:11,220 : INFO : Word2Vec lifecycle event {'msg': 'training on 128480 raw words (68944 effective words) took 0.3s, 247139 effective words/s', 'datetime': '2024-02-09T15:11:11.220213', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 15:11:11,220 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=543, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T15:11:11.220671', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Case\")\n",
    "binary_X_train_sentences = MyCorpus(binary_X_train, 'lemmed_reviews')\n",
    "my_binary_X_train_model = gensim.models.Word2Vec(sentences=binary_X_train_sentences, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "# X test - get embeddings from my_binary_X_train_model -- vec_king = my_binary_X_train_model.wv['king']\n",
    "# binary_X_test_sentences = MyCorpus(binary_X_test, 'lemmed_reviews')\n",
    "# sentences = MyCorpus(sampled_reviews_ratings_df, 'review_body')\n",
    "# print(\"\\nSentences\", binary_X_test_sentences)\n",
    "\n",
    "print(\"\\nTernary Case\")\n",
    "ternary_X_train_sentences = MyCorpus(ternary_X_train, 'lemmed_reviews')\n",
    "my_ternary_X_train_model = gensim.models.Word2Vec(sentences=ternary_X_train_sentences, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar scores\n",
    "\n",
    "- [x] Write summary of differences between their model and my model\n",
    "    - My model doesn't perform as well as the pretrained because the pretrained has trained on more data compared to my model. Thus, my model don't have sufficient training compared to the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = pretrained_word_two_vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1bd4a5c10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_X_train_model = my_binary_X_train_model.wv\n",
    "my_trained_binary_X_train_model\n",
    "\n",
    "my_trained_ternary_X_train_model = my_ternary_X_train_model.wv\n",
    "my_trained_ternary_X_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix with proper exs\n",
    "# my_result = my_trained_binary_X_train_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "# print(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings\n",
    "\n",
    "- Word embeddings [vector representation of each word]\n",
    "- TUTORIAL: [Word2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    "    - Follow for the purpose of using the Gensimm library\n",
    "---\n",
    "\n",
    "## (a) Pretrained Word2Vec\n",
    "\n",
    "- [x] Load the pretrained “word2vec-google-news-300” Word2Vec model\n",
    "- [x] Extract word embeddings (per word)\n",
    "- [ ] Check semantic similarities (ie: (1) King − Man + Woman = Queen, (2) excellent ∼ outstanding) of my own\n",
    "\n",
    "## (b) My trained Word2Vec\n",
    "- [x] Train a Word2Vec model using my own dataset\n",
    "    - [ ] Set the embedding size to be 300\n",
    "    - [ ] Set the window size to be 11\n",
    "    - [ ] Consider a minimum word count of 10\n",
    "- [ ] Check semantic similarities\n",
    "    - [ ] What do you conclude from comparing vectors generated by yourself and the pretrained model? (see answer below)\n",
    "    - [ ] Which of the Word2Vec models seems to encode semantic similarities between words better? (see answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mean_sentences_vectorized\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "    print(len(mean_sentences_vectorized))\n",
    "    return mean_sentences_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "640\n",
      "Binary Pretrained Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_embeddings = np.array(pretrained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_embeddings = np.array(pretrained_binary_test_embeddings)\n",
    "\n",
    "pretrained_binary_train_embeddings.shape, pretrained_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "800\n",
      "Ternary Pretrained Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_embeddings = np.array(pretrained_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary Pretrained Test\")\n",
    "pretrained_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_embeddings = np.array(pretrained_ternary_test_embeddings)\n",
    "\n",
    "pretrained_ternary_train_embeddings.shape, pretrained_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "640\n",
      "Binary My Model Test\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_trained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_train_embeddings = np.array(my_trained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_trained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_test_embeddings = np.array(my_trained_binary_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "800\n",
      "Ternary My Model Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_train_embeddings = np.array(my_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary My Model Test\")\n",
    "my_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_test_embeddings = np.array(my_ternary_test_embeddings)\n",
    "\n",
    "my_ternary_train_embeddings.shape, my_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_word_embeddings(df: pd.DataFrame, col_name: str, model_to_use, about: str):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    aboout: `str`\n",
    "        Specifics of model, classification, and train/test\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    concatenated_vectors: list\n",
    "        List of concatenated vectors for each review (first 10 Word2Vec vectors)\n",
    "    \"\"\"\n",
    "    print(\"About:\", about)\n",
    "    mean_concatenated_vectors = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vectorized_words = []\n",
    "        words = sentence.split(\" \")[:10]  # Select the first 10 words\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "            vectorized_words.append(vector_of_word)\n",
    "        \n",
    "        concatenated_features = np.concatenate(vectorized_words, axis=0)\n",
    "        # Ensure dimensionality of 300 for each sentence\n",
    "        if concatenated_features.shape[0] < 300:\n",
    "            # If concatenated_features has less than 300 dimensions, pad it with zeros\n",
    "            concatenated_features = np.pad(concatenated_features, ((0, 300 - concatenated_features.shape[0]), (0, 0)), mode='constant')\n",
    "        elif concatenated_features.shape[0] > 300:\n",
    "            # If concatenated_features has more than 300 dimensions, truncate it\n",
    "            concatenated_features = concatenated_features[:300 :]\n",
    "            \n",
    "        mean_concatenated_vectors.append(concatenated_features)\n",
    "\n",
    "    mean_concatenated_vectors = np.array(mean_concatenated_vectors)\n",
    "    print(\"   Concat embeddings shape --- \", mean_concatenated_vectors.shape)\n",
    "    print()\n",
    "    return mean_concatenated_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary\n",
      "About: Pretrained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (640, 300)\n",
      "\n",
      "About: Pretrained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (160, 300)\n",
      "\n",
      "About: My trained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (640, 300)\n",
      "\n",
      "About: My trained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (160, 300)\n",
      "\n",
      "--- Ternary ---\n",
      "About: Pretrained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (800, 300)\n",
      "\n",
      "About: Pretrained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (200, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (800, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (200, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary\")\n",
    "pretrained_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")\n",
    "\n",
    "my_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Binary --- Test\")\n",
    "\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")\n",
    "\n",
    "my_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_feature_extraction(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Extract the TF-IDF features from the reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tf_idf_features:\n",
    "        A matrix containing the TF-IDF features extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_features = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    return tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_df = cleaned_reviews_df.dropna(subset=['binary_review_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features = tf_idf_feature_extraction(cleaned_reviews_df, 'lemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4399 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binnary_reviews = cleaned_reviews_df['binary_review_class']\n",
    "binnary_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tf_idf_features, binnary_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models\n",
    "\n",
    "**GOAL:** Train simple models below, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "\n",
    "- [ ] Train a perceptron and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "    - [ ] TF-IDF embeddings\n",
    "- [ ] Train a support vector machine (SVM) and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "    - [ ] TF-IDF embeddings\n",
    "- [ ] Compare\n",
    "    - [ ] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results_dict, w2v_type, method, classification, accuracy):\n",
    "    \"\"\"\n",
    "    Store results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    results_dict (dict): Dictionary to store the results.\n",
    "    w2v_type (str): Type of word2vec.\n",
    "    method (str): Method used.\n",
    "    classification (str): Type of classification.\n",
    "    accuracy (float): Accuracy of the classification.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the stored results.\n",
    "    \"\"\"\n",
    "    results_dict['W2V Type'].append(w2v_type)\n",
    "    results_dict['Method'].append(method)\n",
    "    results_dict['Classification'].append(classification)\n",
    "    results_dict['Accuracy'].append(accuracy)\n",
    "\n",
    "    return pd.DataFrame(results_dict)  # Return DataFrame with a single row\n",
    "\n",
    "results_dict = {'W2V Type': [], 'Method': [], 'Classification': [], 'Accuracy': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_prediction):\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_metric(y_train_true, y_train_predictions):\n",
    "    accuracy = eval_accuracy(y_train_true, y_train_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def test_eval_metric(y_test_true, y_test_predictions):\n",
    "    accuracy = eval_accuracy(y_test_true, y_test_predictions)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = Perceptron(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = perceptron_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.9953125}, {'Accuracy': 0.76875})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76875"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf_perceptron_test_metrics.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       W2V Type          Method Classification  Accuracy\n",
      "0  TF-IDF-train  Perceptron-avg         Binary  0.995313\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "\n",
    "results_df = store_results(results_dict, 'TF-IDF-train', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "perceptron_train_metrics, perceptron_test_metrics = perceptron_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.6921875}, {'Accuracy': 0.625})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_train_metrics, perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "results_df = store_results(results_dict, 'Pretrained-train', 'Perceptron-avg', 'Binary', list(perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'Perceptron-avg', 'Binary', list(perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300), (640,), (160,))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_train_embeddings.shape, my_trained_binary_test_embeddings.shape, binary_y_train.shape, binary_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "my_perceptron_train_metrics, my_perceptron_test_metrics = perceptron_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.61875}, {'Accuracy': 0.45625})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron_train_metrics, my_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'Perceptron-avg', 'Binary', list(my_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'Perceptron-avg', 'Binary', list(my_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = LinearSVC(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics = svm_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.9921875}, {'Accuracy': 0.80625})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'TF-IDF-train', 'SVM-avg', 'Binary', list(tfidf_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'SVM-avg', 'Binary', list(tfidf_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics = svm_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.8859375}, {'Accuracy': 0.76875})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained-train', 'SVM-avg', 'Binary', list(svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'SVM-avg', 'Binary', list(svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics = svm_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.8015625}, {'Accuracy': 0.55625})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5    My model-train         SVM-avg         Binary  0.801562\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'SVM-avg', 'Binary', list(my_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'SVM-avg', 'Binary', list(my_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks (FFNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "- [ ] Train a feedforward multilayer perceptron (MLP) network\n",
    "    - [ ] 2 hidden layers each with 50 and 10 nodes, respectively\n",
    "    - [ ] Cross entropy loss\n",
    "    - [ ] Decide other hyperparameters (ie: nonlinearity, #epochs, etc)\n",
    "- [ ] TUTORIAL: [Pytorch Multi-Layer Perceptron, MNIST](https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook) for image data\n",
    "\n",
    "---\n",
    "\n",
    "## (a) Average Embeddings\n",
    " \n",
    "- [ ] Train a FFNN and report accuracy on the testing split for on average embeddings for\n",
    "    - [ ] Pretrained \n",
    "    - [ ] My trained average embeddings\n",
    "- [ ] Train a FFNN and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "\n",
    "## (b) Concatenate Embeddings\n",
    "\n",
    "- [ ] Concatenate the first 10 Word2Vec vectors for each review\n",
    "\n",
    "- [ ] Compare\n",
    "    - [ ] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_review_class = binary_embeddings_df['binary_review_class']\n",
    "# binary_review_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the NN architecture\"\"\"\n",
    "\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # self.height = heightght = height\n",
    "        # self.width = width\n",
    "        # linear layer (1200 x 300 dot 300 x 50 -> 1200 x 50)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes) # change to 3 for ternery\n",
    "        # dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add hidden layer, with relu activation function, dropout, relu, dropout, output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        # set initial \"min\" to infinity\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass to compute predictions, loss, backward pass to compute gradient wrt model params\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            # print(\"train_loss:\", train_loss)\n",
    "            \n",
    "            # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            #     epoch+1, \n",
    "            #     train_loss,\n",
    "            #     ))\n",
    "            \n",
    "            # # save model if validation loss has decreased\n",
    "            # if train_loss <= valid_loss_min:\n",
    "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            #     valid_loss_min,\n",
    "            #     train_loss))\n",
    "            #     torch.save(self.state_dict(), 'nn_model.pt')\n",
    "            #     valid_loss_min = train_loss\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = self(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Convert predictions and ground truth lists to numpy arrays\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (predictions == ground_truth).mean()\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 300\n"
     ]
    }
   ],
   "source": [
    "N_binary_embeddings, d_binary_embeddings = my_trained_binary_train_embeddings.shape\n",
    "print(N_binary_embeddings, d_binary_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 0)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    print(data_tensor.size(), target_tensor.size())\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "    \n",
    "    print(len(dataset))\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 25\n",
    "    # number_of_epochs = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 0)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "    # print(len(data_tensor))\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.54375\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_pretrained_binary = predict_binary_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5    My model-train         SVM-avg         Binary  0.801562\n",
      "6        Pretrained         FFN-avg         Binary  0.543750\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Binary', acc_avg_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.525\n",
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5    My model-train         SVM-avg         Binary  0.801562\n",
      "6        Pretrained         FFN-avg         Binary  0.543750\n",
      "7          My model         FFN-avg         Binary  0.525000\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_trained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_my_binary = predict_binary_model(my_trained_binary_test_embeddings, binary_y_test)\n",
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Binary', acc_avg_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.575\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_pretrained_binary = predict_binary_model(pretrained_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5    My model-train         SVM-avg         Binary  0.801562\n",
      "6        Pretrained         FFN-avg         Binary  0.543750\n",
      "7          My model         FFN-avg         Binary  0.525000\n",
      "8        Pretrained      FFN-concat         Binary  0.575000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Binary', acc_concat_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_my_binary = predict_binary_model(my_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2    My model-train  Perceptron-avg         Binary  0.618750\n",
      "3      TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4  Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5    My model-train         SVM-avg         Binary  0.801562\n",
      "6        Pretrained         FFN-avg         Binary  0.543750\n",
      "7          My model         FFN-avg         Binary  0.525000\n",
      "8        Pretrained      FFN-concat         Binary  0.575000\n",
      "9          My model      FFN-concat         Binary  0.550000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Binary', acc_concat_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.185\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2     My model-train  Perceptron-avg         Binary  0.618750\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4   Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5     My model-train         SVM-avg         Binary  0.801562\n",
      "6         Pretrained         FFN-avg         Binary  0.543750\n",
      "7           My model         FFN-avg         Binary  0.525000\n",
      "8         Pretrained      FFN-concat         Binary  0.575000\n",
      "9           My model      FFN-concat         Binary  0.550000\n",
      "10        Pretrained         FFN-avg        Ternary  0.185000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Ternary', acc_avg_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.365\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_my_ternary = predict_ternary_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2     My model-train  Perceptron-avg         Binary  0.618750\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4   Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5     My model-train         SVM-avg         Binary  0.801562\n",
      "6         Pretrained         FFN-avg         Binary  0.543750\n",
      "7           My model         FFN-avg         Binary  0.525000\n",
      "8         Pretrained      FFN-concat         Binary  0.575000\n",
      "9           My model      FFN-concat         Binary  0.550000\n",
      "10        Pretrained         FFN-avg        Ternary  0.185000\n",
      "11          My model         FFN-avg        Ternary  0.365000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Ternary', acc_avg_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.39\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2     My model-train  Perceptron-avg         Binary  0.618750\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4   Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5     My model-train         SVM-avg         Binary  0.801562\n",
      "6         Pretrained         FFN-avg         Binary  0.543750\n",
      "7           My model         FFN-avg         Binary  0.525000\n",
      "8         Pretrained      FFN-concat         Binary  0.575000\n",
      "9           My model      FFN-concat         Binary  0.550000\n",
      "10        Pretrained         FFN-avg        Ternary  0.185000\n",
      "11          My model         FFN-avg        Ternary  0.365000\n",
      "12        Pretrained      FFN-concat        Ternary  0.390000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Ternary', acc_concat_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.39\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_my_ternary = predict_ternary_model(my_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  0.995313\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.692187\n",
      "2     My model-train  Perceptron-avg         Binary  0.618750\n",
      "3       TF-IDF-train         SVM-avg         Binary  0.992188\n",
      "4   Pretrained-train         SVM-avg         Binary  0.885938\n",
      "5     My model-train         SVM-avg         Binary  0.801562\n",
      "6         Pretrained         FFN-avg         Binary  0.543750\n",
      "7           My model         FFN-avg         Binary  0.525000\n",
      "8         Pretrained      FFN-concat         Binary  0.575000\n",
      "9           My model      FFN-concat         Binary  0.550000\n",
      "10        Pretrained         FFN-avg        Ternary  0.185000\n",
      "11          My model         FFN-avg        Ternary  0.365000\n",
      "12        Pretrained      FFN-concat        Ternary  0.390000\n",
      "13          My model      FFN-concat        Ternary  0.390000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Ternary', acc_concat_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks (CNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification\n",
    "\n",
    "---\n",
    "\n",
    "- [ ] 2 layer CNN with output channel sizes of 50 and 10, respectively\n",
    "- [ ] Limit each review to 50 words\n",
    "    - [ ] If more, truncate to 50\n",
    "    - [ ] If less pad with 0s to make 50\n",
    "- [ ] Use cross entropy\n",
    "- [ ] Select hyperparameters (ie: nonlinearity, #epochs, etc.) of my choosing\n",
    "- [ ] Train for binary classification\n",
    "- [ ] Train for ternary classification\n",
    "- [ ] Report for testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[# of reviews, 300 embedding] (above code) (works)\n",
    "\n",
    "[# of reviews, 50 words for each review, 300] (below code) (doesnt work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_word_embeddings(df: pd.DataFrame, col_name: str, max_sentence_length: int, model_to_use, about):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    conditioned_sequences\n",
    "    \"\"\"\n",
    "    print(\"About: \", about)\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    concatenated_features = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx, sentence)\n",
    "        words = sentence.split(\" \")\n",
    "        # print(\"Before -- \", len(words))\n",
    "        if len(words) > max_sentence_length:\n",
    "            words = words[:max_sentence_length]\n",
    "        elif len(words) < max_sentence_length:\n",
    "            words += [''] * (max_sentence_length - len(words))\n",
    "\n",
    "        # print(\"After -- \", len(words), words)\n",
    "\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "    # print(sentence_vectorized)\n",
    "        # print()\n",
    "    \n",
    "        \n",
    "    tensors = []\n",
    "    for words in sentence_vectorized:\n",
    "        tensors.append(torch.tensor(words))\n",
    "    padded_sequences = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
    "    print(\"Embeddings shape: \", padded_sequences.shape)\n",
    "    # padded_sequences_updated = np.array(padded_sequences)\n",
    "    # print(\"Embeddings shape: \", padded_sequences_updated.shape)\n",
    "    print()\n",
    "    return padded_sequences\n",
    "    # return sentence_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary ---\n",
      "About:  Pretrained --- Binary --- Train\n",
      "Embeddings shape:  torch.Size([640, 50, 300])\n",
      "\n",
      "About:  Pretrained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([160, 50, 300])\n",
      "\n",
      "About:  My trained --- Binary --- Train\n",
      "Embeddings shape:  torch.Size([640, 50, 300])\n",
      "\n",
      "About:  My trained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([160, 50, 300])\n",
      "\n",
      "--- Ternary ---\n",
      "About:  Pretrained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([800, 50, 300])\n",
      "\n",
      "About:  Pretrained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([200, 50, 300])\n",
      "\n",
      "About:  My trained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([800, 50, 300])\n",
      "\n",
      "About:  My trained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([200, 50, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Binary ---\")\n",
    "pretrained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")\n",
    "\n",
    "my_trained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_trained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Test\")\n",
    "\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")\n",
    "\n",
    "my_trained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_trained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_load_data(embeddings: list, y_true: list, classification_type: str, about: str):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    \n",
    "    \"\"\"Mapping\n",
    "    \n",
    "    1 --- (positive) --- 0 \n",
    "    2 --- (negative) --- 1 \n",
    "    3 --- (neutral)  --- 2 \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\nAbout: \", about, \"\\n\")\n",
    "    print(\"Original mappings --- \", y_true.unique())\n",
    "    if classification_type == \"binary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "\n",
    "    elif classification_type == \"ternary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "        map_y_true_values = map_y_true_values.replace(3, 2)\n",
    "        # print(\"Remappings for \", classification_type, \"---\", map_y_true_values.unique())\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid classification type\")\n",
    "\n",
    "    print(\"Remappings        --- \", map_y_true_values.unique())\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    y_true_tensor = torch.tensor(map_y_true_values.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(embeddings_tensor, y_true_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)    \n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_ternary_train_50_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "About:  Pretrained x Binary x Train \n",
      "\n",
      "Original mappings ---  [2. 1.]\n",
      "Remappings        ---  [1. 0.]\n",
      "\n",
      "About:  Pretrained x Binary x Test \n",
      "\n",
      "Original mappings ---  [1. 2.]\n",
      "Remappings        ---  [0. 1.]\n",
      "\n",
      "About:  My trained x Binary x Train \n",
      "\n",
      "Original mappings ---  [2. 1.]\n",
      "Remappings        ---  [1. 0.]\n",
      "\n",
      "About:  My trained x Binary x Test \n",
      "\n",
      "Original mappings ---  [1. 2.]\n",
      "Remappings        ---  [0. 1.]\n",
      "\n",
      "--- Ternary ---\n",
      "\n",
      "About:  Pretrained x Ternary x Train \n",
      "\n",
      "Original mappings ---  [2. 3. 1.]\n",
      "Remappings        ---  [1. 2. 0.]\n",
      "\n",
      "About:  Pretrained x Ternary x Test \n",
      "\n",
      "Original mappings ---  [3. 1. 2.]\n",
      "Remappings        ---  [2. 0. 1.]\n",
      "\n",
      "About:  My trained x Ternary x Test \n",
      "\n",
      "Original mappings ---  [2. 3. 1.]\n",
      "Remappings        ---  [1. 2. 0.]\n",
      "\n",
      "About:  My trained x Ternary x Test \n",
      "\n",
      "Original mappings ---  [3. 1. 2.]\n",
      "Remappings        ---  [2. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_9372/2017019957.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# print(\"--- Binary ---\")\n",
    "pretrained_binary_train_loader = format_and_load_data(pretrained_binary_train_50_embeddings, binary_y_train, \"binary\", \"Pretrained x Binary x Train\")\n",
    "pretrained_binary_test_loader = format_and_load_data(pretrained_binary_test_50_embeddings, binary_y_test, \"binary\", \"Pretrained x Binary x Test\")\n",
    "\n",
    "my_trained_binary_train_loader = format_and_load_data(my_trained_binary_train_50_embeddings, binary_y_train, \"binary\", \"My trained x Binary x Train\")\n",
    "my_trained_binary_test_loader = format_and_load_data(my_trained_binary_test_50_embeddings, binary_y_test, \"binary\", \"My trained x Binary x Test\")\n",
    "\n",
    "print()\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_loader = format_and_load_data(pretrained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"Pretrained x Ternary x Train\")\n",
    "pretrained_ternary_test_loader = format_and_load_data(pretrained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"Pretrained x Ternary x Test\")\n",
    "\n",
    "my_trained_ternary_train_loader = format_and_load_data(my_trained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"My trained x Ternary x Test\")\n",
    "my_trained_ternary_test_loader = format_and_load_data(my_trained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"My trained x Ternary x Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_channels = 300\n",
    "# embedding_size = 300\n",
    "# output_channels1 = 50\n",
    "# output_channels2 = 10\n",
    "# kernel_size1 = 3\n",
    "# kernel_size2 = 3 \n",
    "# num_classes = 2\n",
    "\n",
    "# nn.Conv1d(16, 33, 3, stride=2)\n",
    "# nn.Conv1d(640, 50, 300) --> (num_channels=300, output_channels1=50, kernel_size1=3)\n",
    "# (num_channels=300, output_channels1=50, kernel_size1=3) --> (num_channels=50, output_channels1=10, kernel_size2=3)\n",
    "\n",
    "# nn.Linear(2960, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=output_channels1, kernel_size=kernel_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=kernel_size2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # to calc 460, refer to docs and write my own utils func\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(460, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Before permute ---\", x.shape)\n",
    "        x = x.permute(0, 2, 1) \n",
    "        # print(\"After permute ---\", x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"After Conv layers ---\", x.shape)\n",
    "        # print(\"1st\", x.shape)\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"After Flatten ---\", x.shape)\n",
    "        x = self.fc_layers(x)\n",
    "        # print(\"After Fully connected layers ---\", x.shape)\n",
    "        # print()\n",
    "        return x\n",
    "\n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader, test_loader):\n",
    "        for epoch in range(number_of_epochs):\n",
    "            print(\"Epoch --- \", epoch)\n",
    "            correct_train = 0.0\n",
    "            total_train_loss = 0.0\n",
    "            \n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "    \n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # predictions\n",
    "                output = self(data)\n",
    "                # print(\"Output\", output.shape)\n",
    "                \n",
    "                # target = target.squeeze()\n",
    "                \n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                correct_train += (predicted == target).sum().item()\n",
    "               \n",
    "            avg_total_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "            print(\"Train avg total loss --- \", avg_total_train_loss)\n",
    "        \n",
    "            total_evaluation_loss = 0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    output = self(data)\n",
    "                    # target = target.squeeze()\n",
    "                    loss = criterion_function(output, target)\n",
    "                    total_evaluation_loss += loss.item() * data.size(0)\n",
    "                    \n",
    "            avg_total_evaluation_loss = total_evaluation_loss / len(train_loader.dataset)\n",
    "            print(\"Evaluation avg total loss --- \", avg_total_evaluation_loss)\n",
    "            \n",
    "            train_accuracy = correct_train / len(train_loader.dataset)\n",
    "            print(f'Epoch: {epoch+1} \\tTraining Loss: {loss:.6f} \\tTraining Accuracy: {train_accuracy * 100:.2f}%')\n",
    "            print()\n",
    "        print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 50, 300)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, N_words, embedding_size = pretrained_binary_train_50_embeddings.shape\n",
    "N, N_words, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=460, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# channels: for each word has 300 dims, so 300 features\n",
    "num_channels = 300\n",
    "\n",
    "output_channels1 = 50\n",
    "output_channels2 = 10\n",
    "kernel_size1 = 3\n",
    "kernel_size2 = 3 \n",
    "num_classes = 2  # Number of output classes, adjust as needed\n",
    "\n",
    "cnn_net_model = CNN_Net(embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes)\n",
    "print(cnn_net_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IGNORE: Loaders to use. Defined and set as a variable above\n",
    "\n",
    "# --- Binary ---\n",
    "# pretrained_binary_train_loader\n",
    "# pretrained_binary_test_loader\n",
    "\n",
    "# my_trained_binary_train_loader\n",
    "# my_trained_binary_test_loader\n",
    "\n",
    "# --- Ternary ---\n",
    "# pretrained_ternary_train_loader\n",
    "# pretrained_ternary_test_loader\n",
    "\n",
    "# my_trained_ternary_train_loader\n",
    "# my_trained_ternary_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-train_binary_cnn_model\n",
      "Epoch ---  0\n",
      "Train avg total loss ---  0.6931009232997895\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 1 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  1\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 2 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  2\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 3 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  3\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 4 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  4\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 5 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  5\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 6 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  6\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 7 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  7\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 8 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  8\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 9 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  9\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 10 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_net_model.parameters(), lr=1, momentum=0.9)\n",
    "# print(\"2-train_binary_cnn_model\")\n",
    "output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, pretrained_binary_train_loader, pretrained_binary_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP HERE --- IGNORE BELOW #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Binary', acc_50_pretrained_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_binary_cnn_model(my_binary_train_embeddings, binary_y_train)\n",
    "# acc_50_my_binary = predict_binary_cnn_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'My model', 'CNN-500', 'Binary', acc_50_my_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = CNN_Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_cnn_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 30\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_cnn_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = cnn_net_model.cnn_predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "# acc_50_pretrained_ternary = predict_ternary_cnn_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Ternary', acc_50_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_50_my_ternary = predict_ternary_cnn_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'CNN-50', 'Ternary', acc_50_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape) # [batch_size = 64 , number_of_class (ternary = 3), (bin = 2) ]\n",
    "#             print(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
