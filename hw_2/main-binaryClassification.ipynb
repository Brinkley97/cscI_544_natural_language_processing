{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2- Binary and Ternary Classification for Sentiment Analysis\n",
    "- Detravious Jamari Brinkley\n",
    "- CSCI-544: Applied Natural Language Processing\n",
    "- python version: 3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/brinkley97/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import sklearn\n",
    "import gensim.models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim import utils\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.test.utils import datapath\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "pretrained_word_two_vec_model = api.load('word2vec-google-news-300')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HW Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset Generation\n",
    "    1. Read data\n",
    "    2. Keep reviews and ratings\n",
    "    3. Create binary and ternary classes\n",
    "    4. Clean data steps\n",
    "    4. Clean data function\n",
    "    5. Split data\n",
    "    6. Load pretrained model and train my model\n",
    "        - Get similarity for the pretrained model\n",
    "        - Get similarity for my trained model\n",
    "2. Word Embedding\n",
    "    - Get word embeddings for pretrained model\n",
    "    - Get word embeddings for my model\n",
    "3. Simple models\n",
    "    - Get accuracy for perceptron on pretrained model\n",
    "    - Get accuracy for svm on pretrained model\n",
    "    - Get accuracy for perceptron on my model\n",
    "    - Get accuracy for svm on my model\n",
    "    - What do I conclude from comparing performances\n",
    "4. Feedforward Neural Networks (FFNN)\n",
    "5. Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation\n",
    "\n",
    "- [x] Load the Amazon reviews dataset\n",
    "- [x] Build a balanced dataset of 250K reviews along with their ratings through random selection\n",
    "    - [x] Rating 1: 50K instances\n",
    "    - [x] Rating 2: 50K instances\n",
    "    - [x] Rating 3: 50K instances\n",
    "    - [x] Rating 4: 50K instances\n",
    "    - [x] Rating 5: 50K instances\n",
    "- [x] Create ternary labels using the ratings\n",
    "    - [x] Class 1: Ratings 4 and 5 (positive sentiment)\n",
    "    - [x] Class 2: Ratings 1 and 2 (negative sentiment)\n",
    "    - [x] Class 3: Rating 3 (neutral sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../datasets/amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "amazon_reviews_copy_df = pd.read_csv(dataset, sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                                     Great product.\n",
       "1                 5  What's to say about this commodity item except...\n",
       "2                 5    Haven't used yet, but I am sure I will like it.\n",
       "3                 1  Although this was labeled as &#34;new&#34; the...\n",
       "4                 4                    Gorgeous colors and easy to use\n",
       "...             ...                                                ...\n",
       "2640249           4  I can't live anymore whithout my Palm III. But...\n",
       "2640250           4  Although the Palm Pilot is thin and compact it...\n",
       "2640251           4  This book had a lot of great content without b...\n",
       "2640252           5  I am teaching a course in Excel and am using t...\n",
       "2640253           5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640254 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ratings_df = amazon_reviews_copy_df.loc[0:, ['star_rating', 'review_body']]\n",
    "reviews_ratings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_reviews(df: pd.DataFrame, review_col_name: str, number_of_reviews: int = 3):\n",
    "    \"\"\"Include reviews and ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    review_col_name: `str`\n",
    "        The specific_column to get the reviews and ratings of\n",
    "    \n",
    "    number_of_reviews: `int`\n",
    "        Number of samples to include\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Nothing; instead, print the reviews with ratings\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    columns_to_include = [review_col_name, 'star_rating']\n",
    "\n",
    "    # Initialize an empty list to store dictionaries\n",
    "    list_of_dicts = []\n",
    "\n",
    "    # Iterate over the specified columns and retrieve the first three rows\n",
    "    for row in df[columns_to_include].head(3).to_dict(orient='records'):\n",
    "        list_of_dicts.append({'star_rating': row['star_rating'], review_col_name: row[review_col_name]})\n",
    "\n",
    "    for dictionary in list_of_dicts:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " ## Create binary and ternary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_type(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Update the data type of the star ratings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with rating values\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ratings = ['1','2','3','4','5']\n",
    "    star_rating_series = df[col_name].copy()\n",
    "\n",
    "    # Convert type to strings\n",
    "    star_rating_series.astype('str')\n",
    "\n",
    "    # Check valid list and see which of our stars match\n",
    "    rows = star_rating_series.index\n",
    "    is_rating_in_valid_ratings = rows[star_rating_series.isin(valid_ratings)]\n",
    "\n",
    "    # Convert to list\n",
    "    is_rating_in_valid_ratings = is_rating_in_valid_ratings.to_list()\n",
    "\n",
    "    updated_df = df.iloc[is_rating_in_valid_ratings]\n",
    "    updated_df[col_name] = updated_df[col_name].astype(int)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_8552/2907883124.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[col_name] = updated_df[col_name].astype(int)\n"
     ]
    }
   ],
   "source": [
    "updated_reviews_ratings_df = update_data_type(reviews_ratings_df, 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640237 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>4</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>4</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>4</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>5</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>5</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "0                  5                                     Great product.\n",
       "1                  5  What's to say about this commodity item except...\n",
       "2                  5    Haven't used yet, but I am sure I will like it.\n",
       "3                  1  Although this was labeled as &#34;new&#34; the...\n",
       "4                  4                    Gorgeous colors and easy to use\n",
       "...              ...                                                ...\n",
       "2640249            4  I can't live anymore whithout my Palm III. But...\n",
       "2640250            4  Although the Palm Pilot is thin and compact it...\n",
       "2640251            4  This book had a lot of great content without b...\n",
       "2640252            5  I am teaching a course in Excel and am using t...\n",
       "2640253            5  A very comprehensive layout of exactly how Vis...\n",
       "\n",
       "[2640080 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_reviews_ratings_df = updated_reviews_ratings_df.dropna()\n",
    "updated_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         star_rating  review_body\n",
      "0              False        False\n",
      "1              False        False\n",
      "2              False        False\n",
      "3              False        False\n",
      "4              False        False\n",
      "...              ...          ...\n",
      "2640249        False        False\n",
      "2640250        False        False\n",
      "2640251        False        False\n",
      "2640252        False        False\n",
      "2640253        False        False\n",
      "\n",
      "[2640080 rows x 2 columns]\n",
      "There are no NaN values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "nan_check = updated_reviews_ratings_df.isna()\n",
    "\n",
    "# Display the DataFrame with True where NaN values exist\n",
    "print(nan_check)\n",
    "\n",
    "# Check if any NaN value exists in the DataFrame\n",
    "if nan_check.any().any():\n",
    "    print(\"There are NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviews per rating star_rating\n",
      "5    1582704\n",
      "4     418348\n",
      "1     306967\n",
      "3     193680\n",
      "2     138381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"# reviews per rating\", updated_reviews_ratings_df['star_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_star_ratings(df: pd.DataFrame, col_name: str, star_value: int, number_of_reviews: int):\n",
    "    \"\"\"Build a subset balanced dataset with reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The dataframe to use\n",
    "    col_name: `str`\n",
    "        The name of the column to get reviews from\n",
    "    star_value: `int`\n",
    "        The star rating of the review\n",
    "    number_of_reviews: `int`\n",
    "        The number of sub reviews to include in sample\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    rating_df, sampled_rating_df: `tuple`\n",
    "        All reviews with that rating and the subset reviews with that rating\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_df = df[df[col_name] == star_value]\n",
    "    sampled_rating_df = rating_df.sample(n=number_of_reviews)\n",
    "    return rating_df, sampled_rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ ] Change #reviews per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_reviews = 50000\n",
    "subset_reviews = 200\n",
    "\n",
    "one_star = 1\n",
    "rating_one, rating_one_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', one_star, subset_reviews)\n",
    "two_stars = 2\n",
    "rating_two, rating_two_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', two_stars, subset_reviews)\n",
    "three_stars = 3\n",
    "rating_three, rating_three_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', three_stars, subset_reviews)\n",
    "four_stars = 4\n",
    "rating_four, rating_four_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', four_stars, subset_reviews)\n",
    "five_stars = 5\n",
    "rating_five, rating_five_sampled = sample_star_ratings(updated_reviews_ratings_df, 'star_rating', five_stars, subset_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_df = pd.concat([rating_one_sampled, rating_two_sampled, rating_three_sampled, rating_four_sampled, rating_five_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2281403</th>\n",
       "      <td>1</td>\n",
       "      <td>I've tried three times (original order) and tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124023</th>\n",
       "      <td>1</td>\n",
       "      <td>This ink actually does run out extremely quick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747477</th>\n",
       "      <td>1</td>\n",
       "      <td>This SAYS it fits the HL-2230 but it does not!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330805</th>\n",
       "      <td>1</td>\n",
       "      <td>The pen is very pretty, it is heavy and has a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553026</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had the Panasonic KX-TG2000B for years wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760957</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933182</th>\n",
       "      <td>5</td>\n",
       "      <td>The batteries that came new with my Uniden pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331851</th>\n",
       "      <td>5</td>\n",
       "      <td>They are a bit big but they write very neatly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587176</th>\n",
       "      <td>5</td>\n",
       "      <td>This printer replaced my HP LaserJet 2200DN &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358613</th>\n",
       "      <td>5</td>\n",
       "      <td>The phones are working very well.  Thank you f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "2281403            1  I've tried three times (original order) and tw...\n",
       "1124023            1  This ink actually does run out extremely quick...\n",
       "1747477            1  This SAYS it fits the HL-2230 but it does not!...\n",
       "1330805            1  The pen is very pretty, it is heavy and has a ...\n",
       "2553026            1  I've had the Panasonic KX-TG2000B for years wi...\n",
       "...              ...                                                ...\n",
       "760957             5                                       Good Product\n",
       "933182             5  The batteries that came new with my Uniden pho...\n",
       "1331851            5  They are a bit big but they write very neatly ...\n",
       "1587176            5  This printer replaced my HP LaserJet 2200DN & ...\n",
       "1358613            5  The phones are working very well.  Thank you f...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_reviews_by_rating(df: pd.DataFrame, rating_col: str, threshold: int, sentiment_type: str):\n",
    "    \"\"\"Categorizes reviews by adding a rating\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    rating_col: `str`\n",
    "        Column with rating values\n",
    "    \n",
    "    threshold: `int`\n",
    "        Where to split the ratings such that categories can be formed\n",
    "\n",
    "    sentiment_type: `str`\n",
    "        One of three types of sentiment: positive, negative, or neural\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the new sentiment appened\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if sentiment_type == 'positive_review_class':\n",
    "        positive_review_threshold = df[rating_col].astype('int32') > threshold\n",
    "        df = df[positive_review_threshold]\n",
    "        df[sentiment_type] = 1\n",
    "\n",
    "    elif sentiment_type == 'negative_review_class':\n",
    "        negative_review_threshold = df[rating_col].astype('int32') < threshold\n",
    "        df = df[negative_review_threshold]\n",
    "        df[sentiment_type] = 2\n",
    "\n",
    "    elif sentiment_type == 'neutral_review_class':\n",
    "        neutral_review_threshold = df[rating_col].astype('int32') == threshold\n",
    "        df = df[neutral_review_threshold]\n",
    "        df[sentiment_type] = 3\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_8552/1400069123.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 2\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_8552/1400069123.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 3\n",
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_8552/1400069123.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[sentiment_type] = 1\n"
     ]
    }
   ],
   "source": [
    "negative_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'negative_review_class')\n",
    "neutral_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'neutral_review_class')\n",
    "positive_review_class_df = separate_reviews_by_rating(sampled_reviews_df, 'star_rating', 3, 'positive_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2281403</th>\n",
       "      <td>1</td>\n",
       "      <td>I've tried three times (original order) and tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124023</th>\n",
       "      <td>1</td>\n",
       "      <td>This ink actually does run out extremely quick...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747477</th>\n",
       "      <td>1</td>\n",
       "      <td>This SAYS it fits the HL-2230 but it does not!...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330805</th>\n",
       "      <td>1</td>\n",
       "      <td>The pen is very pretty, it is heavy and has a ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553026</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had the Panasonic KX-TG2000B for years wi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760957</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933182</th>\n",
       "      <td>5</td>\n",
       "      <td>The batteries that came new with my Uniden pho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331851</th>\n",
       "      <td>5</td>\n",
       "      <td>They are a bit big but they write very neatly ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587176</th>\n",
       "      <td>5</td>\n",
       "      <td>This printer replaced my HP LaserJet 2200DN &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358613</th>\n",
       "      <td>5</td>\n",
       "      <td>The phones are working very well.  Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2281403            1  I've tried three times (original order) and tw...   \n",
       "1124023            1  This ink actually does run out extremely quick...   \n",
       "1747477            1  This SAYS it fits the HL-2230 but it does not!...   \n",
       "1330805            1  The pen is very pretty, it is heavy and has a ...   \n",
       "2553026            1  I've had the Panasonic KX-TG2000B for years wi...   \n",
       "...              ...                                                ...   \n",
       "760957             5                                       Good Product   \n",
       "933182             5  The batteries that came new with my Uniden pho...   \n",
       "1331851            5  They are a bit big but they write very neatly ...   \n",
       "1587176            5  This printer replaced my HP LaserJet 2200DN & ...   \n",
       "1358613            5  The phones are working very well.  Thank you f...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \n",
       "2281403                    2.0                   NaN                    NaN  \n",
       "1124023                    2.0                   NaN                    NaN  \n",
       "1747477                    2.0                   NaN                    NaN  \n",
       "1330805                    2.0                   NaN                    NaN  \n",
       "2553026                    2.0                   NaN                    NaN  \n",
       "...                        ...                   ...                    ...  \n",
       "760957                     NaN                   NaN                    1.0  \n",
       "933182                     NaN                   NaN                    1.0  \n",
       "1331851                    NaN                   NaN                    1.0  \n",
       "1587176                    NaN                   NaN                    1.0  \n",
       "1358613                    NaN                   NaN                    1.0  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df = pd.concat([negative_review_class_df, neutral_review_class_df, positive_review_class_df])\n",
    "sampled_reviews_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_df = sampled_reviews_ratings_df['negative_review_class'].dropna()\n",
    "neutral_reviews_df = sampled_reviews_ratings_df['neutral_review_class'].dropna()\n",
    "positive_reviews_df = sampled_reviews_ratings_df['positive_review_class'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'] = pd.concat([negative_reviews_df, positive_reviews_df])\n",
    "sampled_reviews_ratings_df['ternary_review_class'] = pd.concat([negative_reviews_df, neutral_reviews_df, positive_reviews_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., nan,  1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews_ratings_df['binary_review_class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_to_lower_case(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Convert all reviews to lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the lower cased reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_case_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    \n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        # print(text_reviews_idx, type(text_review), text_review)\n",
    "\n",
    "        # NOT all reviews are strings, thus all can't be converted to lower cased\n",
    "        if type(text_review) != str:\n",
    "            print(True, text_review)\n",
    "            converted_str = str(text_review)\n",
    "            lower_case_reviews.append(text_review)\n",
    "         \n",
    "        else:\n",
    "            update_text_review = text_review.lower()\n",
    "            lower_case_reviews.append(update_text_review)\n",
    "\n",
    "    updated_df['lower_cased'] = lower_case_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased = convert_reviews_to_lower_case(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"reviews_lower_cased:\")\n",
    "# generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove HTML and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_and_urls(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove HTML and URLs from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the html_and_urls removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # url_pattern = re.compile(r'https?://\\S+|www\\. \\S+')\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            # Check and remove HTML tags\n",
    "            has_html = bool(re.search('<.*?>', text_review))\n",
    "            if has_html == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "\n",
    "            no_html_review = re.sub('<.*?>', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML -- \", no_html_review)\n",
    "        \n",
    "            # Check and remove URLs\n",
    "            has_url = bool(re.search(r'http\\S+', no_html_review))\n",
    "            if has_url == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has URL --\", no_html_review)\n",
    "                pass\n",
    "\n",
    "            no_html_url_review = re.sub(r'http\\S+', '', no_html_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without HTML, URL -- \", no_html_url_review)\n",
    "            # print()\n",
    "            cleaned_reviews.append(no_html_url_review)\n",
    "        else:\n",
    "            # print(text_reviews_idx, text_review)\n",
    "            cleaned_reviews.append(text_review)\n",
    "            \n",
    "\n",
    "    updated_df['without_html_urls'] = cleaned_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_html_urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_html_urls:\")\n",
    "# generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when'll\": \"when will\",\n",
    "    \"when'd\": \"when would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where'll\": \"where will\",\n",
    "    \"where'd\": \"where would\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why'll\": \"why will\",\n",
    "    \"why'd\": \"why would\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how'd\": \"how would\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_and_replace_contractions(review):\n",
    "    \"\"\"Find the contractions to replace from a specific review\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    review: `str`\n",
    "        A specific review\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    non_contraction_review: `str`\n",
    "        The updated specific review with contractions expanded\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(review, str):\n",
    "        get_words = review.split()\n",
    "\n",
    "        store_non_contraction_words = []\n",
    "\n",
    "        for word in get_words:\n",
    "            if word in store_contractions:\n",
    "                non_contraction_form = store_contractions[word]\n",
    "                # print(word, \"-->\", non_contraction_form)\n",
    "\n",
    "                store_non_contraction_words.append(non_contraction_form)\n",
    "\n",
    "            else:\n",
    "                # print(word)\n",
    "                store_non_contraction_words.append(word)\n",
    "\n",
    "        non_contraction_review = ' '.join(store_non_contraction_words)\n",
    "        return non_contraction_review\n",
    "    else:\n",
    "        return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove contractions from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_contractions_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"with possible contraction(s) -- \", text_review)\n",
    "\n",
    "        without_contraction = locate_and_replace_contractions(text_review)\n",
    "\n",
    "        # print(\"Review\", text_reviews_idx, \"without contraction -- \", without_contraction)\n",
    "        # print()\n",
    "\n",
    "        without_contractions_reviews.append(without_contraction)\n",
    "\n",
    "    updated_df['without_contractions'] = without_contractions_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_contractions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_contractions:\")\n",
    "# generate_sample_reviews(no_contractions_df, 'without_contractions', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove Non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove Non-alphabetical characters from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the non-alphabetical characters removed\n",
    "    \"\"\"\n",
    "\n",
    "    alphabetical_char_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "        \n",
    "        if isinstance(text_review, str):\n",
    "\n",
    "            # Check for non-alphabetical characters\n",
    "            has_non_alphabetical_char = bool(re.search(r'[^a-zA-Z]', text_review))\n",
    "            if has_non_alphabetical_char == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has HTML -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove non-alphabetical characters\n",
    "            with_alphabetical_char = re.sub(r'[^a-zA-Z\\s]', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"has HTML -- \", with_alphabetical_char)\n",
    "            alphabetical_char_reviews.append(with_alphabetical_char)\n",
    "        else:\n",
    "            alphabetical_char_reviews.append(text_review)\n",
    "\n",
    "    updated_df['with_alpha_chars_only'] = alphabetical_char_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_alpha_chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"with_alpha_chars_only:\")\n",
    "# generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Remove extra spaces from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    single_spaced_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "    # print(text_reviews)\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "        # Check if there are any extra spaces\n",
    "            has_extra_space = bool(re.search(r' +', text_review))\n",
    "            if has_extra_space == True:\n",
    "                # print(\"Review\", text_reviews_idx, \"has extra space -- \", text_review)\n",
    "                pass\n",
    "            \n",
    "            # Remove extra spaces\n",
    "            single_spaced_review = re.sub(r' +', ' ', text_review)\n",
    "            # print(\"Review\", text_reviews_idx, \"without extra space -- \", single_spaced_review)\n",
    "            # print()\n",
    "            \n",
    "            single_spaced_reviews.append(single_spaced_review)\n",
    "        else:\n",
    "            single_spaced_reviews.append(text_review)\n",
    "\n",
    "    updated_df['without_extra_space'] = single_spaced_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_extra_space_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_extra_space:\")\n",
    "# generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop_words(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Filter stop words out from all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    without_stop_words_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]\n",
    "\n",
    "        if isinstance(text_review, str):\n",
    "            text_review_words = word_tokenize(text_review) \n",
    "\n",
    "        \n",
    "\n",
    "            # print(\"Before stop word removal\", text_reviews_idx, \" -- \", text_review)\n",
    "\n",
    "            filtered_review = []\n",
    "\n",
    "            for text_review_words_idx in range(len(text_review_words)):\n",
    "                text_review_word = text_review_words[text_review_words_idx]\n",
    "                \n",
    "                # Check if review word is a stop word\n",
    "                if text_review_word in stop_words:\n",
    "                    # print(\"  Stop word -- \", text_review_word)\n",
    "                    pass\n",
    "                else:\n",
    "                    # print(text_review_word, \" -- is NOT a stop word in review\")\n",
    "                    filtered_review.append(text_review_word)\n",
    "\n",
    "            \n",
    "            filtered_review = \" \".join(filtered_review)\n",
    "            # print(\"After stop word removal\", text_reviews_idx, \" -- \", filtered_review)\n",
    "            # print()\n",
    "            \n",
    "            without_stop_words_reviews.append(filtered_review)\n",
    "        else:\n",
    "            without_stop_words_reviews.append(text_review)\n",
    "        \n",
    "\n",
    "    updated_df['without_stop_words'] = without_stop_words_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_stop_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_stop_words:\")\n",
    "# generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform lemmatization  \n",
    "\n",
    "- \"A sentence with many words\"\n",
    "    - \"words\" -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmentize_review(df:pd.DataFrame, col_name: str):\n",
    "    \"\"\"Lemmentize all reviews\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: `pd.DataFrame`\n",
    "        An updated DataFrame with the extra spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmed_reviews = []\n",
    "    updated_df = df.copy()\n",
    "    text_reviews = df[col_name].values\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    for text_reviews_idx in range(len(text_reviews)):\n",
    "        text_review = text_reviews[text_reviews_idx]   \n",
    "        if isinstance(text_review, str):     \n",
    "            words_in_review = word_tokenize(text_review) \n",
    "\n",
    "            # print(\"Before lem update\", text_reviews_idx, \" -- \", text_review)\n",
    "            # print(\"Lemmed words\", words_in_review)\n",
    "            \n",
    "\n",
    "            lemmed_sentence = []\n",
    "\n",
    "            # Split review into words\n",
    "            for lemmed_words_idx in range(len(words_in_review)):\n",
    "                word = words_in_review[lemmed_words_idx]\n",
    "                \n",
    "                apply_lemmatization = lem.lemmatize(word)\n",
    "                # print(apply_lemmatization)\n",
    "                \n",
    "                lemmed_sentence.append(apply_lemmatization)\n",
    "                filtered_review = \" \".join(lemmed_sentence)\n",
    "        \n",
    "            # print(\"After lem update -- \", filtered_review)\n",
    "            # print()\n",
    "\n",
    "            lemmed_reviews.append(filtered_review)\n",
    "        else:\n",
    "            lemmed_reviews.append(text_review)\n",
    "\n",
    "    updated_df['lemmed_reviews'] = lemmed_reviews\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"without_unlemmed_words:\")\n",
    "# generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clean data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, col_name):\n",
    "    \"\"\"Perform lower case, remove HTML and URLs, remove contractions, remove non-alphabetical characters, remove extra spaces, remove stop words, and lemmatize\"\"\"\n",
    "\n",
    "    print(\"original reviews:\")\n",
    "    # generate_sample_reviews(df, col_name, 3)\n",
    "\n",
    "    reviews_lower_cased = convert_reviews_to_lower_case(df, col_name)\n",
    "    print(\"reviews_lower_cased:\")\n",
    "    # generate_sample_reviews(reviews_lower_cased, 'lower_cased', 3)\n",
    "\n",
    "    no_html_urls_df = remove_html_and_urls(reviews_lower_cased, 'lower_cased')\n",
    "    print(\"without_html_urls:\")\n",
    "    # generate_sample_reviews(no_html_urls_df, 'without_html_urls', 3)\n",
    "\n",
    "    no_contractions_df = remove_contractions(no_html_urls_df, 'without_html_urls')\n",
    "    print(\"without_contractions:\")\n",
    "    # generate_sample_reviews(no_contractions_df, 'without_contractions', 3)\n",
    "\n",
    "    only_alpha_chars_df = remove_non_alphabetical_characters(no_contractions_df, 'without_contractions')\n",
    "    print(\"with_alpha_chars_only:\")\n",
    "    # generate_sample_reviews(only_alpha_chars_df, 'with_alpha_chars_only', 3)\n",
    "\n",
    "    no_extra_space_df = remove_extra_spaces(only_alpha_chars_df, 'with_alpha_chars_only')\n",
    "    print(\"without_extra_space:\")\n",
    "    # generate_sample_reviews(no_extra_space_df, 'without_extra_space', 3)\n",
    "\n",
    "    no_stop_words_df = filter_stop_words(no_extra_space_df, 'without_extra_space')\n",
    "    print(\"without_stop_words:\")\n",
    "    # generate_sample_reviews(no_stop_words_df, 'without_stop_words', 3)\n",
    "    \n",
    "    lemmed_df = lemmentize_review(no_stop_words_df, 'without_stop_words')\n",
    "    print(\"without_unlemmed_words:\")\n",
    "    # generate_sample_reviews(lemmed_df, 'lemmed_reviews', 3)\n",
    "\n",
    "    return lemmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original reviews:\n",
      "reviews_lower_cased:\n",
      "without_html_urls:\n",
      "without_contractions:\n",
      "with_alpha_chars_only:\n",
      "without_extra_space:\n",
      "without_stop_words:\n",
      "without_unlemmed_words:\n"
     ]
    }
   ],
   "source": [
    "cleaned_reviews_df = preprocess_data(sampled_reviews_ratings_df, 'review_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>negative_review_class</th>\n",
       "      <th>neutral_review_class</th>\n",
       "      <th>positive_review_class</th>\n",
       "      <th>binary_review_class</th>\n",
       "      <th>ternary_review_class</th>\n",
       "      <th>lower_cased</th>\n",
       "      <th>without_html_urls</th>\n",
       "      <th>without_contractions</th>\n",
       "      <th>with_alpha_chars_only</th>\n",
       "      <th>without_extra_space</th>\n",
       "      <th>without_stop_words</th>\n",
       "      <th>lemmed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2281403</th>\n",
       "      <td>1</td>\n",
       "      <td>I've tried three times (original order) and tw...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i've tried three times (original order) and tw...</td>\n",
       "      <td>i've tried three times (original order) and tw...</td>\n",
       "      <td>I have tried three times (original order) and ...</td>\n",
       "      <td>I have tried three times  original order  and ...</td>\n",
       "      <td>I have tried three times original order and tw...</td>\n",
       "      <td>I tried three times original order two replace...</td>\n",
       "      <td>I tried three time original order two replacem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124023</th>\n",
       "      <td>1</td>\n",
       "      <td>This ink actually does run out extremely quick...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>this ink actually does run out extremely quick...</td>\n",
       "      <td>this ink actually does run out extremely quick...</td>\n",
       "      <td>this ink actually does run out extremely quick...</td>\n",
       "      <td>this ink actually does run out extremely quick...</td>\n",
       "      <td>this ink actually does run out extremely quick...</td>\n",
       "      <td>ink actually run extremely quickly judging pri...</td>\n",
       "      <td>ink actually run extremely quickly judging pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747477</th>\n",
       "      <td>1</td>\n",
       "      <td>This SAYS it fits the HL-2230 but it does not!...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>this says it fits the hl-2230 but it does not!...</td>\n",
       "      <td>this says it fits the hl-2230 but it does not!...</td>\n",
       "      <td>this says it fits the hl-2230 but it does not!...</td>\n",
       "      <td>this says it fits the hl      but it does not ...</td>\n",
       "      <td>this says it fits the hl but it does not since...</td>\n",
       "      <td>says fits hl since opened return waste money</td>\n",
       "      <td>say fit hl since opened return waste money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330805</th>\n",
       "      <td>1</td>\n",
       "      <td>The pen is very pretty, it is heavy and has a ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>the pen is very pretty, it is heavy and has a ...</td>\n",
       "      <td>the pen is very pretty, it is heavy and has a ...</td>\n",
       "      <td>the pen is very pretty, it is heavy and has a ...</td>\n",
       "      <td>the pen is very pretty  it is heavy and has a ...</td>\n",
       "      <td>the pen is very pretty it is heavy and has a g...</td>\n",
       "      <td>pen pretty heavy good feel unfortunately write...</td>\n",
       "      <td>pen pretty heavy good feel unfortunately write...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553026</th>\n",
       "      <td>1</td>\n",
       "      <td>I've had the Panasonic KX-TG2000B for years wi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i've had the panasonic kx-tg2000b for years wi...</td>\n",
       "      <td>i've had the panasonic kx-tg2000b for years wi...</td>\n",
       "      <td>I have had the panasonic kx-tg2000b for years ...</td>\n",
       "      <td>I have had the panasonic kx tg    b for years ...</td>\n",
       "      <td>I have had the panasonic kx tg b for years wit...</td>\n",
       "      <td>I panasonic kx tg b years eight handsets house...</td>\n",
       "      <td>I panasonic kx tg b year eight handset house l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760957</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "      <td>good product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933182</th>\n",
       "      <td>5</td>\n",
       "      <td>The batteries that came new with my Uniden pho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the batteries that came new with my uniden pho...</td>\n",
       "      <td>the batteries that came new with my uniden pho...</td>\n",
       "      <td>the batteries that came new with my uniden pho...</td>\n",
       "      <td>the batteries that came new with my uniden pho...</td>\n",
       "      <td>the batteries that came new with my uniden pho...</td>\n",
       "      <td>batteries came new uniden phones lasted two ye...</td>\n",
       "      <td>battery came new uniden phone lasted two year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331851</th>\n",
       "      <td>5</td>\n",
       "      <td>They are a bit big but they write very neatly ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>they are a bit big but they write very neatly ...</td>\n",
       "      <td>they are a bit big but they write very neatly ...</td>\n",
       "      <td>they are a bit big but they write very neatly ...</td>\n",
       "      <td>they are a bit big but they write very neatly ...</td>\n",
       "      <td>they are a bit big but they write very neatly ...</td>\n",
       "      <td>bit big write neatly retracting function works...</td>\n",
       "      <td>bit big write neatly retracting function work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587176</th>\n",
       "      <td>5</td>\n",
       "      <td>This printer replaced my HP LaserJet 2200DN &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this printer replaced my hp laserjet 2200dn &amp; ...</td>\n",
       "      <td>this printer replaced my hp laserjet 2200dn &amp; ...</td>\n",
       "      <td>this printer replaced my hp laserjet 2200dn &amp; ...</td>\n",
       "      <td>this printer replaced my hp laserjet     dn   ...</td>\n",
       "      <td>this printer replaced my hp laserjet dn my off...</td>\n",
       "      <td>printer replaced hp laserjet dn officejet read...</td>\n",
       "      <td>printer replaced hp laserjet dn officejet read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358613</th>\n",
       "      <td>5</td>\n",
       "      <td>The phones are working very well.  Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the phones are working very well.  thank you f...</td>\n",
       "      <td>the phones are working very well.  thank you f...</td>\n",
       "      <td>the phones are working very well. thank you fo...</td>\n",
       "      <td>the phones are working very well  thank you fo...</td>\n",
       "      <td>the phones are working very well thank you for...</td>\n",
       "      <td>phones working well thank prompt delivery real...</td>\n",
       "      <td>phone working well thank prompt delivery reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body  \\\n",
       "2281403            1  I've tried three times (original order) and tw...   \n",
       "1124023            1  This ink actually does run out extremely quick...   \n",
       "1747477            1  This SAYS it fits the HL-2230 but it does not!...   \n",
       "1330805            1  The pen is very pretty, it is heavy and has a ...   \n",
       "2553026            1  I've had the Panasonic KX-TG2000B for years wi...   \n",
       "...              ...                                                ...   \n",
       "760957             5                                       Good Product   \n",
       "933182             5  The batteries that came new with my Uniden pho...   \n",
       "1331851            5  They are a bit big but they write very neatly ...   \n",
       "1587176            5  This printer replaced my HP LaserJet 2200DN & ...   \n",
       "1358613            5  The phones are working very well.  Thank you f...   \n",
       "\n",
       "         negative_review_class  neutral_review_class  positive_review_class  \\\n",
       "2281403                    2.0                   NaN                    NaN   \n",
       "1124023                    2.0                   NaN                    NaN   \n",
       "1747477                    2.0                   NaN                    NaN   \n",
       "1330805                    2.0                   NaN                    NaN   \n",
       "2553026                    2.0                   NaN                    NaN   \n",
       "...                        ...                   ...                    ...   \n",
       "760957                     NaN                   NaN                    1.0   \n",
       "933182                     NaN                   NaN                    1.0   \n",
       "1331851                    NaN                   NaN                    1.0   \n",
       "1587176                    NaN                   NaN                    1.0   \n",
       "1358613                    NaN                   NaN                    1.0   \n",
       "\n",
       "         binary_review_class  ternary_review_class  \\\n",
       "2281403                  2.0                   2.0   \n",
       "1124023                  2.0                   2.0   \n",
       "1747477                  2.0                   2.0   \n",
       "1330805                  2.0                   2.0   \n",
       "2553026                  2.0                   2.0   \n",
       "...                      ...                   ...   \n",
       "760957                   1.0                   1.0   \n",
       "933182                   1.0                   1.0   \n",
       "1331851                  1.0                   1.0   \n",
       "1587176                  1.0                   1.0   \n",
       "1358613                  1.0                   1.0   \n",
       "\n",
       "                                               lower_cased  \\\n",
       "2281403  i've tried three times (original order) and tw...   \n",
       "1124023  this ink actually does run out extremely quick...   \n",
       "1747477  this says it fits the hl-2230 but it does not!...   \n",
       "1330805  the pen is very pretty, it is heavy and has a ...   \n",
       "2553026  i've had the panasonic kx-tg2000b for years wi...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   the batteries that came new with my uniden pho...   \n",
       "1331851  they are a bit big but they write very neatly ...   \n",
       "1587176  this printer replaced my hp laserjet 2200dn & ...   \n",
       "1358613  the phones are working very well.  thank you f...   \n",
       "\n",
       "                                         without_html_urls  \\\n",
       "2281403  i've tried three times (original order) and tw...   \n",
       "1124023  this ink actually does run out extremely quick...   \n",
       "1747477  this says it fits the hl-2230 but it does not!...   \n",
       "1330805  the pen is very pretty, it is heavy and has a ...   \n",
       "2553026  i've had the panasonic kx-tg2000b for years wi...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   the batteries that came new with my uniden pho...   \n",
       "1331851  they are a bit big but they write very neatly ...   \n",
       "1587176  this printer replaced my hp laserjet 2200dn & ...   \n",
       "1358613  the phones are working very well.  thank you f...   \n",
       "\n",
       "                                      without_contractions  \\\n",
       "2281403  I have tried three times (original order) and ...   \n",
       "1124023  this ink actually does run out extremely quick...   \n",
       "1747477  this says it fits the hl-2230 but it does not!...   \n",
       "1330805  the pen is very pretty, it is heavy and has a ...   \n",
       "2553026  I have had the panasonic kx-tg2000b for years ...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   the batteries that came new with my uniden pho...   \n",
       "1331851  they are a bit big but they write very neatly ...   \n",
       "1587176  this printer replaced my hp laserjet 2200dn & ...   \n",
       "1358613  the phones are working very well. thank you fo...   \n",
       "\n",
       "                                     with_alpha_chars_only  \\\n",
       "2281403  I have tried three times  original order  and ...   \n",
       "1124023  this ink actually does run out extremely quick...   \n",
       "1747477  this says it fits the hl      but it does not ...   \n",
       "1330805  the pen is very pretty  it is heavy and has a ...   \n",
       "2553026  I have had the panasonic kx tg    b for years ...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   the batteries that came new with my uniden pho...   \n",
       "1331851  they are a bit big but they write very neatly ...   \n",
       "1587176  this printer replaced my hp laserjet     dn   ...   \n",
       "1358613  the phones are working very well  thank you fo...   \n",
       "\n",
       "                                       without_extra_space  \\\n",
       "2281403  I have tried three times original order and tw...   \n",
       "1124023  this ink actually does run out extremely quick...   \n",
       "1747477  this says it fits the hl but it does not since...   \n",
       "1330805  the pen is very pretty it is heavy and has a g...   \n",
       "2553026  I have had the panasonic kx tg b for years wit...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   the batteries that came new with my uniden pho...   \n",
       "1331851  they are a bit big but they write very neatly ...   \n",
       "1587176  this printer replaced my hp laserjet dn my off...   \n",
       "1358613  the phones are working very well thank you for...   \n",
       "\n",
       "                                        without_stop_words  \\\n",
       "2281403  I tried three times original order two replace...   \n",
       "1124023  ink actually run extremely quickly judging pri...   \n",
       "1747477       says fits hl since opened return waste money   \n",
       "1330805  pen pretty heavy good feel unfortunately write...   \n",
       "2553026  I panasonic kx tg b years eight handsets house...   \n",
       "...                                                    ...   \n",
       "760957                                        good product   \n",
       "933182   batteries came new uniden phones lasted two ye...   \n",
       "1331851  bit big write neatly retracting function works...   \n",
       "1587176  printer replaced hp laserjet dn officejet read...   \n",
       "1358613  phones working well thank prompt delivery real...   \n",
       "\n",
       "                                            lemmed_reviews  \n",
       "2281403  I tried three time original order two replacem...  \n",
       "1124023  ink actually run extremely quickly judging pri...  \n",
       "1747477         say fit hl since opened return waste money  \n",
       "1330805  pen pretty heavy good feel unfortunately write...  \n",
       "2553026  I panasonic kx tg b year eight handset house l...  \n",
       "...                                                    ...  \n",
       "760957                                        good product  \n",
       "933182   battery came new uniden phone lasted two year ...  \n",
       "1331851  bit big write neatly retracting function work ...  \n",
       "1587176  printer replaced hp laserjet dn officejet read...  \n",
       "1358613  phone working well thank prompt delivery reall...  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, review_class):\n",
    "    embeddings_df = df.dropna(subset=[review_class])\n",
    "    # print(len(embeddings_df), embeddings_df['star_rating'].unique())\n",
    "\n",
    "    specific_review_class = embeddings_df[review_class]\n",
    "    # print(specific_review_class.unique())\n",
    "\n",
    "    text = embeddings_df.loc[:, ['lemmed_reviews']]\n",
    "    # print(text)\n",
    "\n",
    "    ### Train test split so I can have the same train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text, specific_review_class, test_size=0.2, random_state=42)\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Binary\\n\")\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = split_data(cleaned_reviews_df, 'binary_review_class')\n",
    "# print(\"\\nTernary\")\n",
    "ternary_X_train, ternary_X_test, ternary_y_train, ternary_y_test = split_data(cleaned_reviews_df, 'ternary_review_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, col_name: str):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: `pd.DataFrame`\n",
    "            The data\n",
    "        \n",
    "        col_name: `str`\n",
    "            Column with reviews\n",
    "\n",
    "        words_in_model: `list`\n",
    "            Words in Word2Vec model\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        text_reviews = self.df[self.col_name].values\n",
    "\n",
    "        for text_reviews_idx in range(len(text_reviews)):\n",
    "            text_review = text_reviews[text_reviews_idx]\n",
    "            # print(text_reviews_idx, \"--\", text_review)\n",
    "\n",
    "            yield utils.simple_preprocess(text_review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 14:26:25,059 : INFO : collecting all words and their counts\n",
      "2024-02-09 14:26:25,060 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 14:26:25,168 : INFO : collected 3784 word types from a corpus of 19530 raw words and 640 sentences\n",
      "2024-02-09 14:26:25,169 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 14:26:25,171 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 430 unique words (11.36% of original 3784, drops 3354)', 'datetime': '2024-02-09T14:26:25.171854', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,172 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 11984 word corpus (61.36% of original 19530, drops 7546)', 'datetime': '2024-02-09T14:26:25.172598', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,175 : INFO : deleting the raw counts dictionary of 3784 items\n",
      "2024-02-09 14:26:25,176 : INFO : sample=0.001 downsamples 107 most-common words\n",
      "2024-02-09 14:26:25,176 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 9379.20318679113 word corpus (78.3%% of prior 11984)', 'datetime': '2024-02-09T14:26:25.176635', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,180 : INFO : estimated required memory for 430 words and 300 dimensions: 1247000 bytes\n",
      "2024-02-09 14:26:25,181 : INFO : resetting layer weights\n",
      "2024-02-09 14:26:25,183 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T14:26:25.183092', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 14:26:25,183 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 430 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T14:26:25.183712', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 14:26:25,230 : INFO : EPOCH 0: training on 19530 raw words (9290 effective words) took 0.0s, 232454 effective words/s\n",
      "2024-02-09 14:26:25,271 : INFO : EPOCH 1: training on 19530 raw words (9408 effective words) took 0.0s, 338773 effective words/s\n",
      "2024-02-09 14:26:25,307 : INFO : EPOCH 2: training on 19530 raw words (9345 effective words) took 0.0s, 319005 effective words/s\n",
      "2024-02-09 14:26:25,343 : INFO : EPOCH 3: training on 19530 raw words (9345 effective words) took 0.0s, 323629 effective words/s\n",
      "2024-02-09 14:26:25,379 : INFO : EPOCH 4: training on 19530 raw words (9382 effective words) took 0.0s, 553977 effective words/s\n",
      "2024-02-09 14:26:25,380 : INFO : Word2Vec lifecycle event {'msg': 'training on 97650 raw words (46770 effective words) took 0.2s, 238334 effective words/s', 'datetime': '2024-02-09T14:26:25.380455', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 14:26:25,380 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=430, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T14:26:25.380833', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-09 14:26:25,381 : INFO : collecting all words and their counts\n",
      "2024-02-09 14:26:25,382 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-09 14:26:25,416 : INFO : collected 4245 word types from a corpus of 24553 raw words and 800 sentences\n",
      "2024-02-09 14:26:25,416 : INFO : Creating a fresh vocabulary\n",
      "2024-02-09 14:26:25,418 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 526 unique words (12.39% of original 4245, drops 3719)', 'datetime': '2024-02-09T14:26:25.418786', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,419 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 16012 word corpus (65.21% of original 24553, drops 8541)', 'datetime': '2024-02-09T14:26:25.419382', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,421 : INFO : deleting the raw counts dictionary of 4245 items\n",
      "2024-02-09 14:26:25,422 : INFO : sample=0.001 downsamples 100 most-common words\n",
      "2024-02-09 14:26:25,422 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 13034.457627221018 word corpus (81.4%% of prior 16012)', 'datetime': '2024-02-09T14:26:25.422728', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-09 14:26:25,426 : INFO : estimated required memory for 526 words and 300 dimensions: 1525400 bytes\n",
      "2024-02-09 14:26:25,427 : INFO : resetting layer weights\n",
      "2024-02-09 14:26:25,428 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-09T14:26:25.428364', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-09 14:26:25,428 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 526 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=11 shrink_windows=True', 'datetime': '2024-02-09T14:26:25.428927', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 14:26:25,478 : INFO : EPOCH 0: training on 24553 raw words (13005 effective words) took 0.0s, 351782 effective words/s\n",
      "2024-02-09 14:26:25,523 : INFO : EPOCH 1: training on 24553 raw words (12965 effective words) took 0.0s, 351904 effective words/s\n",
      "2024-02-09 14:26:25,566 : INFO : EPOCH 2: training on 24553 raw words (12968 effective words) took 0.0s, 363917 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ternary Case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 14:26:25,609 : INFO : EPOCH 3: training on 24553 raw words (13070 effective words) took 0.0s, 358141 effective words/s\n",
      "2024-02-09 14:26:25,654 : INFO : EPOCH 4: training on 24553 raw words (12992 effective words) took 0.0s, 413295 effective words/s\n",
      "2024-02-09 14:26:25,654 : INFO : Word2Vec lifecycle event {'msg': 'training on 122765 raw words (65000 effective words) took 0.2s, 288568 effective words/s', 'datetime': '2024-02-09T14:26:25.654659', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-09 14:26:25,655 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=526, vector_size=300, alpha=0.025>', 'datetime': '2024-02-09T14:26:25.655056', 'gensim': '4.3.2', 'python': '3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]', 'platform': 'macOS-13.6.3-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary Case\")\n",
    "binary_X_train_sentences = MyCorpus(binary_X_train, 'lemmed_reviews')\n",
    "my_binary_X_train_model = gensim.models.Word2Vec(sentences=binary_X_train_sentences, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "# X test - get embeddings from my_binary_X_train_model -- vec_king = my_binary_X_train_model.wv['king']\n",
    "# binary_X_test_sentences = MyCorpus(binary_X_test, 'lemmed_reviews')\n",
    "# sentences = MyCorpus(sampled_reviews_ratings_df, 'review_body')\n",
    "# print(\"\\nSentences\", binary_X_test_sentences)\n",
    "\n",
    "print(\"\\nTernary Case\")\n",
    "ternary_X_train_sentences = MyCorpus(ternary_X_train, 'lemmed_reviews')\n",
    "my_ternary_X_train_model = gensim.models.Word2Vec(sentences=ternary_X_train_sentences, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar scores\n",
    "\n",
    "- [x] Write summary of differences between their model and my model\n",
    "    - My model doesn't perform as well as the pretrained because the pretrained has trained on more data compared to my model. Thus, my model don't have sufficient training compared to the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result = pretrained_word_two_vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1aefdd810>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_X_train_model = my_binary_X_train_model.wv\n",
    "my_trained_binary_X_train_model\n",
    "\n",
    "my_trained_ternary_X_train_model = my_ternary_X_train_model.wv\n",
    "my_trained_ternary_X_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix with proper exs\n",
    "# my_result = my_trained_binary_X_train_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "# print(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings\n",
    "\n",
    "- Word embeddings [vector representation of each word]\n",
    "- TUTORIAL: [Word2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    "    - Follow for the purpose of using the Gensimm library\n",
    "---\n",
    "\n",
    "## (a) Pretrained Word2Vec\n",
    "\n",
    "- [x] Load the pretrained “word2vec-google-news-300” Word2Vec model\n",
    "- [x] Extract word embeddings (per word)\n",
    "- [ ] Check semantic similarities (ie: (1) King − Man + Woman = Queen, (2) excellent ∼ outstanding) of my own\n",
    "\n",
    "## (b) My trained Word2Vec\n",
    "- [x] Train a Word2Vec model using my own dataset\n",
    "    - [ ] Set the embedding size to be 300\n",
    "    - [ ] Set the window size to be 11\n",
    "    - [ ] Consider a minimum word count of 10\n",
    "- [ ] Check semantic similarities\n",
    "    - [ ] What do you conclude from comparing vectors generated by yourself and the pretrained model? (see answer below)\n",
    "    - [ ] Which of the Word2Vec models seems to encode semantic similarities between words better? (see answer below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(df: pd.DataFrame, col_name: str, model_to_use):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    mean_sentences_vectorized\n",
    "    \"\"\"\n",
    "\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx)\n",
    "        # print(\"Sentence\", sentences_idx, \"Pre-vectorized -- \", sentence)\n",
    "        for word_idx, word in enumerate(sentence.split(\" \")):\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "        # print(\"Sentence\", sentences_idx, \"Post-vectorized \\n\")\n",
    "        mean_of_sentence = np.mean(sentence_vectorized[sentences_idx], axis=0)\n",
    "        mean_sentences_vectorized.append(mean_of_sentence)\n",
    "    print(len(mean_sentences_vectorized))\n",
    "    return mean_sentences_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Pretrained Train\n",
      "640\n",
      "Binary Pretrained Test\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Binary Pretrained Train\")\n",
    "pretrained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_train_embeddings = np.array(pretrained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary Pretrained Test\")\n",
    "pretrained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_binary_test_embeddings = np.array(pretrained_binary_test_embeddings)\n",
    "\n",
    "pretrained_binary_train_embeddings.shape, pretrained_binary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Pretrained Train\n",
      "800\n",
      "Ternary Pretrained Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary Pretrained Train\")\n",
    "pretrained_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_train_embeddings = np.array(pretrained_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary Pretrained Test\")\n",
    "pretrained_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model)\n",
    "pretrained_ternary_test_embeddings = np.array(pretrained_ternary_test_embeddings)\n",
    "\n",
    "pretrained_ternary_train_embeddings.shape, pretrained_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary My Model Train\n",
      "640\n",
      "Binary My Model Test\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary My Model Train\")\n",
    "my_trained_binary_train_embeddings = word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_train_embeddings = np.array(my_trained_binary_train_embeddings)\n",
    "\n",
    "print(\"Binary My Model Test\")\n",
    "my_trained_binary_test_embeddings = word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_binary_X_train_model)\n",
    "my_trained_binary_test_embeddings = np.array(my_trained_binary_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary My Model Train\n",
      "800\n",
      "Ternary My Model Test\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 300), (200, 300))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ternary My Model Train\")\n",
    "my_ternary_train_embeddings = word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_train_embeddings = np.array(my_ternary_train_embeddings)\n",
    "\n",
    "print(\"Ternary My Model Test\")\n",
    "my_ternary_test_embeddings = word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model)\n",
    "my_ternary_test_embeddings = np.array(my_ternary_test_embeddings)\n",
    "\n",
    "my_ternary_train_embeddings.shape, my_ternary_test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_word_embeddings(df: pd.DataFrame, col_name: str, model_to_use, about: str):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    aboout: `str`\n",
    "        Specifics of model, classification, and train/test\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    concatenated_vectors: list\n",
    "        List of concatenated vectors for each review (first 10 Word2Vec vectors)\n",
    "    \"\"\"\n",
    "    print(\"About:\", about)\n",
    "    mean_concatenated_vectors = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vectorized_words = []\n",
    "        words = sentence.split(\" \")[:10]  # Select the first 10 words\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "            vectorized_words.append(vector_of_word)\n",
    "        \n",
    "        concatenated_features = np.concatenate(vectorized_words, axis=0)\n",
    "        # Ensure dimensionality of 300 for each sentence\n",
    "        if concatenated_features.shape[0] < 300:\n",
    "            # If concatenated_features has less than 300 dimensions, pad it with zeros\n",
    "            concatenated_features = np.pad(concatenated_features, ((0, 300 - concatenated_features.shape[0]), (0, 0)), mode='constant')\n",
    "        elif concatenated_features.shape[0] > 300:\n",
    "            # If concatenated_features has more than 300 dimensions, truncate it\n",
    "            concatenated_features = concatenated_features[:300 :]\n",
    "            \n",
    "        mean_concatenated_vectors.append(concatenated_features)\n",
    "\n",
    "    mean_concatenated_vectors = np.array(mean_concatenated_vectors)\n",
    "    print(\"   Concat embeddings shape --- \", mean_concatenated_vectors.shape)\n",
    "    print()\n",
    "    return mean_concatenated_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary\n",
      "About: Pretrained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (640, 300)\n",
      "\n",
      "About: Pretrained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (160, 300)\n",
      "\n",
      "About: My trained --- Binary --- Train\n",
      "   Concat embeddings shape ---  (640, 300)\n",
      "\n",
      "About: My trained --- Binary --- Test\n",
      "   Concat embeddings shape ---  (160, 300)\n",
      "\n",
      "--- Ternary ---\n",
      "About: Pretrained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (800, 300)\n",
      "\n",
      "About: Pretrained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (200, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Train\n",
      "   Concat embeddings shape ---  (800, 300)\n",
      "\n",
      "About: My trained --- Ternary --- Test\n",
      "   Concat embeddings shape ---  (200, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary\")\n",
    "pretrained_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")\n",
    "\n",
    "my_binary_train_concat_embeddings = concat_word_embeddings(binary_X_train, 'lemmed_reviews', my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_binary_test_concat_embeddings = concat_word_embeddings(binary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Binary --- Test\")\n",
    "\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")\n",
    "\n",
    "my_ternary_train_concat_embeddings = concat_word_embeddings(ternary_X_train, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_ternary_test_concat_embeddings = concat_word_embeddings(ternary_X_test, 'lemmed_reviews', my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_feature_extraction(df: pd.DataFrame, col_name: str):\n",
    "    \"\"\"Extract the TF-IDF features from the reviews.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    tf_idf_features:\n",
    "        A matrix containing the TF-IDF features extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_features = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    return tf_idf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_df = cleaned_reviews_df.dropna(subset=['binary_review_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features = tf_idf_feature_extraction(cleaned_reviews_df, 'lemmed_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4204 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binnary_reviews = cleaned_reviews_df['binary_review_class']\n",
    "binnary_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(tf_idf_features, binnary_reviews, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models\n",
    "\n",
    "**GOAL:** Train simple models below, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "\n",
    "- [ ] Train a perceptron and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "    - [ ] TF-IDF embeddings\n",
    "- [ ] Train a support vector machine (SVM) and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "    - [ ] TF-IDF embeddings\n",
    "- [ ] Compare\n",
    "    - [ ] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results_dict, w2v_type, method, classification, accuracy):\n",
    "    \"\"\"\n",
    "    Store results in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    results_dict (dict): Dictionary to store the results.\n",
    "    w2v_type (str): Type of word2vec.\n",
    "    method (str): Method used.\n",
    "    classification (str): Type of classification.\n",
    "    accuracy (float): Accuracy of the classification.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the stored results.\n",
    "    \"\"\"\n",
    "    results_dict['W2V Type'].append(w2v_type)\n",
    "    results_dict['Method'].append(method)\n",
    "    results_dict['Classification'].append(classification)\n",
    "    results_dict['Accuracy'].append(accuracy)\n",
    "\n",
    "    return pd.DataFrame(results_dict)  # Return DataFrame with a single row\n",
    "\n",
    "results_dict = {'W2V Type': [], 'Method': [], 'Classification': [], 'Accuracy': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_prediction):\n",
    "    return sklearn.metrics.accuracy_score(y_true, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_metric(y_train_true, y_train_predictions):\n",
    "    accuracy = eval_accuracy(y_train_true, y_train_predictions)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def test_eval_metric(y_test_true, y_test_predictions):\n",
    "    accuracy = eval_accuracy(y_test_true, y_test_predictions)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = Perceptron(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics = perceptron_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 1.0}, {'Accuracy': 0.75})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_perceptron_train_metrics, tfidf_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf_perceptron_test_metrics.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       W2V Type          Method Classification  Accuracy\n",
      "0  TF-IDF-train  Perceptron-avg         Binary       1.0\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "\n",
    "results_df = store_results(results_dict, 'TF-IDF-train', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'Perceptron-avg', 'Binary', list(tfidf_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "perceptron_train_metrics, perceptron_test_metrics = perceptron_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.8546875}, {'Accuracy': 0.73125})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_train_metrics, perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n"
     ]
    }
   ],
   "source": [
    "# Update the dictionary with new results\n",
    "results_df = store_results(results_dict, 'Pretrained-train', 'Perceptron-avg', 'Binary', list(perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'Perceptron-avg', 'Binary', list(perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for perceptron on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((640, 300), (160, 300), (640,), (160,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_binary_train_embeddings.shape, my_trained_binary_test_embeddings.shape, binary_y_train.shape, binary_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "my_perceptron_train_metrics, my_perceptron_test_metrics = perceptron_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.765625}, {'Accuracy': 0.54375})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_perceptron_train_metrics, my_perceptron_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'Perceptron-avg', 'Binary', list(my_perceptron_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'Perceptron-avg', 'Binary', list(my_perceptron_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(X_train, X_test, y_train, y_test): \n",
    "\n",
    "    technique = LinearSVC(tol=1e-3, random_state=0)\n",
    "    technique.fit(X_train, y_train)\n",
    "    y_train_predictions = technique.predict(X_train)\n",
    "    y_test_predictions = technique.predict(X_test)\n",
    "\n",
    "\n",
    "    train_metrics = train_eval_metric(y_train, y_train_predictions)\n",
    "    test_metrics = test_eval_metric(y_test, y_test_predictions)\n",
    "\n",
    "    return train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics = svm_model(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 1.0}, {'Accuracy': 0.75})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_svm_train_metrics, tfidf_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'TF-IDF-train', 'SVM-avg', 'Binary', list(tfidf_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'TF-IDF-test', 'SVM-avg', 'Binary', list(tfidf_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics = svm_model(pretrained_binary_train_embeddings, pretrained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.9109375}, {'Accuracy': 0.79375})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train_metrics, svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained-train', 'SVM-avg', 'Binary', list(svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'Pretrained-test', 'SVM-avg', 'Binary', list(svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for svm on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics = svm_model(my_trained_binary_train_embeddings, my_trained_binary_test_embeddings, binary_y_train, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Accuracy': 0.834375}, {'Accuracy': 0.5625})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_train_metrics, my_svm_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5    My model-train         SVM-avg         Binary  0.834375\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model-train', 'SVM-avg', 'Binary', list(my_svm_train_metrics.values())[0])\n",
    "# results_df = store_results(results_dict, 'My model-test', 'SVM-avg', 'Binary', list(my_svm_test_metrics.values())[0])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks (FFNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification, report the accuracy metric, and understand performances\n",
    "\n",
    "---\n",
    "- [ ] Train a feedforward multilayer perceptron (MLP) network\n",
    "    - [ ] 2 hidden layers each with 50 and 10 nodes, respectively\n",
    "    - [ ] Cross entropy loss\n",
    "    - [ ] Decide other hyperparameters (ie: nonlinearity, #epochs, etc)\n",
    "- [ ] TUTORIAL: [Pytorch Multi-Layer Perceptron, MNIST](https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook) for image data\n",
    "\n",
    "---\n",
    "\n",
    "## (a) Average Embeddings\n",
    " \n",
    "- [ ] Train a FFNN and report accuracy on the testing split for on average embeddings for\n",
    "    - [ ] Pretrained \n",
    "    - [ ] My trained average embeddings\n",
    "- [ ] Train a FFNN and report accuracy on the testing split for\n",
    "    - [ ] Pretrained average embeddings\n",
    "    - [ ] My trained average embeddings\n",
    "\n",
    "## (b) Concatenate Embeddings\n",
    "\n",
    "- [ ] Concatenate the first 10 Word2Vec vectors for each review\n",
    "\n",
    "- [ ] Compare\n",
    "    - [ ] What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)? (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_review_class = binary_embeddings_df['binary_review_class']\n",
    "# binary_review_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Define the NN architecture\"\"\"\n",
    "\n",
    "    def __init__(self, num_h1_nodes: int, num_h2_nodes: int, d: int, num_output_classes: int, dropout_rate: float):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # self.height = heightght = height\n",
    "        # self.width = width\n",
    "        # linear layer (1200 x 300 dot 300 x 50 -> 1200 x 50)\n",
    "        self.fc1 = nn.Linear(d, num_h1_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc2 = nn.Linear(num_h1_nodes, num_h2_nodes)\n",
    "        # linear layer (1200 x 50 dot 50 x 10 -> 1200 x 10)\n",
    "        self.fc3 = nn.Linear(num_h2_nodes, num_output_classes) # change to 3 for ternery\n",
    "        # dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add hidden layer, with relu activation function, dropout, relu, dropout, output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader):\n",
    "        # set initial \"min\" to infinity\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass to compute predictions, loss, backward pass to compute gradient wrt model params\n",
    "                output = self(data)\n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update running training loss\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # print training statistics \n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            \n",
    "            # print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            #     epoch+1, \n",
    "            #     train_loss,\n",
    "            #     ))\n",
    "            \n",
    "            # # save model if validation loss has decreased\n",
    "            # if train_loss <= valid_loss_min:\n",
    "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            #     valid_loss_min,\n",
    "            #     train_loss))\n",
    "            #     torch.save(self.state_dict(), 'nn_model.pt')\n",
    "            #     valid_loss_min = train_loss\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            outputs = self(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Convert predictions and ground truth lists to numpy arrays\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = (predictions == ground_truth).mean()\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 300\n"
     ]
    }
   ],
   "source": [
    "N_binary_embeddings, d_binary_embeddings = my_trained_binary_train_embeddings.shape\n",
    "print(N_binary_embeddings, d_binary_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(binary_train_embeddings, binary_train_y):\n",
    "    binary_train_y = binary_train_y.replace(2, 0)\n",
    "    data_tensor = torch.tensor(binary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(binary_train_y.values, dtype=torch.long)\n",
    "    print(data_tensor.size(), target_tensor.size())\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "    \n",
    "    print(len(dataset))\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 25\n",
    "    # number_of_epochs = 50\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binary_model(binary_test_embeddings, binary_test_y):\n",
    "    binary_test_y = binary_test_y.replace(2, 0)\n",
    "    target_array = binary_test_y.values\n",
    "    data_tensor = torch.tensor(binary_test_embeddings, dtype=torch.float32)\n",
    "    # print(len(data_tensor))\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_pretrained_binary = predict_binary_model(pretrained_binary_test_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5    My model-train         SVM-avg         Binary  0.834375\n",
      "6        Pretrained         FFN-avg         Binary  0.500000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Binary', acc_avg_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.5\n",
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5    My model-train         SVM-avg         Binary  0.834375\n",
      "6        Pretrained         FFN-avg         Binary  0.500000\n",
      "7          My model         FFN-avg         Binary  0.500000\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_trained_binary_train_embeddings, binary_y_train)\n",
    "acc_avg_my_binary = predict_binary_model(my_trained_binary_test_embeddings, binary_y_test)\n",
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Binary', acc_avg_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.475\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(pretrained_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_pretrained_binary = predict_binary_model(pretrained_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5    My model-train         SVM-avg         Binary  0.834375\n",
      "6        Pretrained         FFN-avg         Binary  0.500000\n",
      "7          My model         FFN-avg         Binary  0.500000\n",
      "8        Pretrained      FFN-concat         Binary  0.475000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Binary', acc_concat_pretrained_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 300]) torch.Size([640])\n",
      "640\n",
      "Accuracy: 0.55625\n"
     ]
    }
   ],
   "source": [
    "train_binary_model(my_binary_train_concat_embeddings, binary_y_train)\n",
    "acc_concat_my_binary = predict_binary_model(my_binary_test_concat_embeddings, binary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W2V Type          Method Classification  Accuracy\n",
      "0      TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1  Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2    My model-train  Perceptron-avg         Binary  0.765625\n",
      "3      TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4  Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5    My model-train         SVM-avg         Binary  0.834375\n",
      "6        Pretrained         FFN-avg         Binary  0.500000\n",
      "7          My model         FFN-avg         Binary  0.500000\n",
      "8        Pretrained      FFN-concat         Binary  0.475000\n",
      "9          My model      FFN-concat         Binary  0.556250\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Binary', acc_concat_my_binary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = net_model.predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.395\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2     My model-train  Perceptron-avg         Binary  0.765625\n",
      "3       TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4   Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5     My model-train         SVM-avg         Binary  0.834375\n",
      "6         Pretrained         FFN-avg         Binary  0.500000\n",
      "7           My model         FFN-avg         Binary  0.500000\n",
      "8         Pretrained      FFN-concat         Binary  0.475000\n",
      "9           My model      FFN-concat         Binary  0.556250\n",
      "10        Pretrained         FFN-avg        Ternary  0.395000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-avg', 'Ternary', acc_avg_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_avg_my_ternary = predict_ternary_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2     My model-train  Perceptron-avg         Binary  0.765625\n",
      "3       TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4   Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5     My model-train         SVM-avg         Binary  0.834375\n",
      "6         Pretrained         FFN-avg         Binary  0.500000\n",
      "7           My model         FFN-avg         Binary  0.500000\n",
      "8         Pretrained      FFN-concat         Binary  0.475000\n",
      "9           My model      FFN-concat         Binary  0.556250\n",
      "10        Pretrained         FFN-avg        Ternary  0.395000\n",
      "11          My model         FFN-avg        Ternary  0.400000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-avg', 'Ternary', acc_avg_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(pretrained_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_pretrained_ternary = predict_ternary_model(pretrained_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2     My model-train  Perceptron-avg         Binary  0.765625\n",
      "3       TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4   Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5     My model-train         SVM-avg         Binary  0.834375\n",
      "6         Pretrained         FFN-avg         Binary  0.500000\n",
      "7           My model         FFN-avg         Binary  0.500000\n",
      "8         Pretrained      FFN-concat         Binary  0.475000\n",
      "9           My model      FFN-concat         Binary  0.556250\n",
      "10        Pretrained         FFN-avg        Ternary  0.395000\n",
      "11          My model         FFN-avg        Ternary  0.400000\n",
      "12        Pretrained      FFN-concat        Ternary  0.410000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'FFN-concat', 'Ternary', acc_concat_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.465\n"
     ]
    }
   ],
   "source": [
    "train_ternary_model(my_ternary_train_concat_embeddings, ternary_y_train)\n",
    "acc_concat_my_ternary = predict_ternary_model(my_ternary_test_concat_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            W2V Type          Method Classification  Accuracy\n",
      "0       TF-IDF-train  Perceptron-avg         Binary  1.000000\n",
      "1   Pretrained-train  Perceptron-avg         Binary  0.854688\n",
      "2     My model-train  Perceptron-avg         Binary  0.765625\n",
      "3       TF-IDF-train         SVM-avg         Binary  1.000000\n",
      "4   Pretrained-train         SVM-avg         Binary  0.910937\n",
      "5     My model-train         SVM-avg         Binary  0.834375\n",
      "6         Pretrained         FFN-avg         Binary  0.500000\n",
      "7           My model         FFN-avg         Binary  0.500000\n",
      "8         Pretrained      FFN-concat         Binary  0.475000\n",
      "9           My model      FFN-concat         Binary  0.556250\n",
      "10        Pretrained         FFN-avg        Ternary  0.395000\n",
      "11          My model         FFN-avg        Ternary  0.400000\n",
      "12        Pretrained      FFN-concat        Ternary  0.410000\n",
      "13          My model      FFN-concat        Ternary  0.465000\n"
     ]
    }
   ],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'FFN-concat', 'Ternary', acc_concat_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks (CNN)\n",
    "\n",
    "**GOAL:** Train a CNN for sentiment analysis classification\n",
    "\n",
    "---\n",
    "\n",
    "- [ ] 2 layer CNN with output channel sizes of 50 and 10, respectively\n",
    "- [ ] Limit each review to 50 words\n",
    "    - [ ] If more, truncate to 50\n",
    "    - [ ] If less pad with 0s to make 50\n",
    "- [ ] Use cross entropy\n",
    "- [ ] Select hyperparameters (ie: nonlinearity, #epochs, etc.) of my choosing\n",
    "- [ ] Train for binary classification\n",
    "- [ ] Train for ternary classification\n",
    "- [ ] Report for testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[# of reviews, 300 embedding] (above code) (works)\n",
    "\n",
    "[# of reviews, 50 words for each review, 300] (below code) (doesnt work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_word_embeddings(df: pd.DataFrame, col_name: str, max_sentence_length: int, model_to_use, about):\n",
    "    \"\"\"Extract word embeddings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        The data\n",
    "    \n",
    "    col_name: `str`\n",
    "        Column with reviews\n",
    "\n",
    "    model_to_use:\n",
    "        Either the pretrained model or my pretrained model\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    conditioned_sequences\n",
    "    \"\"\"\n",
    "    print(\"About: \", about)\n",
    "    sentence_vectorized = []\n",
    "    mean_sentences_vectorized = []\n",
    "    concatenated_features = []\n",
    "    sentences = df[col_name].values\n",
    "\n",
    "    for sentences_idx in range(len(sentences)):\n",
    "        vectorized_words = []\n",
    "        sentence = sentences[sentences_idx]\n",
    "        # print(\"Sentence\", sentences_idx, sentence)\n",
    "        words = sentence.split(\" \")\n",
    "        # print(\"Before -- \", len(words))\n",
    "        if len(words) > max_sentence_length:\n",
    "            words = words[:max_sentence_length]\n",
    "        elif len(words) < max_sentence_length:\n",
    "            words += [''] * (max_sentence_length - len(words))\n",
    "\n",
    "        # print(\"After -- \", len(words), words)\n",
    "\n",
    "        for word in words:\n",
    "            if word in model_to_use.key_to_index:\n",
    "                vector_of_word = model_to_use[word]\n",
    "                vectorized_words.append(vector_of_word)\n",
    "            else:\n",
    "                vector_of_word = np.random.rand(model_to_use.vector_size)\n",
    "                vectorized_words.append(vector_of_word)\n",
    "\n",
    "        sentence_vectorized.append(vectorized_words)\n",
    "    # print(sentence_vectorized)\n",
    "        # print()\n",
    "    \n",
    "        \n",
    "    tensors = []\n",
    "    for words in sentence_vectorized:\n",
    "        tensors.append(torch.tensor(words))\n",
    "    padded_sequences = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
    "    print(\"Embeddings shape: \", padded_sequences.shape, type(padded_sequences.shape))\n",
    "    print()\n",
    "    return padded_sequences\n",
    "    # return sentence_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary ---\n",
      "About:  Pretrained --- Binary --- Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_8552/512139095.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  tensors.append(torch.tensor(words))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape:  torch.Size([640, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  Pretrained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([160, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  My trained --- Binary --- Train\n",
      "Embeddings shape:  torch.Size([640, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  My trained --- Binary --- Test\n",
      "Embeddings shape:  torch.Size([160, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "--- Ternary ---\n",
      "About:  Pretrained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([800, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  Pretrained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([200, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  My trained --- Ternary --- Train\n",
      "Embeddings shape:  torch.Size([800, 50, 300]) <class 'torch.Size'>\n",
      "\n",
      "About:  My trained --- Ternary --- Test\n",
      "Embeddings shape:  torch.Size([200, 50, 300]) <class 'torch.Size'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Binary ---\")\n",
    "pretrained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Train\")\n",
    "pretrained_binary_train_50_embeddings = np.array(pretrained_binary_train_50_embeddings)\n",
    "\n",
    "pretrained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Binary --- Test\")\n",
    "pretrained_binary_test_50_embeddings = np.array(pretrained_binary_test_50_embeddings)\n",
    "\n",
    "my_trained_binary_train_50_embeddings = condition_word_embeddings(binary_X_train, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Train\")\n",
    "my_trained_binary_train_50_embeddings = np.array(my_trained_binary_train_50_embeddings)\n",
    "\n",
    "my_trained_binary_test_50_embeddings = condition_word_embeddings(binary_X_test, 'lemmed_reviews', 50, my_trained_binary_X_train_model, \"My trained --- Binary --- Test\")\n",
    "my_trained_binary_test_50_embeddings = np.array(my_trained_binary_test_50_embeddings)\n",
    "\n",
    "\n",
    "print(\"--- Ternary ---\")\n",
    "pretrained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Train\")\n",
    "pretrained_ternary_train_50_embeddings = np.array(pretrained_binary_train_50_embeddings)\n",
    "\n",
    "pretrained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, pretrained_word_two_vec_model, \"Pretrained --- Ternary --- Test\")\n",
    "pretrained_ternary_test_50_embeddings = np.array(pretrained_binary_test_50_embeddings)\n",
    "\n",
    "my_trained_ternary_train_50_embeddings = condition_word_embeddings(ternary_X_train, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Train\")\n",
    "my_trained_ternary_train_50_embeddings = np.array(my_trained_binary_train_50_embeddings)\n",
    "\n",
    "my_trained_ternary_test_50_embeddings = condition_word_embeddings(ternary_X_test, 'lemmed_reviews', 50, my_trained_ternary_X_train_model, \"My trained --- Ternary --- Test\")\n",
    "my_trained_ternary_test_50_embeddings = np.array(my_trained_ternary_test_50_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_load_data(embeddings: list, y_true: list, classification_type: str, about: str):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_size = 64\n",
    "    \n",
    "    \"\"\"Mapping\n",
    "    \n",
    "    1 --- (positive) --- 0 \n",
    "    2 --- (negative) --- 1 \n",
    "    3 --- (neutral)  --- 2 \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\nAbout: \", about, \"\\n\")\n",
    "    print(\"Original mappings --- \", y_true.unique())\n",
    "    if classification_type == \"binary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "\n",
    "    elif classification_type == \"ternary\":\n",
    "        map_y_true_values = y_true.replace(1, 0)\n",
    "        map_y_true_values = map_y_true_values.replace(2, 1)\n",
    "        map_y_true_values = map_y_true_values.replace(3, 2)\n",
    "        # print(\"Remappings for \", classification_type, \"---\", map_y_true_values.unique())\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid classification type\")\n",
    "\n",
    "    print(\"Remappings --- \", map_y_true_values.unique())\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    y_true_tensor = torch.tensor(map_y_true_values.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(embeddings_tensor, y_true_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)    \n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "About:  Pretrained x Binary x Train \n",
      "\n",
      "Original mappings ---  [2. 1.]\n",
      "Remappings ---  [1. 0.]\n",
      "\n",
      "About:  Pretrained x Binary x Test \n",
      "\n",
      "Original mappings ---  [1. 2.]\n",
      "Remappings ---  [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# print(\"--- Binary ---\")\n",
    "pretrained_binary_train_loader = format_and_load_data(pretrained_binary_train_50_embeddings, binary_y_train, \"binary\", \"Pretrained x Binary x Train\")\n",
    "pretrained_binary_test_loader = format_and_load_data(pretrained_binary_test_50_embeddings, binary_y_test, \"binary\", \"Pretrained x Binary x Test\")\n",
    "\n",
    "# my_trained_binary_train_loader = format_and_load_data(my_trained_binary_train_50_embeddings, binary_y_train, \"binary\", \"My trained x Binary x Train\")\n",
    "# my_trained_binary_test_loader = format_and_load_data(my_trained_binary_test_50_embeddings, binary_y_test, \"binary\", \"My trained x Binary x Test\")\n",
    "\n",
    "# print()\n",
    "# print(\"--- Ternary ---\")\n",
    "# pretrained_ternary_train_loader = format_and_load_data(pretrained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"Pretrained x Ternary x Train\")\n",
    "# pretrained_ternary_test_loader = format_and_load_data(pretrained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"Pretrained x Ternary x Test\")\n",
    "\n",
    "# my_trained_ternary_train_loader = format_and_load_data(my_trained_ternary_train_50_embeddings, ternary_y_train, \"ternary\", \"My trained x Ternary x Test\")\n",
    "# my_trained_ternary_test_loader = format_and_load_data(my_trained_ternary_test_50_embeddings, ternary_y_test, \"ternary\", \"My trained x Ternary x Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_channels = 300\n",
    "# embedding_size = 300\n",
    "# output_channels1 = 50\n",
    "# output_channels2 = 10\n",
    "# kernel_size1 = 3\n",
    "# kernel_size2 = 3 \n",
    "# num_classes = 2\n",
    "\n",
    "# nn.Conv1d(16, 33, 3, stride=2)\n",
    "# nn.Conv1d(640, 50, 300) --> (num_channels=300, output_channels1=50, kernel_size1=3)\n",
    "# (num_channels=300, output_channels1=50, kernel_size1=3) --> (num_channels=50, output_channels1=10, kernel_size2=3)\n",
    "\n",
    "# nn.Linear(2960, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=output_channels1, kernel_size=kernel_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=kernel_size2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # to calc 460, refer to docs and write my own utils func\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(460, num_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Before permute ---\", x.shape)\n",
    "        x = x.permute(0, 2, 1)  # Reshape for Conv1d: [batch_size, num_channels, embedding_size, sequence_length]\n",
    "        # x = x.reshape(x.shape[0], 1 , x.shape[1]) #### TODO: \n",
    "        # print(\"After permute ---\", x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"After Conv layers ---\", x.shape)\n",
    "        # print(\"1st\", x.shape)\n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"After Flatten ---\", x.shape)\n",
    "        x = self.fc_layers(x)\n",
    "        # print(\"After Fully connected layers ---\", x.shape)\n",
    "        # print()\n",
    "        return x\n",
    "\n",
    "    def train_network(self, number_of_epochs: int, optimizer, criterion_function, train_loader, test_loader):\n",
    "        for epoch in range(number_of_epochs):\n",
    "            print(\"Epoch --- \", epoch)\n",
    "            correct_train = 0.0\n",
    "            total_train_loss = 0.0\n",
    "            \n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "            self.train() # prep model for training\n",
    "    \n",
    "            for data, target in train_loader:\n",
    "                # clear the gradients of all optimized variables\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # predictions\n",
    "                output = self(data)\n",
    "                # print(\"Output\", output.shape)\n",
    "                \n",
    "                # target = target.squeeze()\n",
    "                \n",
    "                loss = criterion_function(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                correct_train += (predicted == target).sum().item()\n",
    "               \n",
    "            avg_total_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "            print(\"Train avg total loss --- \", avg_total_train_loss)\n",
    "        \n",
    "            total_evaluation_loss = 0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    output = self(data)\n",
    "                    # target = target.squeeze()\n",
    "                    loss = criterion_function(output, target)\n",
    "                    total_evaluation_loss += loss.item() * data.size(0)\n",
    "                    \n",
    "            avg_total_evaluation_loss = total_evaluation_loss / len(train_loader.dataset)\n",
    "            print(\"Evaluation avg total loss --- \", avg_total_evaluation_loss)\n",
    "            \n",
    "            train_accuracy = correct_train / len(train_loader.dataset)\n",
    "            print(f'Epoch: {epoch+1} \\tTraining Loss: {loss:.6f} \\tTraining Accuracy: {train_accuracy * 100:.2f}%')\n",
    "            print()\n",
    "        print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 50, 300)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, N_words, embedding_size = pretrained_binary_train_50_embeddings.shape\n",
    "N, N_words, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=460, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# channels: for each word has 300 dims, so 300 features\n",
    "num_channels = 300\n",
    "\n",
    "output_channels1 = 50\n",
    "output_channels2 = 10\n",
    "kernel_size1 = 3\n",
    "kernel_size2 = 3 \n",
    "num_classes = 2  # Number of output classes, adjust as needed\n",
    "\n",
    "cnn_net_model = CNN_Net(embedding_size, num_channels, output_channels1, output_channels2, kernel_size1, kernel_size2, num_classes)\n",
    "print(cnn_net_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IGNORE: Loaders to use. Defined and set as a variable above\n",
    "\n",
    "# --- Binary ---\n",
    "# pretrained_binary_train_loader\n",
    "# pretrained_binary_test_loader\n",
    "\n",
    "# my_trained_binary_train_loader\n",
    "# my_trained_binary_test_loader\n",
    "\n",
    "# --- Ternary ---\n",
    "# pretrained_ternary_train_loader\n",
    "# pretrained_ternary_test_loader\n",
    "\n",
    "# my_trained_ternary_train_loader\n",
    "# my_trained_ternary_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-train_binary_cnn_model\n",
      "Epoch ---  0\n",
      "Train avg total loss ---  0.6945264577865601\n",
      "Evaluation avg total loss ---  0.17471851110458375\n",
      "Epoch: 1 \tTraining Loss: 0.701131 \tTraining Accuracy: 56.25%\n",
      "\n",
      "Epoch ---  1\n",
      "Train avg total loss ---  0.6913781881332397\n",
      "Evaluation avg total loss ---  0.17505110502243043\n",
      "Epoch: 2 \tTraining Loss: 0.704594 \tTraining Accuracy: 53.28%\n",
      "\n",
      "Epoch ---  2\n",
      "Train avg total loss ---  0.6889927446842193\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 3 \tTraining Loss: 0.693147 \tTraining Accuracy: 52.66%\n",
      "\n",
      "Epoch ---  3\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 4 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  4\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 5 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  5\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 6 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  6\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 7 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  7\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 8 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  8\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 9 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Epoch ---  9\n",
      "Train avg total loss ---  0.6931473016738892\n",
      "Evaluation avg total loss ---  0.1732868254184723\n",
      "Epoch: 10 \tTraining Loss: 0.693147 \tTraining Accuracy: 50.00%\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_net_model.parameters(), lr=1, momentum=0.9)\n",
    "print(\"2-train_binary_cnn_model\")\n",
    "output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, pretrained_binary_train_loader, pretrained_binary_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP HERE --- IGNORE BELOW #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Binary', acc_50_pretrained_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_binary_cnn_model(my_binary_train_embeddings, binary_y_train)\n",
    "# acc_50_my_binary = predict_binary_cnn_model(my_binary_train_embeddings, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = store_results(results_dict, 'My model', 'CNN-500', 'Binary', acc_50_my_binary)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ternary_embeddings, d_ternary_embeddings = my_ternary_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CNN_Net.__init__() missing 2 required positional arguments: 'kernel_size2' and 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m num_output_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m----> 6\u001b[0m net_model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_Net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_h1_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_h2_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_binary_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(net_model)\n",
      "\u001b[0;31mTypeError\u001b[0m: CNN_Net.__init__() missing 2 required positional arguments: 'kernel_size2' and 'num_classes'"
     ]
    }
   ],
   "source": [
    "num_h1_nodes = 50\n",
    "num_h2_nodes = 10\n",
    "num_output_classes = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "net_model = CNN_Net(num_h1_nodes, num_h2_nodes, d_binary_embeddings, num_output_classes, dropout_rate)\n",
    "print(net_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ternary_cnn_model(ternary_train_embeddings, ternary_train_y):\n",
    "    ternary_train_y = ternary_train_y.replace(2, 0)\n",
    "    ternary_train_y = ternary_train_y.replace(3, 2)\n",
    "    data_tensor = torch.tensor(ternary_train_embeddings, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(ternary_train_y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 64\n",
    "\n",
    "    # as a positive integer will turn on multi-process data loading with the specified number of loader worker processes; otherwise, single-process data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    number_of_epochs = 30\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net_model.parameters(), lr=0.01)\n",
    "    output_of_model = cnn_net_model.train_network(number_of_epochs, optimizer, criterion, train_loader)\n",
    "    \n",
    "    return output_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ternary_cnn_model(ternary_test_embeddings, ternary_test_y):\n",
    "    ternary_test_y = ternary_test_y.replace(2, 0)\n",
    "    ternary_test_y = ternary_test_y.replace(3, 2)\n",
    "    target_array = ternary_test_y.values\n",
    "    data_tensor = torch.tensor(ternary_test_embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Create a PyTorch tensor from the NumPy array\n",
    "    target_tensor = torch.tensor(target_array, dtype=torch.long)  # Assuming target is of type long/int\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 1\n",
    "\n",
    "    # Define number of DataLoader workers\n",
    "    num_workers = 0  # Set this to a positive integer to enable multi-process data loading\n",
    "\n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Assuming net_model is an instance of your model\n",
    "    predictions = cnn_net_model.cnn_predict(test_loader)\n",
    "    # predictions = np.array(predictions)\n",
    "    # predictions_flat = predictions.flatten()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(pretrained_ternary_train_embeddings, ternary_y_train)\n",
    "# acc_50_pretrained_ternary = predict_ternary_cnn_model(pretrained_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'Pretrained', 'CNN-50', 'Ternary', acc_50_pretrained_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ternary_cnn_model(my_ternary_train_embeddings, ternary_y_train)\n",
    "acc_50_my_ternary = predict_ternary_cnn_model(my_ternary_test_embeddings, ternary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = store_results(results_dict, 'My model', 'CNN-50', 'Ternary', acc_50_my_ternary)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape) # [batch_size = 64 , number_of_class (ternary = 3), (bin = 2) ]\n",
    "#             print(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
